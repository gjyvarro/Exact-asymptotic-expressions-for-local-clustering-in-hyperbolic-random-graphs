\section{Introduction and results}

\subsection{Motivation}

%\MS{I would not write a length introduction to complex networks here (because the paper is quite long already and we do not study networks in general in this paper). So, I would remove the first paragraph/list (or incorporate it as a single sentence in the ensuing paragraph). }
%\PvdH{I agree.}

Hyperbolic random graphs were suggested as a suitable model for complex networks by Krioukov et al.~\cite{krioukov2010hyperbolic}, exhibiting the three main characteristics commonly found in complex networks: 
\begin{enumerate}[\upshape 1)]
\item Broad degree distribution (power-law/scale-free),
\item Strong clustering (community structure),
\item Small path lengths (small world phenomenon).
\end{enumerate}
Indeed, in their seminal paper they showed that these graphs have a power-law degree distribution and exhibit strong clustering, while Bringmann et al.~\cite{bringmann2016average} showed that hyperbolic random graphs have doubly logarithmic shortest path lengths. \TM{ \RD{Remove this reference, and replace with Abdullah-Bode-Fountoulakis!!!!}
In general the proofs that GIRG = HRG are shaky at best, so I do not want to cite anything that relies on that.
Of course, in this case Nikoloas & his crew were  first anyway.
Also, regarding "distances", we probably want to mention graph diameter, studied by Kiwi+Mitsche, Friedrich+Krohmer, M\"uller + Staps.
}
Many other results regarding hyperbolic random graphs have since been established: for instance the existence of efficient routing algorithms~\cite{bringmann2017greedy}, the size of the largest component~\cite{bode2015largest},~\cite{fountoulakis2018law} and the number of cliques and the largest clique~\cite{blasius2018cliques}. \PvdH{@All: Does anyone have other suggestions for results to add?}
\TM{ Yes, of course! Everything Nikolaos & his crew, and I and my crew, and Kiwi-Mitsche have done on this model. This includes 2nd largest componnent, connectivity, spectral gap, diameter, Hamilton cycles & perfect matchings (with Markus, Nikolaos and Mitsche, in preparation but perhaps submitted around the same time as this one.).
Also there is a recent paper on arxiv by Owada+Yogeshwaran on subgraph counts.}

The aim of this paper is to study local clustering in hyperbolic random graphs. The first rigorous mathematical analysis of the local clustering coefficient was done by Gugelmann et al.~\cite{gugelmann2012random}. They show~\cite[Theorem 2.1]{gugelmann2012random} that the local clustering coefficient\footnote{Note that in~\cite{gugelmann2012random} this is called the global clustering coefficient. However, since this term is more often associated in the literature with the density of triangles compared to the number of paths of length 2, we use the term local clustering coefficient as is done, for instance, in~\cite{candellero2016clustering}.} is concentrated around its expectation which is shown to be $\bigT{1}$ as the number of vertices tends to infinity. In particular this result implies that the local clustering coefficient is asymptotically bounded away from zero. The convergence of this coefficient was however not proven. In contrast, the global clustering coefficient was shown~\cite{candellero2016clustering} to converge in probability to some constant which can be explicitly stated as an integral expression, see Theorem 1.2 in \cite{candellero2016clustering}. The authors mention, however, that their analysis needs significant modification to be able to deal with convergence of the local clustering coefficient.



In addition to the clustering coefficients, an important clustering measure that is often studied is the local clustering function $c(k)$. This function computes for any value of $k$ the average of the local clustering coefficient over all vertices of degree $k$. A general expression of this function for hyperbolic random graphs is given in \cite[Equation (59)]{krioukov2010hyperbolic}. The authors conjecture that as $k$ tends to infinity, $c(k)$ decays as $k^{-1}$, which they observe (Figure 8 in \cite{krioukov2010hyperbolic}) in experiments on the infrastructure of the Internet obtained in~\cite{claffy2009internet}. However, despite the importance of the local clustering function and these interesting observations, its behavior in hyperbolic random graphs had not been completely determined and the following crucial questions regarding clustering in hyperbolic random graphs remained open so far:
\begin{enumerate}[\upshape 1)]
\item Does the local clustering coefficient converge, and if so, what is the limit?
\item What is the limit of the local clustering function and how does it scale with the degree?
\end{enumerate}

In this work we resolve these important open questions. We obtain the asymptotic scaling of $c(k)$ as $k \to \infty$, including the leading constant, as well as an exact expression, in terms of known special functions, of the point-wise limit of the local clustering function as the number of vertices tends to infinity. Interestingly, the scaling of $c(k)$ depends on the exponent of the degree distribution and is only $k^{-1}$ when this exponent exceeds $5/2$. For values less than $5/2$ the scaling is $k^{-s}$ where the value of $s$ depends on the degree distribution exponent. Finally, our analysis allows us to also prove a convergence result for the local clustering coefficient where the limit can again be explicitly expressed in terms of known special functions. 

\TM{ I vote to remove this next paragraph. Use of GIRGs is fishy, and also we are not obliged to mention any and all possible alternative approaches. }
We remark that it is possible to prove that the local clustering coefficient converges to some limit, using the technical tools in the recent work on explosion times in Geometric Inhomogeneous Random Graphs~\cite{komjathy2018explosion}. However, this problem is not explicitly addressed in that paper, nor is there any reason to expect these tools to give us the exact expressions as we do derive in this work.

%\PvdH{I recall that J{\'u}lia told me that the results from~\cite{komjathy2018explosion} imply that the local clustering coefficient converges, although they do not mention it in the paper. Should we added a comment related to this?}

%\MS{Yes, I think this would probably be good. Either after the sentence `the convergence of this coefficient was not proven' add something like, as you say, the results form .. imply convergence, although not specifically mentioned there. Or drop that sentence about that the convergence was not shown and also rephrase the sentence in 1, to just what is the limit. It should just be clear to the reader that we are not claiming the credit for the convergence itself, but mostly for finding the exact limit (in an analytic expression) and with a very strong sense of convergence. So, let's better emphasize those last two points and hope that they are good enough.}

\subsection{Outline of the paper}

In the next two subsections \ref{ssec:hyperbolic_model} and \ref{ssec:local_clustering} of the introduction, we give the definitions of the hyperbolic random graph and of the local clustering coefficient and function as used in this paper. After this we present our main results in Section \ref{ssec:main_results} and discuss several insights that can be obtained from them and in Section \ref{ssec:simulations} show simulations which confirm these results. Section \ref{sec:proof_outline} contains a detailed high level outline of the proofs of our main results, split up into different propositions. The proofs of these propositions can be found in the last four sections. Section~\ref{sec:concentration_argument} contains a crucial technical result that will be used in the other sections. The Appendix contains a small section on Meijer's G-function and some known concentration results we use.

\subsection{Hyperbolic random graphs}\label{ssec:hyperbolic_model}

The hyperbolic plane is an infinite $2$-dimensional manifold with constant negative curvature. Many different representations of the hyperbolic plane exist and we refer to~\cite{anderson2006hyperbolic} \cite{katok1992fuchsian} \cite{beardon2012geometry} for an introduction to hyperbolic geometry.  We will work with the \emph{native representation} of the hyperbolic plane which takes the elements of $\R^2$ in polar coordinates $(r,\theta)$ as the underlying set of points and where distances are determined by the hyperbolic law of cosines.

Let $\alpha > \frac{1}{2}$, $\nu > 0$ and define $R_n = 2\log(n/\nu)$. Then the hyperbolic random graph $G_{\H,n}(\alpha, \nu)$ is defined as follows:
\begin{enumerate}
\item The vertex set if given by $n$ points $u_1, \dots, u_n$ denoted in polar coordinates $(r_i, \theta_i)$, where the angular coordinate $\theta$ is chosen uniformly from $(-\pi,\pi]$ while the radial coordinate $r$ is sampled independently according to the cumulative distribution function
\begin{equation}\label{eq:def_hyperbolic_point_distribution}
	F_{n,\alpha,\nu}(r) = \begin{cases}
		0 &\mbox{if } r < 0\\
		\frac{\cosh(\alpha r)-1}{\cosh(\alpha R_n) - 1} &\mbox{if } 0 \le r \le R_n\\
		1&\mbox{if } r > R_n
	\end{cases}
\end{equation}
\item Any two vertices $u_i=(r_i,\theta_i)$ and $u_j=(r_j,\theta_j)$ are adjacent if and only if $d_\H(u_i,u_j) \le R_n$, where $d_\H$ denotes the distance in the hyperbolic plane, i.e.
\[
	\cosh(r_i) \cosh(r_j) - \sinh(r_i) \sinh( r_j) \cos(|\theta_i-\theta_j|_{2\pi}) \le \cosh(R_n),
\]
as follows from the hyperbolic law of cosines.% rule[CITE?].
\TM{ This is only when $r_i+r_j \geq R$! Otherwise the equation does not have a solution as there is no triangle with side lengths $r_i, r_j, R$.}
\end{enumerate}

\begin{figure}[!t]
\centering
\includegraphics[scale=0.3]{figures/KPKVB.png}
\caption{Example of a Hyperbolic Random Graph $G_{\H,n}(\alpha, \nu)$ with $\alpha = 0.9$, $\nu = 0.2$ and $n = 5000$.
%\PvdH{@Tobias: this figure (HyperRGG\_N5000L5.eps) is taken from the figures you send me. Could you fill in the value of $\alpha$ and $\nu$ you used for the simulation.}
}
\label{fig:H_graph_example}
\end{figure}



Figure~\ref{fig:H_graph_example} shows an example of $G_{\H,n}(\alpha, \nu)$.



%We say that the points $u_i \in G_{\H, n}$ are distributed according to a \emph{$(\alpha, R_n)$-quasi uniform distribution}.



\subsection{Clustering}\label{ssec:local_clustering}

Clustering measures for networks consider the fraction of triangles (triples of connected vertices) in the network. 
For instance, for a simple graph $G$, with vertex set $V(G)$ and edge set $E(G)$, denote by $T_G$ and $\Lambda_G$, respectively, the total number of triangles and the total number of paths of length two in $G$. Then its \emph{global clustering coefficient} is defined as
\[
	c_G = \frac{3 \T_G}{\Lambda_G}.
\]
%This coefficient measures the fraction of triangles in the network compared to the total possible number of triangles. 
\TM{ Do we even need to give this definition? We don't study it, so I would say it suffices to just mention there is another, rival defintion to the one we use.}
A different concept for clustering is defined for each vertex and by taking averages also for the entire graph as well as for all vertices with a specified degree: the \emph{local clustering coefficient} of a vertex $v \in V(G)$ is a real number between zero and one for the extent to which the neighborhood of $v$ resembles a clique. If the vertex $v$ has $k\geq 2$ neighbors, there are ${k \choose 2}$ possible edges between them and its local clustering coefficient is the quotient of the number of these pairs that constitute an existing edge in the graph divided by ${k \choose 2}$. \TM{ Here I would add the formula for readability.} 
If the vertex $v$ has no or only one neighbor, its clustering coefficient is set to zero. The (average) local clustering coefficient of the graph is the average over the clustering coefficients of all vertices. The local clustering function maps a natural number $k$ to the average over the clustering coefficients of all vertices with degree $k$ if there are vertices with degree $k$ and zero otherwise.

To introduce the notations for these definitions, let $D_G(v)$ denote the degree of vertex $v$ and $N_G(k)$ the number of vertices with degree $k$ in the graph $G$. In addition let $$\T_G(v,v_1,v_2) = \ind{(v,v_1) \in E} \ind{(v, v_2) \in E} \ind{(v_1, v_2) \in E}$$ be the indicator that $v, v_1$ and $v_2$ form a triangle and write
\begin{equation}
	\T_G(v) = \sum_{v_1, v_2 \in V(G)} \T_G(v,v_1,v_2),
\end{equation}
to denote the number of triangles in which node $v$ participates. Then the \emph{local clustering coefficient} is given by
\[
	c_G = \frac{1}{|V(G)|} \sum_{v \in V(G)} \frac{T_G(v)}{\binom{D_G(v)}{2}}.
\]

\TM{ I would find it natural to define $c(v)$ first and then let $c(G)$ be the average. }
However, since the local clustering coefficient assigns just one value to the whole network, representing its triangular structure, it is unable to characterize local structural properties involving triangles. 
\TM{ This last sentence seems a bit weird to me, rather abstract. }
The local clustering function, on the other hand, measures the fraction of triangles to which vertices of a given degree belong, compared to the maximum number of triangles in which they could participate~\cite{vazquez2002large,serrano2006clustering}. It describes the triangular structure of vertices in the networks based on their degree and gives a more detailed look at the overall structure of the network.
\TM{ Similar comment to the previous one. I am not really sure what this means. }
The formal definition of the \emph{local clustering function} is 
\begin{equation}\label{eq:def_local_clustering_general}
	c_{G}(k) = \begin{cases}
		\frac{1}{N_G(k)} \sum_{v \in V(G)}  \ind{D_G(v) = k} \frac{\T_G(v)}{\binom{k}{2}} &\mbox{if } N_G(k) \ge 1\\
		0 &\mbox{else.}
	\end{cases}
\end{equation} 

\TM{ Perhaps we should say that, while it may be more natural to consider $c(k)$ to be undefined when $N_k=0$, we use this definition for technical convenience. This way 
$c(k)$ is a plain vanilla random variable and we can compute expectation and so on without issue, and speak about convergence in probability and what not. }
%We note that other versions of the local clustering function have been considered, for instance where nodes with degree larger than $k$ are considered instead of nodes with degree exactly $k$. \PvdH{Motivate our choice for this local clustering function.} 

\begin{remark}[Notational convention]\label{rmk:notation}
In the remainder of this paper we will compare the local clustering function and many other characteristics between several different graph models. To make notation less cluttered we will often use a unique subscript to identify the graph with respect to which the specific property refers instead of the full graph description. For instance, we shall write $c_{\H,n}(k)$ to denote $c_{G_{\H,n}(\alpha, \nu)}(k)$ and similar, $D_{\H, n}(u)$ for $D_{G_{\H,n}(\alpha, \nu)}(u)$. For the infinite model we will use the subscript $\infty$.
\end{remark}


\subsection{Main results}\label{ssec:main_results}

We are now ready to state our main results for the clustering coefficient and local clustering function in the hyperbolic random graph. When the degree $k$ is a growing function of $n$ we are able to exactly compute the asymptotic behavior of $c_{\H,n}(k)$. 
\TM{ Seems a little strong. We "exactly compute" the leading terms, I would say. }

The results are obtained by coupling the hyperbolic random graph to an infinite random graph $G_\Pcal(\alpha,\nu)$, which we define in Section~\ref{ssec:infinite_model}, and show that the limit of the local clustering coefficient and function for the hyperbolic random graph are given by those for the infinite model. 

%\MS{Here, I would just say that $B(a,b)$ denotes the beta function $B^-(x,a,b)$ the lower incomplete beta function etc. and put/refer to the integral representations in the appendix, as already done for the Meijer-G function.}
%\subsubsection{Special functions}
We let $B(a,b)$ denote the beta-function and $B^-(x,a,b)$ the lower incomplete beta-function. We also write $\Gamma(z)$ for the Gamma function, $\Gamma^+(q,z)$ for the \TM{ upper } incomplete Gamma function and define $\Gamma^\ast(q,z) := \Gamma^+(1 + q, z) + \Gamma^+(q, z)$. \TM{ How useful is this extra definiton? If it is just to make the expression in Thm 1.1 neater, then I dont think we need it. }
Finally, we write $U(a,b,z)$ for the hypergeometric U-function (also called Tricomi's confluent hypergeometric function), which for $a,b,z\in \mathbb{C}$, $b \not \in \mathbb{Z}_{\leq 0}$, $\mathrm{Re}(a), \mathrm{Re}(z) >0$ has the integral representation 
\[
	U(a,b,z) = \frac{1}{\Gamma(a)} \int_0^\infty e^{-zt} t^{a-1} (1+t)^{b-a-1} dt,
\] 
see~\cite[p.255 Equation (2)]{erdelyi1953higher}, and let $\MeijerGnew{m}{\ell}{p}{q}{{\bf a}}{{\bf b}}{z}$ denote Meijer's G-Function~\cite{meijer1946gfunction}, see Appendix~\ref{sec:Meijer_G_functions} for more details.
\TM{ Should we not maybe also say what the upper incomplete gamma and lower incomplete beta are then? }


\subsubsection{Local clustering coefficient}

Our first result shows that the local clustering coefficient of the hyperbolic random graph converges in expectation to a constant $c_\infty$, which we can explicitly write down in terms of the special functions mentioned above.

\begin{theorem}[Limit for local clustering coefficient in $G_{\H,n}(\alpha,\nu)$] \label{thm:clustering_coefficient_hyperbolic}
Let $\alpha > \frac{1}{2}$, $\nu > 0$, and $\xi_{\alpha,\nu} = \frac{4\alpha \nu}{\pi(2\alpha -1)}$. Then,
\[
	\lim_{n \to \infty} \Exp{\left|c_{\H,n} - c_\infty\right|} = 0,
\]
\TM{ As far as I can tell this is just equivalent to 
$c(G) \plim c_\infty$. I would prefer to use this more standard notation and terminology, certainly at least here, where we present our main results. }
where $c_\infty$ is defined for $\alpha \ne 1$ as
\begin{align*}
	c_\infty 
	&=\frac{2 + 4 \alpha + 13 \alpha^2 - 34 \alpha^3 - 12\alpha^4 + 24 \alpha^5}
		{16(\alpha-1)^2 \alpha (\alpha+1) (2\alpha+1)} 
		+  \frac{2^{-1 - 4 \alpha}}{(\alpha - 1)^2} \\
	&\hspace{10pt}+ \frac{(\alpha - 1/2) (B(2 \alpha, 2 \alpha + 1) + B^-(1/2; 1 + 2 \alpha, -2 + 2 \alpha))}
		{2 (\alpha - 1) (3 \alpha - 1)} \\
	&\hspace{10pt}+ \frac{\xi_{\alpha,\nu}^{2\alpha} \Gamma^\ast( - 2 \alpha, \xi_{\alpha,\nu})}{4(\alpha-1)}
		+ \frac{\xi_{\alpha,\nu}^{2\alpha + 2}\alpha (\alpha - 1/2)^2 \Gamma^\ast(- 2 \alpha - 2, \xi_{\alpha,\nu})}
		{2(\alpha-1)^2} \\
	&\hspace{10pt}- \frac{\xi_{\alpha,\nu}^{2\alpha + 1}\alpha (2\alpha - 1) \Gamma^\ast( - 2 \alpha - 1,\xi_{\alpha,\nu})}{(\alpha-1)}
		- \frac{\xi_{\alpha,\nu}^{6\alpha-2}2^{-4\alpha}(3\alpha - 1)\Gamma^\ast( - 6 \alpha + 2, \xi_{\alpha,\nu})}{(\alpha-1)^2}\\
	&\hspace{10pt}-\frac{\xi_{\alpha,\nu}^{6\alpha - 2}(\alpha - 1/2) B^-(1/2; 1 + 2 \alpha, -2 + 2 \alpha)\Gamma^\ast( - 6 \alpha + 2, \xi_{\alpha,\nu})}{(\alpha-1)} \\
	&\hspace{10pt}- \frac{e^{-\xi_{\alpha,\nu}} \Gamma(2\alpha+1) 
		\left(U(2\alpha+1,1-2\alpha,\xi_{\alpha,\nu}) + U(2\alpha+1,2-2\alpha,\xi_{\alpha,\nu})\right)}{4(\alpha-1)} \\
	&\hspace{10pt}+ \frac{\xi_{\alpha,\nu}^{6\alpha - 2} \Gamma(2\alpha+1)\left( 	
		\MeijerGnew{3}{0}{2}{3}{1,3-2\alpha}{3-4\alpha,-6\alpha+2,0}{\xi_{\alpha,\nu}}
		+ \MeijerGnew{3}{0}{2}{3}{1,3-2\alpha}{3-4\alpha,-6\alpha+3,0}{\xi_{\alpha,\nu}}\right)}{4(\alpha-1)},
\end{align*}
and for $\alpha = 1$ as
\begin{align*}
	c_\infty &= \frac{575 - 12 \pi^2}{576} + \frac{\eta^4(7 + \pi^2)\Gamma^\ast(-4, \eta)}{4}\\
	&\hspace{10pt}- \frac{1}{2} \int_0^1 (1 - 4z + 3z^3)\log(1-z)(z + \eta)e^{-\eta/z} \dd z\\
	&\hspace{10pt}- \int_0^1 \Li_2(z)(z^3 + \eta z^2) e^{-\eta/z} \dd z,		
\end{align*}
with $\eta = 4\nu/\pi$ and $\Li_2(z) = \sum_{t = 1}^\infty z^t/t^2$, the dilogarithm function\footnote{Note that the integrals in the expression for $c_\infty$ for $\alpha = 1$ exists: for the first one note that $1-4z+3z^2=(1-z)(1-3z)$, so the integrand can be bounded by $C(1-z)\log(1-z)$ on $[0,1)$ for some constant $C$, which can be continued continuously to the compact interval $[0,1]$ by noting that the limit for $z \rightarrow 1$ is zero, so the integrand is bounded on a bounded domain and hence, this integral is finite; for the second integral note that $\Li_2(z)$ is bounded by $\Li_2(1)$ on $[0,1]$, which is a series with well-known finite limit, so again the integrand is bounded on a bounded domain and hence the second integral is also finite.}.
\end{theorem}

\TM{ Do we want to remark something re: closed form expression for the $\alpha=1$ case? }

\subsubsection{Local clustering function}

For the local clustering function we also obtain convergence in expectation to a function $c_\infty(k)$ which can be explicitly stated using the special functions.

\begin{theorem}[Limit for local clustering function in $G_{\H,n}(\alpha, \nu)$]
\label{thm:local_clustering_hyperbolic}
Let $\alpha > \frac{1}{2}$, $\nu > 0$, $\xi = \frac{4\alpha\nu}{\pi(2\alpha-1)}$ and $(k_n)_{n \ge 1}$ be any positive sequence, possibly constant, such that $k_n \ge \GR{2}$ and $k_n = \smallO{n^{\frac{1}{2\alpha + 1}}}$. Then, 
\[
	\lim_{n \to \infty} \Exp{\left|\frac{c_{\H,n}(k_n)}{c_\infty(k_n)} - 1\right|} = 0,
\]
where the function $c_\infty(k)$ is defined for $\alpha \ne 1$ as
\begin{align*}
c_\infty(k)  =&\frac{1}{8\alpha (\alpha-1)\Gamma^+(k-2\alpha,\xi)} \left( -\Gamma^+(k - 2 \alpha, \xi) - 2\frac{\alpha (\alpha - 1/2)^2 \xi^{2} \Gamma^+(k - 2 \alpha - 2, \xi)}{(\alpha - 1)} \right. \\ 
&\left.+ 8 \alpha (\alpha - 1/2) \xi \Gamma^+(k - 2 \alpha - 1,\xi) \right.\\ 
&\left.+ 4\xi^{4\alpha - 2} \Gamma^+(k - 6 \alpha + 2, 
      \xi) \left( \frac{2^{ - 4\alpha}(3 \alpha - 1)}{(\alpha - 1)} + (\alpha - 1/2) B^-(1/2; 1 + 2 \alpha, -2 + 2 \alpha) \right)  \right.\\ 
&\left.+ \xi^{k-2\alpha} \Gamma(2\alpha+1)e^{-\xi} U(2\alpha+1,1+k-2\alpha,\xi) \right. \\ 
&\left.- \xi^{4\alpha-2} \Gamma(2\alpha+1)\MeijerG{3}{0}{2}{3}{1,3-2\alpha}{3-4\alpha,-6\alpha+k+2,0}{\xi}  \right)
\end{align*}
and for $\alpha = 1$ as
\begin{align*}
	c_\infty(k) &= \frac{9 \eta^3}{2 k!} 	
		\Gamma^+(k-3,\eta)-\frac{\xi_{\alpha,\nu}^4}{k!}\frac{7+\pi^2}{4}\Gamma^+(k-4,\eta)\\
	&\hspace{10pt}+ \frac{\eta^k}{2k!}\int_0^1 (1-4z+3z^2)\ln(1-z)z^{1-k}e^{-\eta/z}\dd z\\ 
	&\hspace{10pt}+ \frac{\eta^k}{k!}\int_0^1 z^{3-k} \Li_2(z) e^{-\eta/z} \dd z,
\end{align*}
with $\eta = 4\nu/\pi$ and $\Li_2(z) = \sum_{t = 1}^\infty z^t/t^2$, the dilogarithm function.
\end{theorem}

\TM{ I would be tempted to split this thm into two.
The first states that for $k$ fixed
$c(k) \plim c_\infty(k)$. Then we say the proof extends to ... the second thm which states that if
$1 \ll k_n \ll n^{1/(2\alpha+1)}$ then
$c(k_n) = (1+o(1)) c_\infty(k_n)$ a.a.s. \\
In fact, maybe it is good to present a separate -- and short -- proof of the constant case. The equivalence of the models for constant $k$ can be worked out in a few pages while for the sequence we need 50+ pages of super technical computations. What do you think? }

From the expression for $c_\infty(k)$ we obtain its asymptotic behavior as $k \to \infty$. For this we first define, for any $\frac{1}{2} < \alpha < \frac{3}{4}$,
\begin{equation}\label{eq:def_C_alpha}
	C_\alpha = \frac{2^{-4\alpha - 1}(3\alpha - 1)}{\alpha(\alpha-1)^2} 
	+ \frac{\alpha - \frac{1}{2}}{2(\alpha - 1)\alpha} B^-(\frac{1}{2},2\alpha + 1, 2\alpha - 2)
	- \frac{1}{4(\alpha - 1)}B(2\alpha, 3\alpha - 4).
\end{equation}
Moreover, to simplify the statement we define the scaling function 
\begin{equation}\label{eq:def_scaling_function}
s_\alpha(k) = \begin{cases} 
		k^{4\alpha-2} &\frac{1}{2}<\alpha<\frac{3}{4}, \\
		\frac{k}{\log(k)} & \alpha = \frac{3}{4}, \\
		k &\alpha > \frac{3}{4}.
\end{cases}
\end{equation}
\TM{ These exponents are $>0$! (see next comment)}

We now have the following result, where the constant $C_\alpha$ emerges when we consider the local clustering function for $\frac{1}{2} < \alpha < \frac{3}{4}$.

\begin{theorem}[Asymptotic behavior of local clustering function limit]\label{thm:asymptotics_average_clustering_P}
Let $\alpha > \frac{1}{2}$, $\nu > 0$. Then,
\[
	\lim_{k \to \infty} \frac{c_\infty(k)}{s_\alpha(k)} 
	= \begin{cases}
			C_\alpha \left(\frac{4\alpha \nu}{\pi\left(2\alpha - 1\right)}\right)^{4\alpha - 2}
			&\mbox{if } \frac{1}{2} < \alpha < \frac{3}{4},\\
			\frac{3 \nu}{\pi} &\mbox{if } \alpha = \frac{3}{4},\\
			\frac{8\alpha \nu}{\pi\left(4\alpha - 3\right)} &\mbox{if } \alpha > \frac{3}{4},
	\end{cases}
\]
with $C_\alpha$ as defined in \eqref{eq:def_C_alpha}.
\end{theorem}
\TM{ There is a typo here. Since you divide by $s$ in this last thm and $s\to\infty$ as the exponents are $>0$ and clustering coefficients are $\in [0,1]$, you would get limiting value zero. }
\TM{ Also, I dont know if this previous result really deserves the status of ``theorem''. Feels more like a lemma or proposition. } 
The following is an immediate consequence of Theorem~\ref{thm:local_clustering_hyperbolic} and Theorem~\ref{thm:asymptotics_average_clustering_P}.

\begin{corollary}[Asymptotic behavior of local clustering function in $G_{\H,n}(\alpha,\nu)$]
\label{cor:asymptotic_local_clustering_hyperbolic}
Let $\alpha > \frac{1}{2}$, $\nu > 0$, $(k_n)_{n \ge 1}$ be any positive sequence such that $k_n \to \infty$ and $k_n = \smallO{n^{\frac{1}{2\alpha + 1}}}$ and $s_\alpha(k)$ defined as in~\eqref{eq:def_scaling_function}. Then, as $n \to \infty$,
\[
	\frac{c_{\H,n}(k_n)}{s_\alpha(k_n)} \, \stackrel{L^1}{\rightarrow} \, 
	\begin{cases}
				C_\alpha \left(\frac{4\alpha \nu}{\pi\left(2\alpha - 1\right)}\right)^{4\alpha - 2}
				&\mbox{if } \frac{1}{2} < \alpha < \frac{3}{4},\\
				\frac{6\nu}{\pi} &\mbox{if } \alpha = \frac{3}{4},\\
				\frac{8\alpha \nu}{\pi\left(4\alpha - 3\right)} &\mbox{if } \alpha > \frac{3}{4},
		\end{cases}
\]
where $\stackrel{L^1}{\to}$ denotes convergence in expectation.
\end{corollary}

\TM{ I had not heard of ``convergence in expectation'' before this. I have heard of $L_1$-convergence though. }
\MS{@Pim: Are you sure all constants are correct? I thought for $\alpha = 3/4$ it might be $6\nu/\pi$. But if you are sure, you can remove this comment of course.}
\PvdH{@Markus: You might be right. I have updated it and we will later check if you were right.}

This result completely \TM{ ``completely" is pretty strong. Note also we seem to stay away from 
$k = \Theta( n^{1/(2\alpha+1)} )$. So unless we change that, there is reason to say we do not to it ``completely''.} 
characterizes the asymptotic behavior of the local clustering function in Hyperbolic Random Graphs. In particular we observe that the conjectured scaling of $k^{-1}$ from~\cite{krioukov2010hyperbolic} only occurs when $\alpha > 3/4$, or equivalently, when the exponent of the pdf of the degree distribution is larger than $5/2$. 

A major part of this paper is dedicated to prove the convergence result in Theorem~\ref{thm:local_clustering_hyperbolic}, which is based on a sequence of subsequent results for the local clustering function in related random graph models and Theorem~\ref{thm:asymptotics_average_clustering_P}. The exact flow of the argument is explained in Section \ref{sec:proof_outline}. The proof for the convergence result in Theorem~\ref{thm:local_clustering_hyperbolic}, using these results, can be found in Section \ref{ssec:proof_main_results}. Here we also give the proof of the convergence result in Theorem~\ref{thm:clustering_coefficient_hyperbolic}. Another major part of this paper is the exact computation of the local clustering coefficient and function limit $c_\infty$ and $c_\infty(k)$, which involves careful computations of several involved integrals. This is done in Section~\ref{sec:asymptotics_average_clustering_ast_P}, where we also prove Theorem~\ref{thm:asymptotics_average_clustering_P}.

We end this section with some important observations regarding local clustering in hyperbolic random graphs.

\PvdH{Populate these paragraphs}


 

\paragraph{Uniform convergence}

Our results for the local clustering function in particular imply uniform convergence of $c_{\H,n}(k)$ for all $3 \le k \le a_n$ \TM{ Why 3? } where $a_n = \smallO{n^{\frac{1}{2\alpha + 1}}}$. To see this let
\[
	b_n = \arg \max_{3 \le k \le a_n} \Exp{\left|\frac{c_{\H ,n}(k)}{c_\infty(k)} - 1\right|}.
\]
Then $b_n \le a_n = \smallO{n^{\frac{1}{2\alpha + 1}}}$ and therefore by Theorem~\ref{thm:local_clustering_hyperbolic}
\[
	\lim_{n \to \infty} \max_{3 \le k \le a_n} \Exp{\left|\frac{c_{\H ,n}(k)}{c_\infty(k)} - 1\right|} 
	= \lim_{n \to \infty}\Exp{\left|\frac{c_{\H ,n}(b_n)}{c_\infty(b_n)} - 1\right|} = 0.
\]
The same is true for the result in Corollary~\ref{cor:asymptotic_local_clustering_hyperbolic}.

\paragraph{Maximum scaling for $k_n$ is $n^{\frac{1}{2\alpha +1}}$}
All our results for clustering in the hyperbolic random graph are valid for any sequence $k_n$ such that $k_n = \smallO{n^{\frac{1}{2\alpha + 1}}}$. Although one would like to have results for any sequence $k_n \le n$, it turns out that $n^{1/(2\alpha + 1)}$ is the optimal \TM{ again seems to strong to me } scaling for which Theorem~\ref{thm:clustering_coefficient_hyperbolic} can be true. To see why this is the case note that by definition of the local clustering function~\eqref{eq:def_local_clustering_general} we have that $c_{\H,n}(k_n) = 0$ if $N_{\H,n}(k_n) = 0$. Hence, it follows by Markov's inequality that for any positive function $f$
\begin{align*}
	\Exp{\left|\frac{c_{\H,n}(k_n)}{f(k_n)} - 1\right|} 
	&\ge \Prob{N_{\H,n}(k_n) = 0} \ge 1 - \Exp{N_{\H,n}(k_n)}.
\end{align*}
We shall later establish (see Lemma~\ref{lem:N_k_H_P}) that $\Exp{N_{\H,n}(k_n)} = \bigT{n k_n^{-(2\alpha + 1)}}$. Therefore if $k_n$ is such that $k_n^{-(2\alpha + 1)} n$ tends to zero as $n \to \infty$ we have that 
\[
	\lim_{n \to \infty} \Exp{N_{\H,n}(k_n)} = 0
\]
and hence
\[
	\lim_{n \to \infty} \Exp{\left|\frac{c_{\H,n}(k_n)}{f(k_n)} - 1\right|} 
	\ge \lim_{n \to \infty} 1 - \Exp{N_{\H,n}(k_n)} = 1 \ne 0,
\]
for any positive function $f$. This implies that we cannot expect a result like that of Theorem~\ref{thm:clustering_coefficient_hyperbolic} to hold as soon as $k_n$ is not $\smallO{n^{\frac{1}{2\alpha + 1}}}$.

\paragraph{Transition in the scaling of local clustering}

\MS{@All: If we can, then it would be interesting to remark that there is this phase transition in $\alpha$ which appears when the power-law exponent is $2.5=\frac{5}{2}$ and it would be cool if we could give a brief intuitive reason for that or mention that this phase transition point has also been observed in other contexts/models etc.}

\PvdH{@All: I agree with Markus. It would be nice to say something on this. Maybe we can leave it out of the first arXived version and in the mean time do some research on this. We could also ask Remco van der Hofstad.}

\TM{ \RD{\bf VERY IMPORTANT:} we need to appropriately cite the vdHofstad-vLeeuwaarden-Stegehuis crew. We want to avoid a war. (Just between you and me : I do kind of feel theirs is pretty much non-rigorous though. I am not sure if there is some way to say their work is ``non-rigorous'' without offending them.) }

\subsection{Simulations}\label{ssec:simulations}


\TM{ I think the most important simulation to include is for the ``overall'' clustering coefficient $c(G)$. 
Can someone please add that? \\
(The value of $k$ in Figs 2 and 3 seems a bit arbitrary to me btw.) }

The simulations of this section (see Figures \ref{fig:c2allnu}, \ref{fig:c4allnu} and \ref{fig:ckvariable}) confirm \TM{ ``confirm" is too strong. Simulations ``seem to be in agreement with the formulas". } the formulas from the result section. The simulations were obtained by generating the hyperbolic random graph for a given choice of the parameters and number of repetitions and then taking the average over all simulation instances.
\begin{figure}
    \centering
    \includegraphics[scale=0.6]{figures/sim0123c2nualln1000rep20.pdf}
    \caption{Simulation of $c_{\mathbb{H},n}(k_n)$ for $k_n= 2$, for $\alpha$ varying from $0.5$ to $5$ on the horizontal axis and for $\nu=\frac{1}{2}$ (blue), $\nu=1$ (purple), $\nu=2$ (green); simulations (red dots) with $n=1000$ and 20 repetitions.}
    \label{fig:c2allnu}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[scale=0.6]{figures/sim0128c4nualln10000rep20.pdf}
    \caption{Simulation of $c_{\mathbb{H},n}(k_n)$ for $k_n= 4$, for $\alpha$ varying from $0.5$ to $5$ on the horizontal axis and for $\nu=\frac{1}{2}$ (blue), $\nu=1$ (purple), $\nu=2$ (green); simulations (red dots) with $n=10 000$ and 20 repetitions.}
    \label{fig:c4allnu}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[scale=0.6]{figures/sim1024ck225n10000a0609nu2rep20.pdf}
    \caption{Simulation of $c_{\mathbb{H},n}(k_n)$ for $k_n$ varying from 2 to 26 on the horizontal axis, for $\nu=2$ and $\alpha=0.6$ (green) and $\alpha=0.9$ (blue); simulations (red dots) with $n=10 000$ and 20 repetitions.}
    \label{fig:ckvariable}
\end{figure}

\TM{ We may want to put the source code in an appendix.
In fact RuG now officially requires we deposit the source code in some archive. (gasp.) }