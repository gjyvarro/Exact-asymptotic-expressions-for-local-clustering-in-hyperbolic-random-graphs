\section{Concentration of heights for vertices with degree $k$}\label{sec:concentration_argument}

Many of the computations in this paper will involve integrals whose integrand contains $\Prob{\Po(\mu_n(y)) = k}$, where $\Po(\lambda)$ denotes a Poisson random variable with expectation $\lambda$ and $\mu_n(y) \approx e^{y/2}$. For instance, the expected modified local clustering function $c_\infty(k)$ in $G_\Pcal$ equals
\[
	\frac{\int_0^\infty \Prob{\Po(\Mu{\BallPo{0,y}}) = k} \Delta_{\Pcal}(y) \alpha e^{-\alpha y} \dd y}
	{\int_0^\infty \Prob{\Po(\Mu{\BallPo{0,y}}) = k} \alpha e^{-\alpha y} \dd y},
\]
where $\Mu{\BallPo{0,y}} = \xi_{\alpha,\nu} e^{y/2}$. 

In particular, we already established that the degree of a node at height $y$ in the infinite model is Poisson with mean $\xi_{\alpha,\nu} e^{y/2}$. Since we are mostly interested in those nodes with degree $k_n$ and Poisson random variables are well concentrated around their mean, we would therefore like to be able the restrict integration to intervals where $e^{y/2} \approx k_n$. Moreover, we want to be able to do this for all three Poisson models $G_{\HP,n}$, $G_{\Pcal,n}$ and $G_\Pcal$ as well as simultaneously deal with the case where $k_n \to \infty$ and $k_n = \bigT{1}$. In this section we will establish such a result. We start with a concentration lemma for the infinite model (Lemma~\ref{lem:concentration_argument}) and explain in Remark~\ref{rmk:concentration_argument} how such a result will be used throughout the paper. To obtain similar results for the other two models we have to first analyze the average degree in these models which we do in Sections~\ref{ssec:average_degree_HP_n} and~\ref{ssec:average_degree_P_n}. We conclude this section with a general result that allows us to extend the concentration lemma for the infinite model to the hyperbolic random graph and finite box model. 

\subsection{Concentration argument for the infinite model}

Recall that $\rho(y, k)$ is the probability density function of a Poisson random variable with expectation $\mu_{\alpha, \nu}(B_\Pcal(y)) = \xi_{\alpha,\nu}e^{\frac{y}{2}}$. 
For any sequence $\{k_n\}_{n \ge 1}$, possibly constant, define
\begin{equation}\label{eq:def_kappa_n}
	\kappa_n := \begin{cases}
		\log(n) &\mbox{if } k_n = \bigT{1},\\
		\sqrt{k_n \log(k_n)} &\mbox{else.}
	\end{cases}
\end{equation}
and define further, for any $C > 0$
\begin{equation}\label{eq:def_K_C_set}
	\Kcal_C(k_n) = \left\{p=(x,y) \in \R \times \R_+ : \frac{k_n - C \kappa_n}{\xi_{\alpha,\nu}} \vee 0 \le e^{\frac{y}{2}}
	\le \frac{k_n + C \kappa_n}{\xi_{\alpha,\nu}} \wedge e^{R_n/2} \right\},
\end{equation}
\MS{What does $\vee$ and $\wedge$ mean?}

Note that if $k_n = \Omega(\log(n))$ then $e^{y/2} = \bigT{k_n}$ whenever $p=(x,y) \in \Kcal_C(k_n)$. The next lemma states that for a large class of functions $h(y)$, to compute the integral 
\[
	\int_{\Rcal_n} \rho(y,k_n) h(y) f_{\alpha,\nu}(x,y) \dd x \dd y
\]
it is enough to consider integration of $\Kcal_C(k_n)$ instead of $\Rcal_n$. More precisely, for any $\ell_n = (1 + \smallO{1})k_n$ it is enough to consider $\Kcal_C(\ell_n)$. The flexibility of using $(1+\smallO{1})k_n$ instead of $k_n$ will prove useful in later sections.

\begin{lemma}\label{lem:concentration_argument}
Let $\alpha > \frac{1}{2}$, $\nu > 0$, $\{k_n\}_{n \ge 1}$ be any positive sequence such that $k_n = o(n^{\frac{1}{2\alpha + 1}})$ and let $\ell_n = k_n(1 + \epsilon_n)$, with $\epsilon_n \to 0$. In addition let $\beta < \alpha$ and $h : \R_+ \rightarrow  \R$ be a any function such that $h(y) = g(y)e^{\beta y}$ with $|g(y)|$ uniformly bounded on $\R_+$. Then, if we define for $C > 0$
\[
	\lambda_n^\pm = \ell_n \pm C \kappa_n, 
	\quad a_n^\pm = 2 \log\left(\frac{\lambda_n^\pm}{\xi_{\alpha,\nu}}\right),
\] 
we have
\begin{equation}\label{eq:error_bound_int_rho_not_K}
\begin{aligned}
	\int_{\R_+ \setminus [a_n^-, a_n^+]} \rho(y,k_n) h(y) \alpha e^{-\alpha y} \dd x \dd y
	&= \begin{cases}
		\bigO{\log(n)^{k_n} n^{-\frac{C}{2}}} &\mbox{if } k_n = \bigT{1} \\
		\bigO{k_n^{-(1+C^2)/2}} &\mbox{else.}
	\end{cases}
\end{aligned}
\end{equation}
as $n \to \infty$. 

In particular, if $h_n(y)$ is a function such that $h_n(y) = \bigO{n^{-1} k_n^{s} e^{\beta y}}\rho(y,k_n)$ for some $s \in \R$ and $\beta < \alpha$ as $n \to \infty$. Then for $C > 0$ large enough we have
\[
	\lim_{n \to \infty} \int_{\Rcal_n \setminus \Kcal_{C}(\ell_n)} h_n(y) f_{\alpha,\nu}(x,y) \dd x\dd y = 0,
\]
or equivalently,
\[
	\int_{\Rcal_n} \hspace{-5pt} h_n(y) f_{\alpha,\nu}(x,y) \dd x \dd y
	= (1+\smallO{1}) \int_{\Kcal_{C}(\ell_n)} \hspace{-5pt} h_n(y) f_{\alpha,\nu}(x,y) \dd x \dd y.
\]
\end{lemma}

\begin{proof}
Note that in this case
\[
	\kappa_n := \begin{cases}
		\log(n) &\mbox{if } k_n = \bigT{1},\\
		\sqrt{\ell_n \log(\ell_n)} &\mbox{else.}
	\end{cases}
\]
Then, since $a_n^- < k_n$ and $a_n^+ > k_n$, $\rho_{y}(k_n)$, as a function of $y$, is strictly increasing on $[0,a_n^-]$ and strictly decreasing on $[a_n^+,\infty)$. Therefore, by our assumption on $h(y)$,
\begin{align*}
	&\hspace{-20pt}\int_{\R_+ \setminus [a_n^-, a_n^+]} h(y) \rho_{k_n}(y) 
		\alpha e^{-\alpha y} \dd y\\
    &= \bigO{1} \int_0^{a_n^-} e^{\beta y} \rho_{k_n}(y) \alpha e^{-\alpha y} \dd y 
    	+ \bigO{1}\int_{a_n^+}^{R_n} e^{\beta y} \rho_{k_n}(y) \alpha e^{-\alpha y}x \dd y \\
    &= \bigO{1} \int_0^{a_n^-} \rho_{k_n}(y) e^{-(\alpha-\beta) y} \dd y 
   		+ \bigO{1} \int_{a_n^+}^{R_n} \rho_{k_n}(y) e^{-(\alpha-\beta) y} \dd y\\
   	&\le \bigO{1}\rho(a_n^-,k_n)\int_0^{a_n^-} e^{-(\alpha-\beta) y} \, \dd y
   		+ \bigO{1} \rho(y,a_n^+) \int_{a_n^+}^{R_n} e^{-(\alpha-\beta) y} \, \dd y.
\end{align*}
Note that if $k_n = \bigT{1}$ then, for large enough $n$, the first integral is zero. We conclude that
\begin{equation}\label{eq:concentration_lemma_integral_bound}
\begin{aligned}
	\int_{\R_+ \setminus [a_n^-, a_n^+]} h(y) \rho_{k_n}(y) \alpha e^{-\alpha y} \dd y
	&= \begin{cases}
		\bigO{1} \rho(a_n^+,k_n) &\mbox{if } k_n = \bigT{1},\\
		\bigO{1} \left(\rho(a_n^-,k_n) + \rho(a_n^+,k_n)\right) &\mbox{else.}
	\end{cases}	
\end{aligned}
\end{equation}

We shall now bound the terms $\rho(a_n^\pm,k_n)$, starting with $\rho(a_n^+,k_n)$.  Using that $k! > \sqrt{2\pi} k^{k + 1/2} e^{-k}$ we write
\begin{align*}
	\rho(a_n^+,k_n) &\le \frac{\Mu{\BallPo{a_n^+}}^{k_n}}{k_n!} e^{-\Mu{\BallPo{a_n^+}}} \\
	&\le (2\pi)^{-1/2} k_n^{-1/2} \left(\frac{\Mu{\BallPo{a_n^+}}}{k_n}\right)^{k_n} e^{-(\Mu{\BallPo{a_n^+}} - k_n)}\\
	&= (2\pi)^{-1/2} k_n^{-1/2} 
		e^{-k_n\left(\frac{\Mu{\BallPo{a_n^+}}}{k_n} - 1 - \log\left(\frac{\Mu{\BallPo{a_n^+}}}{k_n}\right)\right)}.
\end{align*}

Let us first consider the case $k_n \to \infty$, in which case $\kappa_n = \sqrt{\ell_n \log(\ell_n)}$. Since 
\[
	\frac{\Mu{\BallPo{a_n^+}}}{k_n} = \frac{\lambda_n^-}{k_n} = 1 + \epsilon_n + C \frac{\kappa_n}{k_n} 
	= 1 + \epsilon_n + C \sqrt{\frac{(1+\epsilon_n)\log((1+\epsilon_n)k_n)}{k_n}},
\]
and $x - \log(1 + x) \sim x^2/2$ as $x \to 0$, we get 
\begin{align*}
	\rho(a_n^+,k_n) 
	&\le \sqrt{2\pi} k_n^{-1/2} 
		e^{-k_n\left(\epsilon_n + C \frac{\kappa_n}{k_n} - \log\left(1 + \epsilon_n + C \frac{\kappa_n}{k_n}\right)\right)}\\
	&\sim (2\pi)^{-1/2} k_n^{-1/2} e^{-\frac{k_n \left(\epsilon_n + C \kappa_n/k_n\right)^2}{2}} \\
	&= \bigO{k_n^{-(1+C^2)/2}},		\numberthis \label{eq:concentration_lemma_bound_an+}
\end{align*}
where for the last line we used that
\[
	-k_n \frac{\left(\epsilon_n + C \kappa_n/k_n\right)^2}{2} = -\frac{C^2}{2}\log(k_n) + \bigT{1}.
\]
Let use now consider the case $k_n = \bigT{1}$. Then $\kappa_n = \log(n)$,
\[
	\frac{\Mu{\BallPo{a_n^+}}}{k_n} = \frac{\lambda_n^+}{k_n} = 1 + \epsilon_n + C \frac{\log(n)}{k_n},
\] 
and hence
\begin{equation}\label{eq:concentration_lemma_bound_an+_constant}
	\rho(a_n^-,k_n) \le \bigO{\log(n)^{k_n} n^{-C}}.
\end{equation}


Note that for $\rho(a_n^-,k_n)$ we only need to consider the case $k_n \to \infty$. A similar analysis as for $a_n^-$ yields
\begin{equation}\label{eq:concentration_lemma_bound_an-}
	\rho(a_n^-,k_n) \le \bigT{1} k_n^{-1/2} e^{-\frac{k_n \left(\epsilon_n - C \kappa_n/k_n\right)^2}{2}} = \bigO{k_n^{-(1+C^2)/2}}.
\end{equation} 


Plugging~\eqref{eq:concentration_lemma_bound_an-}, \eqref{eq:concentration_lemma_bound_an+} and~\eqref{eq:concentration_lemma_bound_an+_constant} into~\eqref{eq:concentration_lemma_integral_bound} yields the result. The second statement immediately follows from the first by choosing a large enough $C$ and observing that
\[
	\int_{\Rcal_n \setminus \Kcal_{C}(\ell_n)} h_n(y) f_{\alpha,\nu}(x,y) \dd x\dd y
	\le n \int_{\R_+ \setminus [a_n^-, a_n^+]} h_n(y) \alpha e^{-\alpha y} \dd y.
\]
\end{proof}

\begin{remark}[Concentration argument]\label{rmk:concentration_argument}
Lemma~\ref{lem:concentration_argument} will prove very useful in the remainder of this paper since we often have to deal with integrands of the form $g_n(y) f_{\alpha,\nu}(x,y)$ where $g_n(y) = \bigO{n^{-1} k_n^{s}}e^{\beta y}\rho(y,k_n)$ for some $s \in \R$ and $\beta < \alpha$. In this case the lemma tells us that for a suitable $C > 0$ we only need to integrate over $\Kcal_C(k_n)$. In order words, we may always assume for $g_n(y)$ (for a penalty of $\smallO{1}$) that $k_n - C \kappa_n \le \xi_{\alpha,\nu} e^{y/2} \le k_n + C \kappa_n$. We will refer to this as a \emph{concentration argument}, e.g. \emph{by a concentration argument}
\[
	\int_{\Rcal_n} g_n(y) f_{\alpha, \nu}(x,y) \dd x \dd y 
	= (1+\smallO{1}) \int_{\Kcal_C(k_n)} g_n(y) f_{\alpha, \nu}(x,y) \dd x \dd y.
\]
\PvdH{It might be a good idea to add one more example of how the concentration argument is used in the paper.}
\end{remark}

\subsection{Average degree in the hyperbolic random graph}\label{ssec:average_degree_HP_n}

Recall that under the coupling between the hyperbolic random graph and the finite box model $|x-x^\prime|_{\pi e^{R_n/2}} \le \Omega(r,r^\prime)$, while the coupling lemma (Lemma~\ref{lem:asymptotics_Omega_hyperbolic}) gives that  
\[
	e^{\frac{1}{2}(y+y^\prime)} - K e^{\frac{3}{2}(y+y^\prime) - R_n} \leq \Omega(r, r^\prime) 
		\leq  e^{\frac{1}{2}(y+y^\prime)} + K e^{\frac{3}{2}(y+y^\prime) - R_n},
\]
for $y + y^\prime < R_n$. This result enables us to determine the measure of a ball around a given point $p=(0,y)$ which will be fairly useful in our subsequent analysis. 

\begin{lemma}\label{lem:average_degree_hyperbolic}
Let $\eps \in (0, 1)$. Then for all $0 \le y \le (1 - \eps)R_n$
\[
	 1 - \phi_{\H,n}^{(1)}(y) - \phi_{\H,n}^{(2)}(y) \le \frac{\Mu{\BallHyp{0,y}}}{\Mu{\BallPo{0,y}}} 
	 \le 1 - \phi_{\H,n}^{(1)}(y) + \phi_{\H,n}^{(2)}(y),
\]
where
\[
	\phi_{\H,n}^{(1)}(y) = \frac{2\alpha - 1 - 4\pi}{4\pi} e^{-(\alpha - \frac{1}{2})(R_n - y)} 
		+ \left(\alpha - \frac{1}{2}\right)\pi e^{-(\alpha - \frac{1}{2})R_n - y/2},
\]
and
\[
	\phi_{\H,n}^{(2)}(y) = 
	+ \begin{cases}
			\frac{(2\alpha - 1)K}{3 - 2\alpha}\left(e^{-(\alpha - \frac{1}{2})(R_n - y)} - e^{-(R_n - y)}\right)
			&\mbox{if } 1/2 < \alpha < 3/2,\\
			\frac{(2\alpha -1)K}{2} (R_n - y)e^{-(R_n - y)} &\mbox{if } \alpha = 3/2,\\
			\frac{(2\alpha - 1)K}{2\alpha - 3} \left(e^{-(R_n - y)} - e^{-(\alpha - \frac{1}{2})(R_n - y)}\right)
			&\mbox{if } \alpha > 3/2.
		\end{cases}
\]
\end{lemma}

\begin{proof}
We will split the computation of $\Mu{\BallHyp{0,y}}$ into the cases $y^\prime > R_n - y$ and $y^\prime \le R_n - y$ where for the latter we utilize Lemma~\ref{lem:asymptotics_Omega_hyperbolic}. Recall that $\Mu{\BallPo{0,y}} = \xi_{\alpha,\nu} e^{y/2}$ where $\xi_{\alpha,\nu} = \frac{4\alpha\nu}{\pi(2\alpha - 1)}$.

By~\eqref{eq:tail_inclusion_hyperbolic_ball}, we have $\BallHyp{(0,y)} \cap \Rcal_n ([R_n - y, R_n])= \Rcal_n([R_n-y,R_n])$. 
Thus, 
\begin{align*}
	&\hspace{-30pt}\Mu{\BallHyp{(0,y)} \cap \Rcal_n ([R_n - y, R_n])} \\
	&= \int_{R_n - y}^{R_n} \int_{I_n} f_{\alpha,\nu}(x^\prime, y^\prime) \dd x^\prime \dd y^\prime
		= \nu \alpha e^{R_n/2}\left(e^{-\alpha(R_n - y)} - e^{-\alpha R_n}\right)\\
	&= \Mu{\BallPo{0,y}} \frac{2\alpha - 1}{4\pi} \left( e^{-(\alpha - \frac{1}{2})(R_n - y)}
		- e^{-(\alpha - \frac{1}{2})R_n - y/2}\right) \numberthis \label{eq:mu_hyperbolic_ball_part_1}
\end{align*}


Next we will establish upper and lower bounds on $\Mu{\BallHyp{(0,y)} \cap\Rcal_n[(0,R_n-y)]}$. Using Lemma~\ref{lem:asymptotics_Omega_hyperbolic} we have 
\begin{align*}
	\Mu{\BallHyp{(0,y)} \cap\Rcal_n[(0,R_n-y)]} 
	&\le \frac{2\nu \alpha}{\pi} \int_{0}^{R_n - y} \left(e^{\frac{y + y^\prime}{2}} + K e^{\frac{3}{2}(y + y^\prime) - R_n}\right)
		e^{-\alpha y^\prime} \dd y^\prime\\
	&= \Mu{\BallPo{0,y}}\left(1 - e^{-(\alpha - \frac{1}{2})(R_n - y)}\right) \\
	&\hspace{10pt}+ \frac{2\nu \alpha}{\pi} K e^{\frac{3 y}{2} - R_n}\int_0^{R_n-y} e^{(\frac{3}{2} - \alpha)y^\prime} \dd y^\prime
\end{align*}
The last integral depends on the value of $\alpha$,
\[
	\int_0^{R_n-y} e^{(\frac{3}{2} - \alpha)y^\prime} \dd y^\prime
	= \begin{cases}
		\frac{2}{3 - 2\alpha}\left(e^{(\frac{3}{2} - \alpha)(R_n - y)} - 1\right) &\mbox{if } 1/2 < \alpha < 3/2,\\
		R_n - y &\mbox{if } \alpha = 3/2,\\
		\frac{2}{2\alpha-3}\left(1 - e^{-(\alpha - \frac{3}{2})(R_n - y)}\right) &\mbox{if } \alpha > 3/2.
	\end{cases}
\]
Therefore we get
\begin{align*}
	&\hspace{-30pt}\frac{2\nu \alpha}{\pi} K e^{\frac{3 y}{2} - R_n}
		\int_0^{R_n-y} e^{(\frac{3}{2} - \alpha)y^\prime} \dd y^\prime\\
	&= \Mu{\BallPo{0,y}}\begin{cases}
		\frac{(2\alpha - 1)K}{3 - 2\alpha}\left(e^{-(\alpha - \frac{1}{2})(R_n - y)} - e^{-(R_n - y)}\right)
		&\mbox{if } 1/2 < \alpha < 3/2,\\
		\frac{(2\alpha -1)K}{2} (R_n - y)e^{-(R_n - y)} &\mbox{if } \alpha = 3/2,\\
		\frac{(2\alpha - 1)K}{2\alpha - 3} \left(e^{-(R_n - y)} - e^{-(\alpha - \frac{1}{2})(R_n - y)}\right)
		&\mbox{if } \alpha > 3/2.
	\end{cases}\\
	&= \Mu{\BallPo{0,y}} \phi_{\H,n}^{(2)}(y)
\end{align*}
and hence
\[
	\Mu{\BallHyp{(0,y)} \cap\Rcal_n[(0,R_n-y)]} = \Mu{\BallPo{0,y}}\left(1 - e^{-(\alpha - \frac{1}{2})(R_n - y)} 
	+ \phi_{\H,n}^{(2)}(y)\right).
\]
Combining this with~\eqref{eq:mu_hyperbolic_ball_part_1} yields the required upper bound.

The lower bound follows by observing that the only difference with the above computations is the change of sign in front of 
\[
	\frac{2 \nu \alpha}{\pi} K e^{\frac{3 y}{2} - R_n}\int_0^{R_n-y} e^{(\frac{3}{2} - \alpha)y^\prime} \dd y^\prime.
\]
\end{proof}

\subsection{Average degree in the finite box model}\label{ssec:average_degree_P_n}

For the finite box model $G_{\Pcal,n}(\alpha, \nu)$ we obtain a similar result for the average degree.

\begin{lemma}\label{lem:average_degree_P_n}
For all $p \in \Rcal_n$, such that $y > 2\log(\pi/2)$,
\[
	\mu_{\alpha,\nu}(B_{\Pcal,n}(p)) = \mu_{\alpha,\nu}(B_\Pcal(p))\left(1 - \phi_n(y)\right)
\]
where $\phi_n(y) \ge 0$ is given by
\[
	\phi_n(y) = \left(\frac{\pi}{2}\right)^{-(2\alpha - 1)}e^{-(\alpha-\frac{1}{2})(R_n - y)}
	- \frac{(2\alpha - 1)\pi}{4\alpha}\left(\left(\frac{\pi}{2}\right)^{-2\alpha} 
	e^{-(\alpha - \frac{1}{2})(R_n - y)} - e^{-(\alpha - \frac{1}{2})R_n - \frac{y}{2}}\right).
\]
On the other hand, if $y \le 2 \log(\pi/2)$ then
\[
	\mu_{\alpha,\nu}(B_{\Pcal,n}(p)) = \mu_{\alpha,\nu}(B_\Pcal(p))\left(1 - e^{-(\alpha - \frac{1}{2})R_n}\right).
\]
\end{lemma}

\begin{proof}
First note that since we have identified the boundaries of $[-\frac{\pi}{2}e^{\frac{R_n}{2}}, \frac{\pi}{2}e^{\frac{R_n}{2}}]$ we can assume, without loss of generality, that $p = (0,y)$. We then have that
\[
	\BallPon{p} = \left\{p^\prime \in \Rcal_n \, : \, |x^\prime|_n \le e^{\frac{y + y^\prime}{2}}\right\},
\] 
whose boundaries given by the equations $x^\prime = \pm e^{\frac{y+y^\prime}{2}}$ intersect the left and right boundaries of $[-\frac{\pi}{2}e^{\frac{R_n}{2}}, \frac{\pi}{2}e^{\frac{R_n}{2}}]$ at height
\[
	h(y) = R_n + 2 \log\left(\frac{\pi}{2}\right) - y.
\]
Therefore, if $y \le 2 \log(\pi/2)$ this intersection occurs above the height $R_n$ of the box $\Rcal_n$ while in the other case the full region of the box above $h(y)$ is connected to $p$. 

We will first consider the case where $y > 2 \log(\pi/2)$. Recall that $\mu_{\alpha,\nu}(B_\Pcal(p)) = \xi_{\alpha,\nu}e^{\frac{y}{2}}$ where $\xi_{\alpha,\nu} = \frac{4\alpha \nu}{(2\alpha - 1)\pi}$. Then, after some simple algebra, we have that
\begin{align*}
	\mu_{\alpha,\nu}(B_{\Pcal,n}(p))
	&= \int_0^{h(y)} \int_{-\frac{\pi}{2}e^{\frac{R_n}{2}}}^{\frac{\pi}{2}e^{\frac{R_n}{2}}} 
		\ind{|x^\prime| \le e^{\frac{y+y^\prime}{2}}} f_{\alpha,\nu}(x^\prime,y^\prime) \, dx^\prime \, dy^\prime\\
	&\hspace{10pt}+ \int_{h(y)}^{R_n} \int_{-\frac{\pi}{2}e^{\frac{R_n}{2}}}^{\frac{\pi}{2}e^{\frac{R_n}{2}}} 
		f_{\alpha,\nu}(x^\prime,y^\prime) \, dx^\prime \, dy^\prime\\
	&= \frac{2 \alpha \nu}{\pi} e^{\frac{y}{2}} \int_0^{h(y)} e^{-(\alpha - \frac{1}{2})y^\prime} \, dy^\prime
		+ \alpha \nu e^{\frac{R_n}{2}} \int_{h(y)}^{R_n} e^{-\alpha y^\prime} \, dy^\prime \\
	&= \xi_{\alpha,\nu} e^{\frac{y}{2}}\left(1 - \left(\frac{\pi}{2}\right)^{-(2\alpha - 1)} 
		e^{-(\alpha - \frac{1}{2})(R_n - y)}\right)\\
	&\hspace{10pt}+ \nu e^{\frac{R_n}{2}}\left(\left(\frac{\pi}{2}\right)^{-2\alpha} e^{-\alpha(R_n - y)} 
		- e^{-\alpha R_n}\right)\\
	&= \mu_{\alpha,\nu}(B_\Pcal(p))\left(1 - \phi_n(y)\right).
\end{align*}
Since, for all $\alpha > \frac{1}{2}$,
\[
	\left(\frac{\pi}{2}\right)^{-(2\alpha - 1)} \ge \frac{(2\alpha - 1)\pi}{4\alpha} \left(\frac{\pi}{2}\right)^{-2\alpha}
\]
it follows that $\phi_n(y) \ge 0$.

When $y \le 2 \log(\pi/2)$ we have
\begin{align*}
	\mu_{\alpha,\nu}(B_{\Pcal,n}(p))
	&= \int_0^{R_n} \int_{-\frac{\pi}{2}e^{\frac{R_n}{2}}}^{\frac{\pi}{2}e^{\frac{R_n}{2}}} 
		\ind{|x^\prime| \le e^{\frac{y+y^\prime}{2}}} f_{\alpha,\nu}(x^\prime,y^\prime) \, dx^\prime \, dy^\prime\\
	&= \frac{2 \alpha \nu}{\pi} e^{\frac{y}{2}} \int_0^{R_n} e^{-(\alpha - \frac{1}{2})y^\prime} \, dy^\prime\\
	&= \mu_{\alpha,\nu}(B_\Pcal(p))\left(1 - e^{-(\alpha - \frac{1}{2})R_n}\right).
\end{align*}
\end{proof}

\subsection{Concentration argument for hyperbolic and finite box model}

With the results on the average degrees in the two finite Poisson models, we can now establish a result that allows us to extend the concentration argument to the case where instead of $\rho(y,k)$ we consider the degree distributions $\rho_{\HP,n}(y,k)$ and $\rho_{n}(y,k)$ in $G_{\HP,n}$ and $G_{\Pcal,n}$, respectively. The lemma is stated in a more general form, to make it applicable to slightly other cases later in the paper. To understand the general conditions presented in the statement of the lemma we write $\mu_n(y) = \Mu{\BallPon{y}}$ and $\mu(y) = \Mu{\BallPo{y}}$ and recall the result from Lemma~\ref{lem:average_degree_P_n}
\[
	\mu_n(y) = \mu(y)(1 - \phi_n(y)).
\]
For the error function $\phi_n(y)$ we note that for any $0 < \varepsilon < 1$, as $n \to \infty$, 
\[
	\sup_{0 \le y \le (1-\varepsilon)R_n} \phi_n(y) = \smallO{1},
\]
while
\[
	\frac{\partial \phi_n(y)}{\partial y} = \bigT{\phi_n(y)}.
\]
These are exactly the crucial properties of the error function that allow for a concentration argument. Moreover, if the integrand also contains a function $g_n(y)$ that is reasonably nice, we can replace the $\rho_{n}(y,k_n)$ in the integrand by $\rho(y,k_n)$. In particular, the triangle counting function $\Delta_{\Pcal}(y)$ will satisfy these conditions, see Section~\ref{sec:clustering_Pn_to_P}.

\begin{lemma}\label{lem:concentration_argument_rho_approximation}
Let $\alpha > \frac{1}{2}, \nu > 0$, $k_n$ be any positive sequence such that $k_n = o(n^{\frac{1}{2\alpha + 1}})$, $\ell_n = (1 + \epsilon_n)k_n$, with $\epsilon_n \to 0$ and let $\Kcal_{C}(\ell_n)$ be defined as in~\eqref{eq:def_K_C_set}. In addition, define $\hat{\rho}_n(y,k) = \Prob{\Po(\hat{\mu}_n(y)) = k}$, where $\hat{\mu}_n(y)$ is any function satisfying 
\[
	\Mu{\BallPo{0,y}}(1 - \phi_n^{(1)}(y)) \le \hat{\mu}_n(y) \le \Mu{\BallPo{0,y}}(1 + \phi_n^{(2)}(y))
\]
where $\phi_n^{(i)} : \R_+ \to \R_+$ are such that $\sup_{0 \le y \le (1-\varepsilon)R_n} \phi_n(y)^{(i)} = \smallO{1}$ for any $0 < \varepsilon < 1$ and $\partial \phi_n^{(i)}(y)/\partial y = \bigT{\phi_n^{(i)}(y)}$. Then the following holds for some $C > 0$ large enough:
\begin{enumerate}
\item for any sequence of functions $h_n(y) : \R_+ \mapsto \R$ such that $h_n(y) = \hat{\rho}_n(y,k_n) \bigO{k_n^s e^{\beta y}}$ for some $s < 2\alpha(2\alpha + 1)$ and $\beta < \alpha$, we have,
\[
	\int_{\Rcal_n} h_n(y) f_{\alpha,\nu}(x,y) \dd x \dd y 
	= (1 + o(1)) \int_{\Kcal_{C}(\ell_n)} h_n(y) f_{\alpha,\nu}(x,y) \dd x \dd y.
\]
\item for any uniformly bounded function $h : \R_+ \mapsto \R$,
\[
	\int_{\Rcal_n} h(y) \hat{\rho}_n(y,k_n) f_{\alpha,\nu}(x,y) \dd x \dd y 
	= (1 + o(1)) \int_{\Rcal_n} \rho(y,k_n) h(y) f_{\alpha,\nu}(x,y) \dd x \dd y.
\]
\end{enumerate}
\end{lemma}

\begin{proof}
Similarly to the proof of Lemma~\ref{lem:concentration_argument} we define 
\[
	\lambda_n^\pm = \ell_n \pm C \kappa_n, \quad \text{and} \quad a_n^\pm = 2 \log\left(\frac{\lambda_n^\pm}{\xi_{\alpha,\nu}}\right).
\] 
In addition we fix $\varepsilon > 0$ such that $\varepsilon < \min\{1- \frac{s}{2\alpha(2\alpha + 1)}, 1\}$. Then since $h_n(y) = \hat{\rho}_n(y,k_n) \bigO{k_n^s}$ we have
\begin{align*}
	k_n^s \int_{(1-\varepsilon)R_n}^{R_n} \hat{\rho}_n(y,k_n) e^{-\alpha y} \dd y
	&= \bigO{1} \hat{\rho}_n((1-\varepsilon)R_n,k_n) k_n^s e^{-\alpha(1-\varepsilon)R_n} \\
	&= \bigO{\hat{\rho}_n((1-\varepsilon)R_n,k_n) k_n^s n^{-2\alpha(1 - \varepsilon)}} \\
	&= \smallO{ \hat{\rho}_n((1-\varepsilon)R_n,k_n) n^{\frac{s}{2\alpha + 1} - 2\alpha(1-\varepsilon)}} = \smallO{1},
\end{align*}
where the last step follows by our choice of $\varepsilon$. Hence it is enough to prove both statement on $\Rcal_n((1-\varepsilon)R_n, R_n)$ instead of $\Rcal_n$.
\PvdH{This is where we use that $s < 2\alpha(2\alpha + 1)$. I think this can be removed by using a more careful bound but I am not sure this is needed.}

For simplicity we write $\mu(y) := \mu_{\alpha,\nu}\left(\BallPo{y}\right)$. For both statements we shall make a change of variables $y \to z$ such that
\begin{equation}\label{eq:concentration_argument_variable_change}
	\hat{\mu}_n(y) = \mu(z),
\end{equation}
which implies
\[
	\mu^{-1}(y) = 2 \log\left(\frac{y}{\xi_{\alpha,\nu}}\right),
\]
Observe that since
\[
	\mu^{-1}(y) = 2 \log\left(\frac{y}{\xi_{\alpha,\nu}}\right),
\]
is strictly monotonic in $y$, we have, by our assumptions on $\hat{\mu}_n$, that
\begin{equation}\label{eq:concentration_argument_z_upper_bound}
	z(y) \le 2 \log\left(\frac{\mu(y)}{\xi_{\alpha,\nu}}\right) + 2 \log\left(1 + \phi_n^{(2)}(y)\right)
	= y + 2 \log\left(1 + \phi_n^{(2)}(y)\right),
\end{equation}
and similarly
\begin{equation}\label{eq:concentration_argument_z_lower_bound}
	z(y) \ge y + 2 \log\left(1 - \phi_n^{(1)}(y)\right) \
\end{equation}
Denote by $g^\prime(z)$ the derivative of a function $g$. Then by assumption $(\phi_n^{(i)})^\prime(z) = \bigT{\phi_n^{(i)}(z)}$ and hence
\begin{align*}
	\hat{\mu}_n^\prime(z) = \mu^\prime(z)\left(1 + o(1)\right).
\end{align*}

\paragraph{Proof of statement 1.} By our assumption on $h_n(y)$ it is enough to show that for sufficiently large $C > 0$,
\[
	\lim_{n \to \infty} k_n^s \int_0^{a_n^-} \hat{\rho}_n(y,k_n) e^{(\beta-\alpha) y} \dd y = 0,
\]
and
\[
	\lim_{n \to \infty} k_n^s \int_{a_n^+}^{(1-\varepsilon)R_n} \hat{\rho}_n(y,k_n) e^{(\beta-\alpha) y} \dd y = 0.
\]
We shall prove the last statement. The first statement follows using similar computations.

By our assumption on $\phi_n^{(2)}(y)$ we have for all $a_n^+ < y \le (1 - \varepsilon)R_n$,
\begin{equation}
	e^{(\beta-\alpha) y} = (1 + o(1))e^{(\beta-\alpha) z}.
\end{equation}
Therefore, with this change of variables~\eqref{eq:concentration_argument_variable_change}, using that $\hat{a}_n^+ := z(a_n^+) = a_n^+(1 + \smallO{1})$ and $z((1-\varepsilon)R_n) = (1-\varepsilon)R_n(1+\smallO{1})$, we have
\begin{align*}
	k_n^s \int_{a_n^+}^{(1-\varepsilon)R_n} \hat{\rho}_n(y,k_n) e^{(\beta - \alpha)y} \dd y
	&=  k_n^s \int_{a_n^+}^{(1-\varepsilon)R_n} \Prob{\mathrm{Po}\left(\hat{\mu}_n(y) = k_n\right)} 
		e^{(\beta-\alpha) y} \dd y\\
	&= (1 + \smallO{1})k_n^s\int_{a_n^+}^{(1-\varepsilon)R_n} \Prob{\mathrm{Po}\left(\mu(z) = k_n\right)}
		e^{(\beta-\alpha) z} \frac{\hat{\mu}_n^\prime(z)}{\mu^{\prime}(z)} \, dz \\
	&\le (1 + \smallO{1})k_n^s\int_{a_n^+}^{R_n} \rho(y,k_n)
			e^{(\beta-\alpha) z} \dd z.
\end{align*}
The result then follows by Lemma~\ref{lem:concentration_argument}.

\paragraph{Proof of statement 2.}

The proof of the second statement follows the same line of reasoning as above. First note that by Lemma~\ref{lem:concentration_argument} and the first statement, it is enough to show that
\[
	\int_{a_n^-}^{a_n^+} \rho_n(y,k_n) h(y) e^{-\alpha y} \dd y
	= \left(1 + o(1)\right) \int_{a_n^-}^{a_n^+} \rho(z,k_n) h(z) e^{-\alpha z} \dd z.
\] 
For this we observe that~\eqref{eq:concentration_argument_z_upper_bound} and~\eqref{eq:concentration_argument_z_lower_bound} imply that there exists a $b_n \to 0$ such that $y = z(y) + b_n$, uniformly for $a_n^- \le y \le a_n^+$. Hence, if we define $g_n(z) = h(z + b_n)$, then $g_n(z)$ is uniformly bounded and converges point wise to $h(z)$. Therefore, with the change of variables~\eqref{eq:concentration_argument_variable_change} we get
\begin{align*}
	\int_{a_n^-}^{a_n^+} \rho_n(y,k_n) h(y) e^{-\alpha y} \dd y
	&=  \int_{a_n^-}^{a_n^+} \Prob{\Po(\hat{\mu}_n(y)) = k_n} h(y) e^{-\alpha y} \dd y\\
	&= (1 + \smallO{1})\int_{a_n^-}^{a_n^+}  \Prob{\Po(\mu(z))}
		g_n(z) e^{-\alpha z} \frac{\hat{\mu}_n^\prime(z)}{\mu^{\prime}(z)} \dd z\\
	&= (1 + \smallO{1})\int_{a_n^-}^{a_n^+}  \Prob{\Po(\mu(z))}
		g_n(z) e^{-\alpha z} \dd z\\
	&= \left(1 + o(1)\right) \int_{a_n^-}^{a_n^+} \rho(z,k_n) h(z) e^{-\alpha z} \dd z.
\end{align*}
where the last line follow by dominated convergence.
\end{proof}

Observe that Lemma~\ref{lem:average_degree_P_n} implies that $\Mu{\BallHyp{(0,y)}}$ satisfies the requirements in Lemma~\ref{lem:concentration_argument_rho_approximation} while Lemma~\ref{lem:average_degree_hyperbolic} states that this holds for $\Mu{\BallPon{0,y}}$. We therefore have the following important Corollary.

\begin{corollary}\label{cor:concentration_argument_other_models}
Let $\hat{\rho}_n(y,k)$ be any of the two distribution functions $\rho_{\HP,n}(y,k)$ and $\rho_{n}(y,k)$. Then, for any function $g_n(y)$ such that $g_n(y) = \bigO{n^{-1} k_n^{s} e^{\beta y}}\hat{\rho}_n(y,k_n)$, as $n \to \infty$, for some $s < 2\alpha(2\alpha + 1)$ and $\beta < \alpha$,
\[
	\int_{\Rcal_n} g_n(y) f_{\alpha,\nu}(x,y) \dd x \dd y
	= (1+\smallO{1})\int_{\Kcal_C(k_n)} g_n(y) f_{\alpha,\nu}(x,y) \dd x \dd y,
\]
for some $C > 0$ large enough.
\end{corollary}

In particular we conclude that, similarly to the infinite limit model, concentration arguments as described in Remark~\ref{rmk:concentration_argument} can be applied in the case of the hyperbolic random graph and finite box model.

%\begin{lemma}
%\[
%	\Prob{D_{\HP,n}= k_n} = (1 + \smallO{1})\Prob{D_\Pcal = k_n}
%\]
%\end{lemma}

