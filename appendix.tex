\begin{appendices}

\section{Meijer's G-function}\label{sec:Meijer_G_functions}

Recall that $\Gamma(z)$ denotes the Gamma function. Let $p, q, m, \ell$ be four integers satisfying $0 \le m \le q$ and $0 \le \ell \le p$ and consider two sequences ${\bf a}_p = \{a_1, \dots, a_p\}$ and ${\bf b}_q = \{b_1, \dots, b_q\}$ of reals such that $a_i - b_j$ is not a positive integer for all $1 \le i \le p$ and $1 \le j \le q$ and $a_i - a_j$ is not an integer for all distinct indices $1 \le i, j \le p$. Then, with $\iota$ denoting the complex unit, Meijer's G-Function~\cite{meijer1946gfunction} is defined as
\begin{equation}\label{eq:def_Meijer_G_function}
	\MeijerGnew{m}{\ell}{p}{q}{{\bf a}}{{\bf b}}{z} 
	= \frac{1}{2 \pi \iota} \int_L 
	\frac{\prod_{j = 1}^{m}\Gamma(b_j - t) \prod_{j = 1}^\ell \Gamma(1 - a_j + t)}
	{\prod_{j = m + 1}^{q}\Gamma(1 - b_j + t) \prod_{j = \ell + 1}^p\Gamma_n(a_j-t)} \, z^t \dd t,
\end{equation}
where the path $L$ is an upward oriented loop contour which separates the poles of the function $\prod_{j = 1}^{m}\Gamma(b_j - t)$ from those of $\prod_{j = 1}^n \Gamma(1 - a_j + t)$ and begins and ends at $+\infty$ or $-\infty$.

The Meijer's G-Function is of a very general nature and has relation to many known special functions such as the Gamma function and the generalized hypergeometric function. For more details, such as many identities for $\MeijerGnew{m}{\ell}{p}{q}{{\bf a}}{{\bf b}}{z}$ see \cite{gradshteyn2015table,luke2014mathematical}.

For our purpose we need the following identity which follows from an Mellin transform operation.

\begin{lemma}\label{lem:gamma_meijer_G}
For any $a\in \R$ and $\xi, s>0$,
$$\Gamma^+(-a-1,\xi/s) = \MeijerGnew{2}{0}{1}{2}{1}{-a-1,0}{\frac{\xi}{s}}$$
\end{lemma}
\begin{proof}
Let $x>0$ and $q\in\R$ and note that as the $\Gamma$-function is the Mellin transform of $e^{-x}$, by the inverse Mellin transform formula, we have $e^{-x}=\frac{1}{2\pi i}\int_{c-i\infty}^{c+i\infty} \Gamma(p)x^{-p}dp$ for $c>0$ (see \cite[p.196]{davies2012integral}). Applying the change of variable $p(r)=q-r$ yields $e^{-x}=\frac{1}{2\pi i}\int_{c+q-i\infty}^{c+q+i\infty} \Gamma(q-r) x^{r-q}dr$, then multiplying both sides with $-x^{q-1}$ gives $-x^{q-1}e^{-x} = -\frac{1}{2\pi i}\int_{c+q-i\infty}^{c+q+i\infty} \Gamma(q-r) x^{r-1}dr$. Now, integrating both sides gives $\int_x^\infty t^{q-1}e^{-t}dt = \frac{1}{2\pi i}\int_{c+q-i\infty}^{c+q+i\infty}\frac{\Gamma(q-r)}{-r}x^r dr$. On the left-hand side is the incomplete gamma function and on the right-hand side with using $-r= \frac{\Gamma(1-r)}{\Gamma(-r)}$ is the Meijer $G$-function, i.e. $\Gamma^+(q,x)=\MeijerG{2}{0}{1}{2}{1}{q,0}{x}$. The claim follows by plugging in $q=-a-1$ and $x=\frac{\xi}{s}$.
\end{proof}

%\section*{Properties of $G_{\H,n}(\alpha,\nu)$}
%
%\PvdH{@Markus: is the lemma below only needed to prove the second lemma? If so than we can remove it because the statements of the second lemma have already been established in other parts of the paper.}
%
%\begin{lemma}[Intensity measure of the intersection of two neighbourhood balls]\label{lem:mu3}
%Consider the Poisson hyperbolic random graph conditioned on having two vertices with degree $k$ at heights $y_1, y_2 \in [y_{k,-},y_{k,+}]$ and horizontal distance $d$. Then the expected number of vertices in the intersection of their neighbourhood balls in the infinite limit model is $\Theta(k^{2\alpha}d^{1-2\alpha})$.
%\end{lemma}
%\begin{proof}
%Denote the two vertices by $v$ and $w$.
%If the neighbourhood balls of $v$ and $w$ intersect at the minimal height $y_h$, then $d = e^{\frac{y_h+y_1}{2}}+e^{\frac{y_h+y_2}{2}}$.
%
%Due to our assumption $y_1, y_2 \in [y_{k,-},y_{k,+}]$, this condition implies $d \leq 2 e^{\frac{y_h+y_{k,+}}{2}}$. This implies $y_h \geq 2\ln \frac{d}{2} - y_{k,+}$. (In a similar fashion, we infer that $y_h \leq 2\ln \frac{d}{2} -y_{k,-}$.)
%
%Therefore, using $y_1,y_2 \in [y_{k,-}, y_{k,+}]$ and $e^{\frac{y_1}{2}}, e^{\frac{y_2 }{2}} \sim k$, $e^{\frac{y_h}{2}} \sim \frac{d}{k}$:
%\begin{align*}
%\mu_3 &= \int_{y_h}^\infty (e^{\frac{y_1+y}{2}}+e^{\frac{y_2+y}{2}} - d) \alpha e^{-\alpha y} dy =\Theta(k)\frac{\alpha}{\alpha-\frac{1}{2}} e^{(\frac{1}{2}-\alpha) y_h} +d e^{-\alpha y_h} \\
%&= \Theta(k (\frac{d}{k})^{1-2\alpha} +d(\frac{d}{k})^{-2\alpha}) = \Theta(k^{2\alpha} d^{1-2\alpha})
%\end{align*}
%\end{proof}
%
%The following proposition establishes several properties on the number of vertices with degrees $k_n$ in the Poisson hyperbolic graph $G_{\widetilde{\H},n}(\alpha,\nu)$.
%
%\begin{proposition}
%Let $\alpha > \frac{1}{2}$, $\nu > 0$, consider the Poisson hyperbolic random geometric graph $G_{\widetilde{\H},n}(\alpha,\nu)$ and let $N_{\widetilde{\H},n}(k_n)$ be the number of degree $k_n$ vertices.
%\begin{enumerate}
%	\item If $2 \leq k_n \leq n-1$ such that $k_n \rightarrow \infty$ as $n \rightarrow \infty$, then $\E[N_{\widetilde{\H},n}(k_n)] \sim 2\alpha \xi^{2\alpha} n k_n^{-(2\alpha +1)}$ as $n \rightarrow \infty$.
%	
%	\item In particular if $k_n \gg n^{\frac{1}{2\alpha+1}}$, then $N_{\widetilde{\H},n}(k_n) = 0$ w.h.p.
%	
%	\item Furthermore, if $k_n \ll n^{\frac{1}{2\alpha+1}}$, then $\frac{\E[N_{\widetilde{\H},n}(k_n)^2]}{\E[N_{\widetilde{\H},n}(k_n)]^2} \rightarrow 1$, so in particular in this case w.h.p. $$N_{\widetilde{\H},n}(k_n) = (1+o(1))\E[N_{\widetilde{\H},n}(k_n)]$$ and $N_{\widetilde{\H},n}(k_n) > 0$ w.h.p.
%\end{enumerate}
%\end{proposition}
%
%\begin{proof}
%\PvdH{The results from this lemma are already stated elsewhere but I kept the lemma to not break any references. It will be cleaned next round.}
	
%(i): First of all, we note that the degree of a vertex at height $y \in [0,R_n]$ is a Poisson random variable with expectation $\lambda_n(y) := \mu_{\alpha,\nu}(\BallHyp{y})$. As $N_{\widetilde{\H},n}(k_n) = \sum_{v\in V(G_{\widetilde{\H},n})} \1_{\{D_{\widetilde{\H},n}(v) = k_n\}}$, by the Palm-Mecke formula,
%\begin{align*}
%	\Exp{N_{\widetilde{\H},n}(k_n)} &= \int_{\Rcal_n} \Prob{D_{\widetilde{\H},n}(y) = k_n} 
%		f_{\alpha,\nu}(x,y) \dd x \dd y \\
%	&= n \int_0^{R_n} \Prob{D_{\widetilde{\H},n}(y) = k_n} \alpha e^{-\alpha y} \dd y 
%		= n \int_0^{R_n} \Prob{\Po(\lambda_n(y))=k_n} \alpha e^{-\alpha y}dy.
%\end{align*}
%
%Now, we would like to replace the $\mu_{y}^{(n)}$ in the Poisson probability above by $\xi e^{\frac{y}{2}}$. The argument for this splits into the following parts: $\mu_{y}^{(n)} \rightarrow \xi e^{\frac{y}{2}}$ as $n\rightarrow \infty$, together with the concentration resp. range of $y$, it follows that $\Pee(Po(\mu_y^{(n)})=k_n) \sim \Pee(Po(\xi e^{\frac{y}{2}})=k_n)$; finally the asymptotic equivalence of the integrands implies the asymptotic equivalence of the integrals. 
%
%The convergence of $\mu_{y}^{(n)}$ to $\xi e^{\frac{y}{2}}$ is by  Lemma \ref{lem:expecdeg}. \PvdH{I do not follow this argument. By Lemma \ref{lem:expecdeg}, for some fixed $\varepsilon > 0$,
%\[
%	\mu_y^{(n)} = \xi e^{y/2}\left(1 \pm \varepsilon\right).
%\]
%Hence
%\begin{align*}
%	\frac{\Pee(Po(\mu_y^{(n)})=k_n)}{\Pee(Po(\xi e^{\frac{y}{2}})=k_n)} 
%	&= \left(\frac{\mu_y^{(n)}}{\xi e^{\frac{y}{2}}}\right)^{k_n} e^{-\mu_y^{(n)}+\xi e^{y/2}} \\
%	&= (1\pm \varepsilon)^{k_n} e^{-\varepsilon(k_n \pm C\sqrt{k_n \log(k_n)})}\\
%	&= e^{\pm \varepsilon k_n + \bigO{\varepsilon^2 k_n} -\varepsilon(k_n \pm C\sqrt{k_n \log(k_n)})}\\
%	&= e^{\mp \varepsilon C\sqrt{k_n \log(k_n)} + \bigO{\varepsilon^2 k_n}}.
%\end{align*}
%This will not converge to $1$ for any fixed $\varepsilon$ and hence we need to make sure that $\varepsilon \ll \sqrt{k_n \log(k_n)}$. Can we do that in Lemma \ref{lem:expecdeg}?
%}
%
%
%\PvdH{By a concentration argument we can assume that $\xi e^{\frac{y}{2}} \in [k_n-c\sqrt{k_n \log k_n},k_n+c\sqrt{k_n \log k_n}]$ (for a sufficiently large $c$). The quotient of the Poisson probabilities is $\frac{\Pee(Po(\mu_y^{(n)})=k_n)}{\Pee(Po(\xi e^{\frac{y}{2}})=k_n)} = (\frac{\mu_y^{(n)}}{\xi e^{\frac{y}{2}}})^{k_n} e^{\mu_y^{(n)}-\mu_y}$.} The second factor converges to one and for the first factor, we have $(\frac{\mu_y^{(n)}}{\xi e^{\frac{y}{2}}})^{k_n} = (1+\frac{\mu_y^{(n)}-\xi e^{\frac{y}{2}}}{\xi e^{\frac{y}{2}}})^{k_n} \leq e^{(\mu_y^{(n)}-\xi e^{\frac{y}{2}})\frac{k_n}{\xi e^{\frac{y}{2}}}}$. (I think we also need a lower bound (e.g. 1) here, but I do not see how to get it, because $\mu_y^{(n)}$ is monotone increasing to $\xi e^{\frac{y}{2}}$). Due to the restricted range of $y$, $\frac{k_n}{\xi e^{\frac{y}{2}}}$ converges to one (in particular, it is bounded), and $\mu_y^{(n)} - \xi e^{\frac{y}{2}}$ converges to zero; so also the first factor of the quotient of the Poisson probabilities converges to one. The final step is an application of the following lemma:
%\begin{lemma}
%If $f_n, g_n: \R \rightarrow \R_{\geq 0}$ are sequences of asymptotically equivalent functions, i.e. if for all $x \in \R$, $f_n(x) \sim g_n(x)$ as $n \rightarrow \infty$, then $\int_{\R} f_n(x)dx \sim \int_{\R} g_n(x)dx$.
%\end{lemma}
%\begin{proof}
%$f_n(x) \sim g_n(x)$ implies that for all $\epsilon >0$, $|f_n(x)-g_n(x)| \leq \epsilon g_n(x)$. Therefore, $|\frac{\int f_n(x)dx}{\int g_n(x)dx}-1| = |\frac{\int f_n(x)-g_n(x)dx}{\int g_n(x)dx}| \leq \frac{\int |f_n(x)-g_n(x)|dx}{\int g_n(x)dx} \leq \epsilon$.
%\end{proof}
%
%So, far we have established that 
%\begin{align*}
%\E[N_{k_n}] \sim n\int_0^\infty \Pee(Po(\xi e^{\frac{y}{2}}) = k_n)\alpha e^{-\alpha y}dy
%\end{align*}
%
%
%Substituting $t = \xi e^{\frac{y}{2}}$ yields% $\frac{2\alpha n (2\xi)^{k_n}}{k_n!}\int_0^1 z^{2\alpha-k_n-1} e^{-2\xi z^{-1}}  dz$. Now, substituting $t= 2\xi z^{-1}$ gives
%	$$n\frac{2\alpha  \xi^{2\alpha}}{k_n!}\int_{\xi}^\infty t^{k_n-2\alpha-1} e^{-t}  dt = n 2\alpha  \xi^{2\alpha} \frac{\Gamma^+(k_n-2\alpha,\xi)}{\Gamma(k_n+1)}.$$ Using the asymptotic expansion of $\Gamma^+$ (resp. $\Gamma^-$) as the first argument tends to $\infty$ and the Stirling formula for the $\Gamma$-function shows that $\frac{\Gamma^+(n+q,w)}{\Gamma(n)} \sim n^q$ as $n \rightarrow \infty$ for $q \in \mathbb{C}$. Applied to our situation, this is the statement $\frac{\Gamma^+(k_n-2\alpha,\xi)}{\Gamma(k_n+1)} \sim k_n^{-2\alpha-1}$ as $k_n \rightarrow \infty$, This gives the claim.
%	
%	
%(ii): The \textbf{second statement} then follows immediately from the first moment method / Markov's inequality.
%
%(iii): The \textbf{third statement} (that the second moment of $N_{k_n}$ is asymptotically equivalent to the square of the first moment of $N_{k_n}$) is seen as follows: first of all, we show the concentration of the degree $k_n$ vertices within a small strip of a particular height. Then, we obtain an integral expression for the second moment of $N_{k_n}$ over two points in the box from Mecke's formula. We split the integration into two cases according to the horizontal distance of the two points which we integrate over.
%	
%\PvdH{We note that due to concentration argument $\E[N_{k_n}] \sim \E[N_{k_n,0}]$ and $\E[N_{k_n}^2] \sim \E[N_{k_n,0}^2]$: the choice of $c=4\alpha+3$ ensures that $nk^{-c} \ll nk^{-2\alpha-1}$ and $n^2k^{-c} \ll n^2 k^{-4\alpha -2}$. It follows that $nk_n^{-2\alpha-1} \sim \E[N_{k_n}] \sim \E[N_{k_n,0}]$ and $$\E[N_k^2] =\E[N_{k,0}^2+N_{k,-}^2+N_{k,+}^2 +2N_{k,0}N_{k,-}+2N_{k,0}N_{k,+}+2N_{k,-}N_{k,+} ]\sim \E[N_{k,0}^2].$$}
%	
%In other words, it is sufficient to count the number $N_{k_n,0}$ of degree $k_n$ vertices with heights between $y_{k_n,-}$ and $y_{k_n,+}$ (we denote this strip of the box by $S_{k_n}$). We can think of $N_{k_n,0}$ as a sum of indicator random variables over the points of the Poisson process in $S_{k_n}$. For the square $N_{k_n,0}^2$ making a case distinction into diagonal and off-diagonal terms:
%$$\E[N_{k_n}^2] = \E[\sum_{v \in V(G_{H,Po})\cap S_{k_n}} \1_{\{\deg(v)=k_n\}}]+\E[\sum_{v \not = w \in V(G_{H,Po})\cap S_{k_n}} \1_{\{\deg(v)=k_n\}} \1_{\{\deg(w)=k_n\}} ],$$ the first term is just $\E[N_{k_n,0}]$ again (and thus asymptotically less than the square of the first moment as $\E[N_{k_n}] \rightarrow \infty$ as $n \rightarrow \infty$ for $k_n \ll n^{\frac{1}{2\alpha+1}}$). For the second term the Palm-Mecke formula gives $$=\int \int_{(box \cap S_{k_n})^2} \varphi(v,w,k-|\mathcal{B}(v)\cap \{w\}|,k-|\mathcal{B}(w)\cap \{v\}|) \lambda^2 e^{-\alpha(y(v)+y(w))} dvdw$$
% where 
%$$\varphi(v,w,k_1,k_2) = \Pee(|\mathcal{B}(v)\cap \Pcal|=k_1,|\mathcal{B}(w) \cap \Pcal|=k_2)$$
% where $\Pcal$ is a Poisson process in $S_{k_n}$ (with the natural density). Note that $|\mathcal{B}(v)\cap \{w\}|,|\mathcal{B}(w)\cap \{v\}| \in \{0,1\}$, so the value of $\varphi$ is not changed by this asymptotically and we can simplify to
% $$=\int \int_{(box \cap S_{k_n})^2} \varphi(v,w,k,k) \lambda^2 e^{-\alpha(y(v)+y(w))} dvdw$$
% 
%  
%
%We write $v=(x_1,y_1)$, $w=(x_2,y_2)$ for the two points and $d=|x_2-x_1|_{\pi e^{\frac{R}{2}}}$ for the horizontal distance between them. 
%
% 
%Let $X_3$ denote the number of vertices (of the Poisson process $\Pcal$) in the intersection of the neighbourhood balls of $v$ and $w$, let $X_1$ denote the number of vertices which are only in the neighbourhood ball of $v$ and let $X_2$ denote the number of vertices which are only in the neighbourhood ball of $w$. 
%
%$X_1,X_2, X_3$ are independent Poisson random variables with expectations $\mu_1,\mu_2,\mu_3$.
%
%By Lemma \ref{lem:mu3} we have $\mu_3 = \Theta(k^{2\alpha} d^{1-2\alpha})$.
%
%
%
%%\RD{From this calculation, we infer that $\mu_3 \rightarrow \infty$ (because $d =O(n)$ and $k \ll n$). Furthermore if $d \gg k$, then $k^{2\alpha} d^{1-2\alpha} \ll k$ and hence $\mu_3 \ll k$. The assumption $y_1 \in [y_{k_n,-},y_{k_n,+}]$ implies that $\mu_1+\mu_3 \sim k$.}
%
%Pick $0 < \delta < 2\alpha - 1$ and set $\epsilon = \delta(2\alpha-1)>0$. Pick $c=4\alpha+3$ as before. By Lemma \ref{lem:joint_distribution_Poisson} about the joint Poisson probability, it follows that if $d \gg k^{1+\delta}$, then 
%\begin{align*}
%\varphi(v,w,k,k)=\Pee(X_1+X_3=X_2+X_3=k) &=(1+o(1)) \Pee(X_1+X_3=k)\Pee(X_2+X_3=k) +O(k^{-c^2}) \\
%&= (1+o(1))\Pee(|\mathcal{B}(v) \cap \Pcal|=k)\Pee(|\mathcal{B}(w) \cap \Pcal|=k)+O(k^{-c^2})
%\end{align*}
% % $ \E Z = \Theta(\mu_{y_2})$. 
%
%From this for the integral from above (obtained from the Mecke formula), it follows that for $d \gg k_n^{1+\delta}$, 
%\begin{align*}
%&\int \int_{(box\cap S_{k_n})^2, d \gg k_n^{1+\delta}} \varphi(v,w,k_n,k_n) \lambda^2 e^{-\alpha (y(v)+y(w))} dvdw \\
%&\int \int_{(box\cap S_{k_n})^2, d \gg k_n^{1+\delta}} \left( (1+o(1))\Pee(|\mathcal{B}(v) \cap \Pcal|=k)\Pee(|\mathcal{B}(w) \cap \Pcal|=k)+O(k^{-c^2}) \right) \lambda^2 e^{-\alpha (y(v)+y(w))} dvdw 
%\end{align*}
%Now using that the integrand is independent of the horizontal coordinates  and the computation of the first moment of $N_{k_n}$ gives
%\begin{align*}
%&=O( n(n-k_n^{1+\delta}) \int_0^\infty \int_0^\infty \Pee(|\mathcal{B}(v) \cap \Pcal|=k)\Pee(|\mathcal{B}(w) \cap \Pcal|=k) \alpha^2 e^{-\alpha(y_1+y_2)} dy_1 dy_2)+O(k^{-c^2}) \\
%&=O(n(n-k_n^{1+\delta}) (k_n^{-2\alpha-1})^2)
%\end{align*} 
%which is asymptotic to the square of the first moment.
%
% If $d =O(k_n^{1+\delta})$, we can always upper bound $\Pee(X_1+X_3=X_2+X_3=k_n) \leq \Pee(Po(\mu_{y_1})=k_n)$:
%\begin{align*}
%&\int \int_{(box \cap S_{k_n})^2, d =O(k_n^{1+\delta})} \Pee(\deg(v)=\deg(w)=k_n)\lambda^2 e^{-\alpha (y(v)+y(w))} dvdw \\
%&=O(nk_n^{1+\delta} \int_{y_{k_n,-}}^\infty \int_{y_{k_n,-}}^\infty \Pee(Po(\mu_{y_1})=k_n) \alpha^2 e^{-\alpha(y_1+y_2)} dy_1 dy_2 ) \\
%&=O(n k_n^{1+\delta} k_n^{-2\alpha-1} k_n^{-2\alpha})
%\end{align*} 
%So after cancellation with the square of the first moment we get $O(k_n^{2+\delta} n^{-1})$ which tends to zero as $n \rightarrow \infty$ for $k_n \ll n^{\frac{1}{2\alpha+1}}$ for $\alpha>\frac{1}{2}$ because $\frac{2+\delta}{2\alpha+1} < 1$.

%\end{proof}

%To extend the result to the original hyperbolic random graph with a fixed number of $n$ vertices, we note that the claim in the previous proposition concerns / depends on the first and second moment of the number of degree $k_n$ vertices, so we only need to show:
%\begin{proposition}
%Let $\alpha > \frac{1}{2}$, $\nu > 0$ and consider the hyperbolic graph $G_{\H,n}(\alpha,\nu)$ and the Poisson version $G_{\widetilde{\H},n}(\alpha,\nu)$. Then
%\begin{enumerate}
%\item $\E[N_{\H,n}(k_n)] \sim \E[N_{\widetilde{\H},n}(k_n)]$,
%\item $\E[N_{\H,n}(k_n)^2] \sim \E[N_{\widetilde{\H},n}(k_n)^2]$.
%\end{enumerate}
%\end{proposition}


%\begin{lemma}
%Let $\alpha > \frac{1}{2}$, $\nu > 0$ and $\{k_n\}_{n \ge 1}$ be a non-decreasing sequence such that $k_n = \bigO{n^{\frac{1}{2\alpha + 1}}}$. Then, as $n \to \infty$,
%\begin{align*}
%	\Exp{N_{\Pcal,n}(k_n)^2} = (1 + o(1)) \Exp{N_{\Pcal,n}(k_n)}^2
%\end{align*}
%\end{lemma}


\section{Some results for random variables}

We start with the following concentration result which follows from~\cite[Theorem 4]{freedman1973another}, together with the note directly after it.

\begin{lemma}\label{lem:general_concentration_sum_indicators}
Let $X_n$ be a sum of $n$, possibly dependent, indicators and $c > 0$. Then
\[
	\Prob{|X_n - \Exp{X_n}| > c \Exp{X_n}} \le 2 e^{-\frac{c^2 \Exp{X_n}^2}{2}}.
\]
\end{lemma}

Let $H(x) = x\log(x) - x + 1$. Then by a Chernoff bound, see for instance \cite[Lemma 1.2]{penrose2003random},
\begin{align*}
	&\Prob{\Po(\lambda) \ge k} \le e^{-\lambda H(k/\lambda)} \quad \text{for all } k \ge \lambda\\
	&\Prob{\Po(\lambda) \le k} \le e^{-\lambda H(k/\lambda)} \quad \text{for all } k \le \lambda.
\end{align*}
Note that $H(x) \le (x-1)^2/2$ for all $0 \le x \le 1$. Therefore, 
\begin{align*}
	\Prob{\left|\mathrm{Po}(\lambda) - \lambda\right| \ge x} \ge 1 - e^{-\lambda H(1 - x/\lambda)} - e^{-\lambda H(1 + x/\lambda)}
\end{align*}
 $\Pee(X >k) \leq e^{-\mu H(\frac{k}{\mu})}$ and for $k<\mu$, $\Pee(X< k) \leq e^{-\mu H(\frac{k}{\mu})}$, where $H(x) = x\ln x -x+1$, \cite{penrose2003random}), it follows that $\Pee(N \in [n-c\sqrt{n\log n},n+c\sqrt{n\log n}]) \geq 1-e^{-n H(\frac{n-c\sqrt{n\log n}}{n})}-e^{-nH(\frac{n+c\sqrt{n \log n}}{n})} \geq  1-2e^{-n \frac{c^2 n \log n}{n^2}} = 1-2e^{-c^2\log n}=1-2n^{-c^2}$ (where we have used that $H(x) = (x-1)^2$ for $x$ close to 1)

By a Chernoff bound we have
\begin{equation}\label{eq:def_chernoff_bound_poisson}
	\Prob{\left|\mathrm{Po}(\lambda) - \lambda\right| \ge x} \le 2e^{-\frac{x^2}{2(\lambda + x)}}.
\end{equation}
In particular, if $\lambda_n \to \infty$, then, for any $0 < \varepsilon < 1$,
\[
	\Prob{\left|\mathrm{Po}(\lambda_n) - \lambda_n\right| \ge \lambda_n^{\frac{1+\varepsilon}{2}}} \le 2e^{-\frac{\lambda_n^\varepsilon}{2\left(1 + \lambda_n^{-(1 - \varepsilon)/2}\right)}}
	= \bigO{e^{-\lambda_n^\varepsilon}}.
\]

%\PvdH{The results below still need to be converted to match style and notation.}
%\begin{lemma}\label{lem:Poissonstab}
%Let $k_n$ be a sequence of natural numbers, $\mu_n^{(1)}, \mu_n^{(2)}$ sequences of positive real numbers and $X_n \sim \mathrm{Po}(\mu_n^{(1)})$ be Poisson random variables with expectation $\mu_n^{(1)}$. In addition, Let $h_n = \smallO{\sqrt{\frac{k_n}{\log k_n}}}$, $c > 0$ and assume that $k_n-\mu_n^{(2)} = \mu_n^{(1)}+a_n$ with $a_n \in [-c\sqrt{k_n\log k_n},c\sqrt{k_n\log k_n}]$ and $\mu_n^{(1)}=\Theta(k_n)$. 
%Then, as $n \to \infty$,
%\[
%	\Prob{X_n = k_n - \mu_n^{(2)}} \sim \Prob{X_n = k_n - \mu_n^{(2)} + h_n}.
%\]
%\end{lemma}
%\begin{proof}
%	Firstly, we upper bound the ratio:
%	\begin{align*}
%	\frac{\Pee(Po(\mu_1)=k-\mu_3)}{\Pee(Po(\mu_1)=k-\mu_3 +h)} &= \mu_1^{-h} \frac{(k-\mu_3+h)!}{(k-\mu_3)!}  \leq \mu_1^{-h} (k-\mu_3+h)^h\\
%	 &= \mu_1^{-h} (\mu_1+g+h)^h = (1+\frac{g+h}{\mu_1})^h \leq e^{\frac{(g+h)h}{\mu_1}}
%	\end{align*}
%	which tends to 1 by the assumptions on $g,h$ and $\mu_1$.
%	
%	Similarly, the ratio can be lower bounded by 1:
%	\begin{align*}
%	\frac{\Pee(Po(\mu_1)=k-\mu_3)}{\Pee(Po(\mu_1)=k-\mu_3 +h)} &= \mu_1^{h} \frac{(k-\mu_3+h)!}{(k-\mu_3)!}  \geq \mu_1^{-h} (k-\mu_3)^h\\
%	 &= \mu_1^{-h} (\mu_1+g)^h = (1+\frac{g}{\mu_1})^h \geq 1 
%	\end{align*}
%\end{proof}
%
%\begin{lemma}[Near independence of Poisson variables]\label{lem:joint_distribution_Poisson}
%Let $\lambda_1(n), \lambda_2(n), \lambda_3(n)$ be sequences of positive numbers and $X_1(n), X_2(n), X_3(n)$ be sequences independent Poisson random variables with expectations $\lambda_1(n), \lambda_2(n)$ and $\lambda_3(n)$, respectively. 
%Let $k_n, \ell_n \in \N_0$ be a sequences of natural numbers. Then as $n \to \infty$ the following holds
%\[
%	\Prob{X_1(n) + X_3(n) = k_n, X_2(n) + X_3(n) = \ell_n} = \Prob{X_1(n) = k_n}\Prob{X_2(n) = \ell_n}
%	+ \bigO{\lambda_3(n)}
%\]
%
%If in addition $\lambda_3(n) \le k_n^{1-\varepsilon}$, for large enough $n$ and some $0 < \varepsilon < 1$ and $\lambda_1(n), \lambda_2(n) = \bigT{k_n}$, then for any $C > 0$, as $n \to \infty$,
%\begin{align*}
%	&\hspace{-30pt}\Prob{X_1(n) + X_3(n) = k_n, X_2(n) + X_3(n) = k_n} \\
%	&= (1+\smallO{1})\Prob{X_1(n) + X_3(n) = k_n}\Prob{X_2(n) + X_3(n) = k_n} + \bigO{k_n^{-\frac{C^2}{2}}}.
%\end{align*}
%\end{lemma}
%\begin{proof}
%Let $C>0$ and $c = \sqrt{C}$.
%From $\lambda_3 \leq k^{1-\epsilon}$ it follows by the Chernoff bound for Poisson random variables that $\Pee(|X_3-\lambda_3| \geq c\sqrt{k^{1-\epsilon} \log k}) \leq k^{-c^2} = k^{-C}$. Note that $h:= c\sqrt{k^{1-\epsilon} \log k} = o(\sqrt{\frac{k}{\log k}})$.
%
%Conditioning on the value of $X_3$ and then separating the values according to the event $A$ that $X_3 \in [\lambda_3-h,\lambda_3 +h]$ gives
%\begin{align*}
%\Pee( X_1+X_3 = X_2 + X_3 =k) &= \sum_{\kappa \in [\lambda_3-h,\lambda_3+h]} \Pee(X_1=X_2=k-\kappa)\Pee(X_3=\kappa) \\
% \\ &\qquad \qquad+ \sum_{\kappa \not \in [\lambda_3-h,\lambda_3+h]} \Pee(X_1=X_2=k-\kappa)\Pee(X_3=\kappa) \\
%&=\sum_{\kappa \in [\lambda_3-h,\lambda_3+h]} \Pee(X_1=k-\kappa)\Pee(X_2=k-\kappa)\Pee(X_3=\kappa) + k^{-C}
%\end{align*}
%where we have used the independence of $X_1$ and $X_2$ in the last line. Now applying Lemma~\ref{lem:Poissonstab} (as $k-\kappa = k-\lambda_3+\lambda_3-\kappa$ and $\lambda_3-\kappa \leq h = o(\sqrt{\frac{k}{\log k}})$) and then summarizing all the terms results in
%\begin{align*}
%=\sum_{\kappa \in [\lambda_3-h,\lambda_3+h]} (1+o(1))\Pee(X_1=k-\lambda_3)\Pee(X_2=k-\lambda_3)\Pee(X_3=\kappa) +k^{-C} \\
%= (1+o(1)) \Pee( X_1 =k- \lambda_3 ) \Pee(  X_2 = k-\lambda_3) +k^{-C}
%\end{align*}
%On the other hand, again by conditioning on the event $A$,
%\begin{align*}
%\Pee(X_1+X_3=k) = \sum_{\kappa \in [\lambda_3-h,\lambda_3+h]} \Pee(X_1=k-\kappa)\Pee(X_3 = \kappa)+\Pee(X_1+X_3=k|A^c)\Pee(A^c)\\
% = (1+o(1))\Pee(X_1=k- \lambda_3)\Pee(A) +k^{-C} \\
% =(1+o(1))\Pee(X_1=k-\lambda_3)+k^{-C}
%\end{align*}
%and likewise
%\begin{align*}
%\Pee(X_2+X_3=k)
% = (1+o(1))\Pee(X_2=k- \lambda_3) +k^{-C}
%\end{align*}
%Therefore, their product has the form
%\begin{align*}
%&\Pee(X_1+X_3=k)\Pee(X_2+X_3=k) \\
%&= (1+o(1))\Pee(X_1=k-\lambda_3)\Pee(X_2=k-\lambda_3)+O(k^{-C})
%\end{align*}
%We conclude that both sides in the claim agree.
%\end{proof}
\end{appendices}