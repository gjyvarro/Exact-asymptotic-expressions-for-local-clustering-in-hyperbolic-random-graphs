\begin{appendices}

\section{Meijer's G-function}\label{sec:Meijer_G_functions}

Recall that $\Gamma(z)$ denotes the Gamma function. Let $p, q, m, \ell$ be four integers satisfying $0 \le m \le q$ and $0 \le \ell \le p$ and consider two sequences ${\bf a}_p = \{a_1, \dots, a_p\}$ and ${\bf b}_q = \{b_1, \dots, b_q\}$ of reals such that $a_i - b_j$ is not a positive integer for all $1 \le i \le p$ and $1 \le j \le q$ and $a_i - a_j$ is not an integer for all distinct indices $1 \le i, j \le p$. Then, with $\iota$ denoting the complex unit, Meijer's G-Function~\cite{meijer1946gfunction} is defined as
\begin{equation}\label{eq:def_Meijer_G_function}
	\MeijerGnew{m}{\ell}{p}{q}{{\bf a}}{{\bf b}}{z} 
	= \frac{1}{2 \pi \iota} \int_L 
	\frac{\prod_{j = 1}^{m}\Gamma(b_j - t) \prod_{j = 1}^\ell \Gamma(1 - a_j + t)}
	{\prod_{j = m + 1}^{q}\Gamma(1 - b_j + t) \prod_{j = \ell + 1}^p\Gamma(a_j-t)} \, z^t \dd t,
\end{equation}
where the path $L$ is an upward oriented loop contour which separates the poles of the function $\prod_{j = 1}^{m}\Gamma(b_j - t)$ from those of $\prod_{j = 1}^n \Gamma(1 - a_j + t)$ and begins and ends at $+\infty$ or $-\infty$.

The Meijer's G-Function is of very general nature and has relation to many known special functions such as the Gamma function and the generalized hypergeometric function. For more details, such as many identities for $\MeijerGnew{m}{\ell}{p}{q}{{\bf a}}{{\bf b}}{z}$ see \cite{gradshteyn2015table,luke2014mathematical}.

For our purpose we need the following identity which follows from an Mellin transform operation.

\begin{lemma}\label{lem:gamma_meijer_G}
For any $a\in \R$ and $\xi, s>0$,
$$\Gamma^+(-a-1,\xi/s) = \MeijerGnew{2}{0}{1}{2}{1}{-a-1,0}{\frac{\xi}{s}}$$
\end{lemma}
\begin{proof}
Let $x>0$ and $q\in\R$ and note that as the $\Gamma$-function is the Mellin transform of $e^{-x}$, by the inverse Mellin transform formula, we have $e^{-x}=\frac{1}{2\pi \iota}\int_{c-\iota\infty}^{c+\iota\infty} \Gamma(p)x^{-p}dp$ for $c>0$ (see \cite[p.196]{davies2012integral}). Applying the change of variable $p(r)=q-r$ yields $e^{-x}=\frac{1}{2\pi \iota}\int_{c+q-\iota\infty}^{c+q+\iota\infty} \Gamma(q-r) x^{r-q}dr$, then multiplying both sides with $-x^{q-1}$ gives $-x^{q-1}e^{-x} = -\frac{1}{2\pi \iota}\int_{c+q-\iota\infty}^{c+q+\iota\infty} \Gamma(q-r) x^{r-1}dr$. Now, integrating both sides gives $\int_x^\infty t^{q-1}e^{-t}dt = \frac{1}{2\pi \iota}\int_{c+q-\iota\infty}^{c+q+\iota\infty}\frac{\Gamma(q-r)}{-r}x^r dr$. On the left-hand side is the incomplete gamma function and on the right-hand side with using $-r= \frac{\Gamma(1-r)}{\Gamma(-r)}$ is the Meijer $G$-function, i.e. $\Gamma^+(q,x)=\MeijerGnew{2}{0}{1}{2}{1}{q,0}{x}$. The claim follows by plugging in $q=-a-1$ and $x=\frac{\xi}{s}$.
\end{proof}

\section{Incomplete Beta function}\label{sec:beta_function}

Here we derive the asymptotic behavior for the function $B^-(1-z; 2\alpha, 3-4\alpha )$ as $z \to 0$, which is used to analyze the asymptotic behavior of $P(y)$, see Section~\ref{ssec:asymptotics_local_clustering_P}.

\begin{lemma}\label{lem:asymptotics_incomplete_beta}
We have the following asymptotic results for $B^-(1-z; 2\alpha, 3-4\alpha )$
\begin{enumerate}
\item For $1/2 < \alpha < 3/4$
\[
	\lim_{z \to 0} B^-(1-z, 2\alpha, 3-4\alpha ) = B(2\alpha, 3 - 4\alpha).
\]
\item When $\alpha = 3/4$,
\[
	\lim_{z \to 0} \frac{B^-(1-z, 2\alpha, 3-4\alpha)}{\log(z)} = -1.
\]
\item For $\alpha > 3/4$,
\[
	\lim_{z \to 0} z^{4\alpha - 3} B^-(1-z, 2\alpha, 3-4\alpha ) = \frac{1}{4\alpha - 3}.
\]
\end{enumerate}
\end{lemma}

\begin{proof}
We use the hypergeometric representation of the incomplete Beta function,
\[
	B^-(x, a, b) = \frac{x^a}{2a}F(a, 1-b,a+1,x),
\]
where $F$ denote the hypergeometric function~\cite{temme2011special} (or see~\cite[Section 8.17 (ii)]{dlmf2019digital}). In particular we have that
\[
	B^-(1-z; 2\alpha, 3-4\alpha ) = \frac{(1-z)^{2\alpha}}{2\alpha} F(2\alpha,4\alpha-2,2\alpha+1,1-z).
\]

The behavior of $F(a,b,c,1-z)$ as $z \to 0$ depend on the real part of the sum of $c - a - b$ and whether $c = a + b$~\cite{andrews2000special} (or see~\cite[Section 15.4(ii)]{dlmf2019digital}). Since in our case $a,b,c$ will be real it only depends on the sum of $c - a - b$. For $c - a - b > 0$ we have
\begin{equation}\label{eq:F_cab_less_than_zero}
	\lim_{z \to 0} F(a,b,c,1-z) = \frac{\Gamma(c)\Gamma(c-a-b)}{\Gamma(c-a)\Gamma(c-b)},
\end{equation}
if $c = a + b$ then
\begin{equation}\label{eq:F_cab_zero}
	\lim_{z \to 0} \frac{F(a,b,c,1-z)}{\log(z)} = -\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)},
\end{equation}
and finally, when $c - a - b < 0$
\begin{equation}\label{eq:F_cab_greater_than_zero}
	\lim_{z \to 0} \frac{F(a,b,c,1-z)}{z^{c - a - b}} = \frac{\Gamma(c)\Gamma(a + b -c)}{\Gamma(a)\Gamma(b)}.
\end{equation}

In our case we have,
\[
	B^-(1-z; 2\alpha, 3-4\alpha ) = \frac{(1-z)^{2\alpha}}{2\alpha} F(a,b,c,1-z),
\]
with $a := 2\alpha$, $b: = 4\alpha-2$ and $c := 2\alpha + 1$. Therefore,
\[
	c - a - b = 2\alpha + 1 - 2\alpha -(4\alpha - 2) = 3 - 4\alpha.
\]

Now if $\alpha < 3/4$ then $c - a - b > 0$ and hence
\[
	\lim_{z \to 0} B^-(1-z; 2\alpha, 3-4\alpha ) = \frac{1}{2\alpha} \frac{\Gamma(2\alpha + 1)\Gamma(3 - 4\alpha)}{\Gamma(1)\Gamma(3-2\alpha)} = \frac{\Gamma(2\alpha)\Gamma(3 - 4\alpha)}{\Gamma(3-2\alpha)}
	= B(2\alpha, 3 - 4\alpha),
\]
where we used that $\Gamma(2\alpha + 1) = 2\alpha \Gamma(2\alpha)$.

When $\alpha =3/4$ then $c - a - b = 0$ and therefore~\eqref{eq:F_cab_zero}, together with the fact that $(1-z)^{3/2} \sim 1$ as $z \to 0$, implies that
\begin{align*}
	\lim_{z \to 0} \frac{B^-(1-z; 2\alpha, 3-4\alpha )}{\log(z)} 
	= -\frac{1}{2\alpha} \frac{\Gamma(6\alpha - 2)}{\Gamma(2\alpha)\Gamma(4\alpha - 2)}
	= - \frac{\Gamma(5/2)}{\frac{3}{2}\Gamma(3/2)} = -1.
\end{align*}

Finally, when $\alpha > 3/4$, $c - a - b = 3 - 4\alpha < 0$ and using~\eqref{eq:F_cab_greater_than_zero} we get
\begin{align*}
	\lim_{z \to 0} z^{4\alpha - 3} B^-(1-z, 2\alpha, 3-4\alpha ) 
	= \frac{1}{2\alpha}\frac{\Gamma(2\alpha + 1)\Gamma(4\alpha - 3)}{\Gamma(2\alpha)\Gamma(4\alpha - 2)}
	= \frac{\Gamma(4\alpha - 3)}{\Gamma(4\alpha - 2)} = \frac{1}{4\alpha - 3}.
\end{align*}
\end{proof}

\section{Some results on functions}

\begin{lemma}\label{lem:arccos_approx}
For any $0 < \lambda < 1$ there exists a $K > 0$, such that for all $0 < x \le (1 - \lambda)2$
\[
	\frac{1}{2}\arccos(1-x)
	\le \frac{x}{\sqrt{1-(1-x)^2}} 
	\le \frac{1}{2}\arccos(1-x)\left(1 + x\right).
\]
In particular, as $x \to 0$,
\[
	\frac{x}{\sqrt{1-(1-x)^2}} \sim \frac{1}{2}\arccos(1-x).
\]
\end{lemma}

\begin{proof}
First we observe that for all $0 < x < 2$
\[
	0 < \sqrt{2x}\left(1 - \frac{x}{\sqrt{8}}\right) \le \arccos(1-x) \le 
	\sqrt{2x}\left(1 + \frac{x}{\sqrt{8}}\right)
\]
while for every $0 < \lambda < 1$, there exists a $K > 0$ such that for all $0 < x \le (1-\lambda) 2$,
\[
	0 < \frac{1}{\sqrt{2x}}\left(1 - \frac{x}{2}\right) \le \frac{1}{\sqrt{1 - (1 - x)^2}} \le
	\frac{1}{\sqrt{2x}}\left(1 + K x\right).
\]
It then follows that for all $0 < x \le (1-\lambda) 2$,
\begin{align*}
	\frac{x}{\sqrt{1 - (1-x^2)}} &\le \frac{1}{2} \sqrt{2x}\left(1 + K\frac{x}{\sqrt{2}}\right)
		\le \frac{1}{2} \arccos(1-x) \frac{1 + Kx}{1 - \frac{x}{\sqrt{8}}}
		\le \frac{1}{2} \arccos(1-x)\left(1 + \frac{(K + 1)x}{1 - x}\right),
\end{align*}
and
\begin{align*}
	\frac{x}{\sqrt{1 - (1-x^2)}} &\ge \frac{1}{2} \sqrt{2x}\left(1 - \frac{x}{2}\right)
		\ge \frac{1}{2} \arccos(1-x) \frac{1 - \frac{x}{2}}{1 + \frac{x}{\sqrt{8}}}
		\ge \frac{1}{2} \arccos(1-x)\left(1 - \frac{(1+\sqrt{2})x}{1 + x}\right),
\end{align*}
which finishes the proof.
\end{proof}

%\section*{Properties of $G_{\H,n}(\alpha,\nu)$}
%
%\PvdH{@Markus: is the lemma below only needed to prove the second lemma? If so than we can remove it because the statements of the second lemma have already been established in other parts of the paper.}
%
%\begin{lemma}[Intensity measure of the intersection of two neighbourhood balls]\label{lem:mu3}
%Consider the Poisson hyperbolic random graph conditioned on having two vertices with degree $k$ at heights $y_1, y_2 \in [y_{k,-},y_{k,+}]$ and horizontal distance $d$. Then the expected number of vertices in the intersection of their neighbourhood balls in the infinite limit model is $\Theta(k^{2\alpha}d^{1-2\alpha})$.
%\end{lemma}
%\begin{proof}
%Denote the two vertices by $v$ and $w$.
%If the neighbourhood balls of $v$ and $w$ intersect at the minimal height $y_h$, then $d = e^{\frac{y_h+y_1}{2}}+e^{\frac{y_h+y_2}{2}}$.
%
%Due to our assumption $y_1, y_2 \in [y_{k,-},y_{k,+}]$, this condition implies $d \leq 2 e^{\frac{y_h+y_{k,+}}{2}}$. This implies $y_h \geq 2\ln \frac{d}{2} - y_{k,+}$. (In a similar fashion, we infer that $y_h \leq 2\ln \frac{d}{2} -y_{k,-}$.)
%
%Therefore, using $y_1,y_2 \in [y_{k,-}, y_{k,+}]$ and $e^{\frac{y_1}{2}}, e^{\frac{y_2 }{2}} \sim k$, $e^{\frac{y_h}{2}} \sim \frac{d}{k}$:
%\begin{align*}
%\mu_3 &= \int_{y_h}^\infty (e^{\frac{y_1+y}{2}}+e^{\frac{y_2+y}{2}} - d) \alpha e^{-\alpha y} dy =\Theta(k)\frac{\alpha}{\alpha-\frac{1}{2}} e^{(\frac{1}{2}-\alpha) y_h} +d e^{-\alpha y_h} \\
%&= \Theta(k (\frac{d}{k})^{1-2\alpha} +d(\frac{d}{k})^{-2\alpha}) = \Theta(k^{2\alpha} d^{1-2\alpha})
%\end{align*}
%\end{proof}
%
%The following proposition establishes several properties on the number of vertices with degrees $k_n$ in the Poisson hyperbolic graph $G_{\widetilde{\H},n}(\alpha,\nu)$.
%
%\begin{proposition}
%Let $\alpha > \frac{1}{2}$, $\nu > 0$, consider the Poisson hyperbolic random geometric graph $G_{\widetilde{\H},n}(\alpha,\nu)$ and let $N_{\widetilde{\H},n}(k_n)$ be the number of degree $k_n$ vertices.
%\begin{enumerate}
%	\item If $2 \leq k_n \leq n-1$ such that $k_n \rightarrow \infty$ as $n \rightarrow \infty$, then $\E[N_{\widetilde{\H},n}(k_n)] \sim 2\alpha \xi^{2\alpha} n k_n^{-(2\alpha +1)}$ as $n \rightarrow \infty$.
%	
%	\item In particular if $k_n \gg n^{\frac{1}{2\alpha+1}}$, then $N_{\widetilde{\H},n}(k_n) = 0$ w.h.p.
%	
%	\item Furthermore, if $k_n \ll n^{\frac{1}{2\alpha+1}}$, then $\frac{\E[N_{\widetilde{\H},n}(k_n)^2]}{\E[N_{\widetilde{\H},n}(k_n)]^2} \rightarrow 1$, so in particular in this case w.h.p. $$N_{\widetilde{\H},n}(k_n) = (1+o(1))\E[N_{\widetilde{\H},n}(k_n)]$$ and $N_{\widetilde{\H},n}(k_n) > 0$ w.h.p.
%\end{enumerate}
%\end{proposition}
%
%\begin{proof}
%\PvdH{The results from this lemma are already stated elsewhere but I kept the lemma to not break any references. It will be cleaned next round.}
	
%(i): First of all, we note that the degree of a vertex at height $y \in [0,R_n]$ is a Poisson random variable with expectation $\lambda_n(y) := \mu_{\alpha,\nu}(\BallHyp{y})$. As $N_{\widetilde{\H},n}(k_n) = \sum_{v\in V(G_{\widetilde{\H},n})} \1_{\{D_{\widetilde{\H},n}(v) = k_n\}}$, by the Palm-Mecke formula,
%\begin{align*}
%	\Exp{N_{\widetilde{\H},n}(k_n)} &= \int_{\Rcal_n} \Prob{D_{\widetilde{\H},n}(y) = k_n} 
%		f_{\alpha,\nu}(x,y) \dd x \dd y \\
%	&= n \int_0^{R_n} \Prob{D_{\widetilde{\H},n}(y) = k_n} \alpha e^{-\alpha y} \dd y 
%		= n \int_0^{R_n} \Prob{\Po(\lambda_n(y))=k_n} \alpha e^{-\alpha y}dy.
%\end{align*}
%
%Now, we would like to replace the $\mu_{y}^{(n)}$ in the Poisson probability above by $\xi e^{\frac{y}{2}}$. The argument for this splits into the following parts: $\mu_{y}^{(n)} \rightarrow \xi e^{\frac{y}{2}}$ as $n\rightarrow \infty$, together with the concentration resp. range of $y$, it follows that $\Pee(Po(\mu_y^{(n)})=k_n) \sim \Pee(Po(\xi e^{\frac{y}{2}})=k_n)$; finally the asymptotic equivalence of the integrands implies the asymptotic equivalence of the integrals. 
%
%The convergence of $\mu_{y}^{(n)}$ to $\xi e^{\frac{y}{2}}$ is by  Lemma \ref{lem:expecdeg}. \PvdH{I do not follow this argument. By Lemma \ref{lem:expecdeg}, for some fixed $\varepsilon > 0$,
%\[
%	\mu_y^{(n)} = \xi e^{y/2}\left(1 \pm \varepsilon\right).
%\]
%Hence
%\begin{align*}
%	\frac{\Pee(Po(\mu_y^{(n)})=k_n)}{\Pee(Po(\xi e^{\frac{y}{2}})=k_n)} 
%	&= \left(\frac{\mu_y^{(n)}}{\xi e^{\frac{y}{2}}}\right)^{k_n} e^{-\mu_y^{(n)}+\xi e^{y/2}} \\
%	&= (1\pm \varepsilon)^{k_n} e^{-\varepsilon(k_n \pm C\sqrt{k_n \log(k_n)})}\\
%	&= e^{\pm \varepsilon k_n + \bigO{\varepsilon^2 k_n} -\varepsilon(k_n \pm C\sqrt{k_n \log(k_n)})}\\
%	&= e^{\mp \varepsilon C\sqrt{k_n \log(k_n)} + \bigO{\varepsilon^2 k_n}}.
%\end{align*}
%This will not converge to $1$ for any fixed $\varepsilon$ and hence we need to make sure that $\varepsilon \ll \sqrt{k_n \log(k_n)}$. Can we do that in Lemma \ref{lem:expecdeg}?
%}
%
%
%\PvdH{By a concentration argument we can assume that $\xi e^{\frac{y}{2}} \in [k_n-c\sqrt{k_n \log k_n},k_n+c\sqrt{k_n \log k_n}]$ (for a sufficiently large $c$). The quotient of the Poisson probabilities is $\frac{\Pee(Po(\mu_y^{(n)})=k_n)}{\Pee(Po(\xi e^{\frac{y}{2}})=k_n)} = (\frac{\mu_y^{(n)}}{\xi e^{\frac{y}{2}}})^{k_n} e^{\mu_y^{(n)}-\mu_y}$.} The second factor converges to one and for the first factor, we have $(\frac{\mu_y^{(n)}}{\xi e^{\frac{y}{2}}})^{k_n} = (1+\frac{\mu_y^{(n)}-\xi e^{\frac{y}{2}}}{\xi e^{\frac{y}{2}}})^{k_n} \leq e^{(\mu_y^{(n)}-\xi e^{\frac{y}{2}})\frac{k_n}{\xi e^{\frac{y}{2}}}}$. (I think we also need a lower bound (e.g. 1) here, but I do not see how to get it, because $\mu_y^{(n)}$ is monotone increasing to $\xi e^{\frac{y}{2}}$). Due to the restricted range of $y$, $\frac{k_n}{\xi e^{\frac{y}{2}}}$ converges to one (in particular, it is bounded), and $\mu_y^{(n)} - \xi e^{\frac{y}{2}}$ converges to zero; so also the first factor of the quotient of the Poisson probabilities converges to one. The final step is an application of the following lemma:
%\begin{lemma}
%If $f_n, g_n: \R \rightarrow \R_{\geq 0}$ are sequences of asymptotically equivalent functions, i.e. if for all $x \in \R$, $f_n(x) \sim g_n(x)$ as $n \rightarrow \infty$, then $\int_{\R} f_n(x)dx \sim \int_{\R} g_n(x)dx$.
%\end{lemma}
%\begin{proof}
%$f_n(x) \sim g_n(x)$ implies that for all $\epsilon >0$, $|f_n(x)-g_n(x)| \leq \epsilon g_n(x)$. Therefore, $|\frac{\int f_n(x)dx}{\int g_n(x)dx}-1| = |\frac{\int f_n(x)-g_n(x)dx}{\int g_n(x)dx}| \leq \frac{\int |f_n(x)-g_n(x)|dx}{\int g_n(x)dx} \leq \epsilon$.
%\end{proof}
%
%So, far we have established that 
%\begin{align*}
%\E[N_{k_n}] \sim n\int_0^\infty \Pee(Po(\xi e^{\frac{y}{2}}) = k_n)\alpha e^{-\alpha y}dy
%\end{align*}
%
%
%Substituting $t = \xi e^{\frac{y}{2}}$ yields% $\frac{2\alpha n (2\xi)^{k_n}}{k_n!}\int_0^1 z^{2\alpha-k_n-1} e^{-2\xi z^{-1}}  dz$. Now, substituting $t= 2\xi z^{-1}$ gives
%	$$n\frac{2\alpha  \xi^{2\alpha}}{k_n!}\int_{\xi}^\infty t^{k_n-2\alpha-1} e^{-t}  dt = n 2\alpha  \xi^{2\alpha} \frac{\Gamma^+(k_n-2\alpha,\xi)}{\Gamma(k_n+1)}.$$ Using the asymptotic expansion of $\Gamma^+$ (resp. $\Gamma^-$) as the first argument tends to $\infty$ and the Stirling formula for the $\Gamma$-function shows that $\frac{\Gamma^+(n+q,w)}{\Gamma(n)} \sim n^q$ as $n \rightarrow \infty$ for $q \in \mathbb{C}$. Applied to our situation, this is the statement $\frac{\Gamma^+(k_n-2\alpha,\xi)}{\Gamma(k_n+1)} \sim k_n^{-2\alpha-1}$ as $k_n \rightarrow \infty$, This gives the claim.
%	
%	
%(ii): The \textbf{second statement} then follows immediately from the first moment method / Markov's inequality.
%
%(iii): The \textbf{third statement} (that the second moment of $N_{k_n}$ is asymptotically equivalent to the square of the first moment of $N_{k_n}$) is seen as follows: first of all, we show the concentration of the degree $k_n$ vertices within a small strip of a particular height. Then, we obtain an integral expression for the second moment of $N_{k_n}$ over two points in the box from Mecke's formula. We split the integration into two cases according to the horizontal distance of the two points which we integrate over.
%	
%\PvdH{We note that due to concentration argument $\E[N_{k_n}] \sim \E[N_{k_n,0}]$ and $\E[N_{k_n}^2] \sim \E[N_{k_n,0}^2]$: the choice of $c=4\alpha+3$ ensures that $nk^{-c} \ll nk^{-2\alpha-1}$ and $n^2k^{-c} \ll n^2 k^{-4\alpha -2}$. It follows that $nk_n^{-2\alpha-1} \sim \E[N_{k_n}] \sim \E[N_{k_n,0}]$ and $$\E[N_k^2] =\E[N_{k,0}^2+N_{k,-}^2+N_{k,+}^2 +2N_{k,0}N_{k,-}+2N_{k,0}N_{k,+}+2N_{k,-}N_{k,+} ]\sim \E[N_{k,0}^2].$$}
%	
%In other words, it is sufficient to count the number $N_{k_n,0}$ of degree $k_n$ vertices with heights between $y_{k_n,-}$ and $y_{k_n,+}$ (we denote this strip of the box by $S_{k_n}$). We can think of $N_{k_n,0}$ as a sum of indicator random variables over the points of the Poisson process in $S_{k_n}$. For the square $N_{k_n,0}^2$ making a case distinction into diagonal and off-diagonal terms:
%$$\E[N_{k_n}^2] = \E[\sum_{v \in V(G_{H,Po})\cap S_{k_n}} \1_{\{\deg(v)=k_n\}}]+\E[\sum_{v \not = w \in V(G_{H,Po})\cap S_{k_n}} \1_{\{\deg(v)=k_n\}} \1_{\{\deg(w)=k_n\}} ],$$ the first term is just $\E[N_{k_n,0}]$ again (and thus asymptotically less than the square of the first moment as $\E[N_{k_n}] \rightarrow \infty$ as $n \rightarrow \infty$ for $k_n \ll n^{\frac{1}{2\alpha+1}}$). For the second term the Palm-Mecke formula gives $$=\int \int_{(box \cap S_{k_n})^2} \varphi(v,w,k-|\mathcal{B}(v)\cap \{w\}|,k-|\mathcal{B}(w)\cap \{v\}|) \lambda^2 e^{-\alpha(y(v)+y(w))} dvdw$$
% where 
%$$\varphi(v,w,k_1,k_2) = \Pee(|\mathcal{B}(v)\cap \Pcal|=k_1,|\mathcal{B}(w) \cap \Pcal|=k_2)$$
% where $\Pcal$ is a Poisson process in $S_{k_n}$ (with the natural density). Note that $|\mathcal{B}(v)\cap \{w\}|,|\mathcal{B}(w)\cap \{v\}| \in \{0,1\}$, so the value of $\varphi$ is not changed by this asymptotically and we can simplify to
% $$=\int \int_{(box \cap S_{k_n})^2} \varphi(v,w,k,k) \lambda^2 e^{-\alpha(y(v)+y(w))} dvdw$$
% 
%  
%
%We write $v=(x_1,y_1)$, $w=(x_2,y_2)$ for the two points and $d=|x_2-x_1|_{\pi e^{\frac{R}{2}}}$ for the horizontal distance between them. 
%
% 
%Let $X_3$ denote the number of vertices (of the Poisson process $\Pcal$) in the intersection of the neighbourhood balls of $v$ and $w$, let $X_1$ denote the number of vertices which are only in the neighbourhood ball of $v$ and let $X_2$ denote the number of vertices which are only in the neighbourhood ball of $w$. 
%
%$X_1,X_2, X_3$ are independent Poisson random variables with expectations $\mu_1,\mu_2,\mu_3$.
%
%By Lemma \ref{lem:mu3} we have $\mu_3 = \Theta(k^{2\alpha} d^{1-2\alpha})$.
%
%
%
%%\RD{From this calculation, we infer that $\mu_3 \rightarrow \infty$ (because $d =O(n)$ and $k \ll n$). Furthermore if $d \gg k$, then $k^{2\alpha} d^{1-2\alpha} \ll k$ and hence $\mu_3 \ll k$. The assumption $y_1 \in [y_{k_n,-},y_{k_n,+}]$ implies that $\mu_1+\mu_3 \sim k$.}
%
%Pick $0 < \delta < 2\alpha - 1$ and set $\epsilon = \delta(2\alpha-1)>0$. Pick $c=4\alpha+3$ as before. By Lemma \ref{lem:joint_distribution_Poisson} about the joint Poisson probability, it follows that if $d \gg k^{1+\delta}$, then 
%\begin{align*}
%\varphi(v,w,k,k)=\Pee(X_1+X_3=X_2+X_3=k) &=(1+o(1)) \Pee(X_1+X_3=k)\Pee(X_2+X_3=k) +O(k^{-c^2}) \\
%&= (1+o(1))\Pee(|\mathcal{B}(v) \cap \Pcal|=k)\Pee(|\mathcal{B}(w) \cap \Pcal|=k)+O(k^{-c^2})
%\end{align*}
% % $ \E Z = \Theta(\mu_{y_2})$. 
%
%From this for the integral from above (obtained from the Mecke formula), it follows that for $d \gg k_n^{1+\delta}$, 
%\begin{align*}
%&\int \int_{(box\cap S_{k_n})^2, d \gg k_n^{1+\delta}} \varphi(v,w,k_n,k_n) \lambda^2 e^{-\alpha (y(v)+y(w))} dvdw \\
%&\int \int_{(box\cap S_{k_n})^2, d \gg k_n^{1+\delta}} \left( (1+o(1))\Pee(|\mathcal{B}(v) \cap \Pcal|=k)\Pee(|\mathcal{B}(w) \cap \Pcal|=k)+O(k^{-c^2}) \right) \lambda^2 e^{-\alpha (y(v)+y(w))} dvdw 
%\end{align*}
%Now using that the integrand is independent of the horizontal coordinates  and the computation of the first moment of $N_{k_n}$ gives
%\begin{align*}
%&=O( n(n-k_n^{1+\delta}) \int_0^\infty \int_0^\infty \Pee(|\mathcal{B}(v) \cap \Pcal|=k)\Pee(|\mathcal{B}(w) \cap \Pcal|=k) \alpha^2 e^{-\alpha(y_1+y_2)} dy_1 dy_2)+O(k^{-c^2}) \\
%&=O(n(n-k_n^{1+\delta}) (k_n^{-2\alpha-1})^2)
%\end{align*} 
%which is asymptotic to the square of the first moment.
%
% If $d =O(k_n^{1+\delta})$, we can always upper bound $\Pee(X_1+X_3=X_2+X_3=k_n) \leq \Pee(Po(\mu_{y_1})=k_n)$:
%\begin{align*}
%&\int \int_{(box \cap S_{k_n})^2, d =O(k_n^{1+\delta})} \Pee(\deg(v)=\deg(w)=k_n)\lambda^2 e^{-\alpha (y(v)+y(w))} dvdw \\
%&=O(nk_n^{1+\delta} \int_{y_{k_n,-}}^\infty \int_{y_{k_n,-}}^\infty \Pee(Po(\mu_{y_1})=k_n) \alpha^2 e^{-\alpha(y_1+y_2)} dy_1 dy_2 ) \\
%&=O(n k_n^{1+\delta} k_n^{-2\alpha-1} k_n^{-2\alpha})
%\end{align*} 
%So after cancellation with the square of the first moment we get $O(k_n^{2+\delta} n^{-1})$ which tends to zero as $n \rightarrow \infty$ for $k_n \ll n^{\frac{1}{2\alpha+1}}$ for $\alpha>\frac{1}{2}$ because $\frac{2+\delta}{2\alpha+1} < 1$.

%\end{proof}

%To extend the result to the original hyperbolic random graph with a fixed number of $n$ vertices, we note that the claim in the previous proposition concerns / depends on the first and second moment of the number of degree $k_n$ vertices, so we only need to show:
%\begin{proposition}
%Let $\alpha > \frac{1}{2}$, $\nu > 0$ and consider the hyperbolic graph $G_{\H,n}(\alpha,\nu)$ and the Poisson version $G_{\widetilde{\H},n}(\alpha,\nu)$. Then
%\begin{enumerate}
%\item $\E[N_{\H,n}(k_n)] \sim \E[N_{\widetilde{\H},n}(k_n)]$,
%\item $\E[N_{\H,n}(k_n)^2] \sim \E[N_{\widetilde{\H},n}(k_n)^2]$.
%\end{enumerate}
%\end{proposition}


%\begin{lemma}
%Let $\alpha > \frac{1}{2}$, $\nu > 0$ and $\{k_n\}_{n \ge 1}$ be a non-decreasing sequence such that $k_n = \bigO{n^{\frac{1}{2\alpha + 1}}}$. Then, as $n \to \infty$,
%\begin{align*}
%	\Exp{N_{\Pcal,n}(k_n)^2} = (1 + o(1)) \Exp{N_{\Pcal,n}(k_n)}^2
%\end{align*}
%\end{lemma}


\section{Some results for random variables}

Here we summarize several known results for random variables and provide one technical lemma for Binomial random variables.

We start with the following concentration result which follows from~\cite[Theorem 4]{freedman1973another}, together with the note directly after it.

\begin{lemma}\label{lem:general_concentration_sum_indicators}
Let $X_n$ be a sum of $n$, possibly dependent, indicators and $c > 0$. Then
\[
	\Prob{|X_n - \Exp{X_n}| > c \Exp{X_n}} \le 2 e^{-\frac{c^2 \Exp{X_n}^2}{2}}.
\]
\end{lemma}

Next we recall two versions of the Chernoff bound for Poisson and Binomial random variables. They can be found in \cite[Lemma 1.2]{penrose2003random}; note that the Chernoff bound exists in many different versions, the original idea was developed by Chernoff in the context of efficiency of statistical hypothesis testing in \cite{Chernoff1952}):.

\begin{lemma}
Let $\Po(\lambda)$ denote a Poisson random variable with mean $\lambda$ and let $H(x) = x\log(x) - x + 1$. Then
\begin{align*}
	&\Prob{\Po(\lambda) \ge k} \le e^{-\lambda H(k/\lambda)} \quad \text{for all } k \ge \lambda\\
	&\Prob{\Po(\lambda) \le k} \le e^{-\lambda H(k/\lambda)} \quad \text{for all } k \le \lambda.
\end{align*}
\end{lemma}

It follows from the above lemma that
\begin{equation*}
	\Prob{\left|\mathrm{Po}(\lambda) - \lambda\right| \ge x} \le 2e^{-\frac{x^2}{2(\lambda + x)}}.
\end{equation*}
In particular, if $\lambda_n \to \infty$, then, for any $C>0$,
\begin{equation*}
	\Prob{\left|\mathrm{Po}(\lambda_n) - \lambda_n\right| \ge C \sqrt{\lambda_n\log(\lambda_n)}} \le 2e^{-\frac{C^2 \lambda_n \log(\lambda_n}{2\left(\lambda_n + C\sqrt{\lambda_n\log(\lambda_n)}\right)}}
	= \bigO{\lambda_n^{-\frac{C^2}{2}}}.
\end{equation*}

Note that these are equations~\eqref{eq:def_chernoff_bound_poisson} and~\eqref{eq:def_chernoff_bound_poisson_C} from the main text.

Let $\mathrm{Bin}(n,p)$ denote a Binomial random variable with $n$ trials and success probability $p$, and $0 < \delta < 1$. Then we have the following well-known Chernoff bound.
\begin{equation}\label{eq:def_chernoff_bound_binomial}
	\Prob{|\mathrm{Bin}(n,p) - np| > \delta np} \le e^{-\frac{\delta^2 np}{3}}.
\end{equation}

The following lemma gives an upper bound on the Binomial distribution for $p = \lambda/n$ in terms a Poisson distribution with mean $\lambda$.
The following lemma  gives a standard comparison between Binomial and Poisson distribution. We provide a short proof  for completeness.

\begin{lemma}\label{lem:binomial_poisson_bound}
Let $n \ge 1$, $0 < \lambda < n$. Then, for any integer $0 \le k \le n - 1$,
\[
	\Prob{\mathrm{Bin}(n,\lambda/n) = k} \le \frac{e}{\sqrt{2\pi}} \sqrt{\frac{n}{n-k}} \Prob{\Po(\lambda) = k}.
\]
\end{lemma}

\begin{proof}
Using Stirling's bounds (see e.g.~\cite{Dutkay2013},~\cite{Nanjundiah1959})
	\[
	\sqrt{2\pi s} \left(\frac{s}{e}\right)^{-s} \le s! \le e \sqrt{s} \left(\frac{s}{e}\right)^{-s},
	\]
we have
\begin{align*}
	\Prob{\mathrm{Bin}(n,\lambda/n) = k}
	&= \binom{n}{k} \left(\frac{\lambda}{n}\right)^k \left(1 - \frac{\lambda}{k}\right)^{n - k}\\
	&\le \frac{e}{\sqrt{2\pi}} \sqrt{\frac{n}{n - k}} \frac{n^n}{k!} (n-k)^{-(n-k)} e^{-k}
		\left(\frac{\lambda}{n}\right)^k \left(1 - \frac{\lambda}{n}\right)^{n - k}\\
	&= \frac{e}{\sqrt{2\pi}} \sqrt{\frac{n}{n - k}} \frac{\lambda^k e^{-\lambda}}{k!}
		\left(\frac{n-\lambda}{n - k}\right)^{n - k} e^{\lambda - k}\\
	&= \frac{e}{\sqrt{2\pi}} \sqrt{\frac{n}{n-k}} \Prob{\Po(\lambda) = k}
		\left(\frac{n-\lambda}{n - k}\right)^{n - k} e^{\lambda - k}.
\end{align*}
The result then follows by observing that $\left(\frac{n-\lambda}{n - k}\right)^{n - k} e^{\lambda - k} \le 1$ for all $0 < \lambda < n$ and $0 \le k \le n - 1$.
\end{proof}

%The following technical lemma establishes two results that are important in Section~\ref{sec:coupling_H_P_n}.
%
%\begin{lemma}\label{lem:concentration_height_binomial}
%Let $\mathrm{Bin}(m,p)$ denote a Binomial random variable with $m$ trials and success probability $p$, let $k_n \to \infty$ be a sequence of integers such that $k_n = \smallO{n}$ and fix some $C > 0$. Then the following holds for any $s, t > 0$:
%\begin{enumerate}
%\item for any $0 < p_n < 1$ such that $|n p_n - k_n| > C \sqrt{k_n \log(k_n)}$,
%\[
%	\Prob{\mathrm{Bin}(n-t,p_n) = k_n} = \bigO{k_n^{-\frac{C^2}{3}}}.
%\]
%\item for any $0 < p_n < 1$ such that $|n p_n - k_n| \le  C \sqrt{k_n \log(k_n)}$ and any sequence of integers $m_n$ such that $|m_n -n|\le C \sqrt{n \log(n)}$
%\[
%	\Prob{\mathrm{Bin}(n-t,p_n) = k_n} = (1+\smallO{1})\Prob{\mathrm{Bin}(m_n-s,p_n) = k_n}.
%\]
%\end{enumerate}
%\end{lemma}
%
%\begin{proof}
%First we observe that
%\[
%	\frac{\partial}{\partial x} \Prob{\mathrm{Bin}(m,x) = k} = \binom{m}{k}\left(k x^{k-1}(1-x)^{m-k}
%	- (m-k)x^k (1-x)^{m-k-1}\right).
%\]
%Hence, the function $x \mapsto \Prob{\mathrm{Bin}(m,x) = k}$ attains it maximum at $x = k/m$ and is strictly increasing on $(0,k/m]$ and strictly decreasing on $[k/m,1)$.
%
%We proceed with proving the first statement. Consider the case where $n p_n < k_n - C \sqrt{k_n \log(k_n}$. Define 
%\[
%	q_n = \frac{k_n - C \sqrt{k_n \log(k_n})}{n-t}
%\]
%and set $Y_n = \mathrm{Bin}(n-t,q_n)$. Then, since $q_n < k_n/(n-t)$ we get
%\begin{align*}
%	\Prob{\mathrm{Bin}(n-t,p_n) = k_n} &\le \Prob{Y_n = k_n} \le \Prob{Y_n > k_n-1}
%		= \Prob{Y_n > (1+\delta_n)(n-t)q_n},
%\end{align*}
%with
%\[
%	\delta_n = \frac{k_n - 1 -nq_n}{(n -t)q_n}
%	= \frac{C \sqrt{k_n \log(k_n)} - 1}{k_n - C \sqrt{k_n \log(k_n)}}.
%\]
%By a Chernoff bound we get
%\begin{align*}
%	\Prob{\mathrm{Bin}(n-t,p_n) = k_n} &\le \Prob{Y_n > (1+\delta_n)nq_n}
%		\le e^{-\frac{\delta_n^2 n q_n}{2}} \le e^{-\frac{\delta_n^2 n q_n}{3}}.
%\end{align*}
%The result now follows by observing that $\delta_n^2 n q_n = \Omega\left(C^2 \log(k_n)\right)$.
%
%For the case $n p_n > k_n - C \sqrt{k_n \log(k_n}$ we set $q_n = (k_n + C \sqrt{k_n \log(k_n})/n > k_n/n$ and 
%\[
%	\delta_n = \frac{nq_n - (k_n +1)}{n q_n}
%	= \frac{C \sqrt{k_n \log(k_n)} - 1}{k_n + C \sqrt{k_n \log(k_n)}}.
%\]
%Then, $p_n > q_n > k_n/n$ and $\delta_n^2 n q_n = \Omega\left(C^2 \log(k_n)\right)$. Thus by another Chernoff bound
%\begin{align*}
%	\Prob{\mathrm{Bin}(n-t,p_n) = k_n} &\le \Prob{Y_n = k_n} \le \Prob{Y_n < k_n+1}\\
%	&= \Prob{Y_n < (1-\delta_n)nq_n}
%		\le e^{-\frac{\delta_n^2 n q_n}{3}} = \bigO{k_n^{-\frac{C^2}{3}}}.
%\end{align*}
%
%Now let us prove the second statement. For this we first note that by Stirling's formula
%\[
%	\binom{n-t}{k_n} \sim (2\pi k_n)^{-1/2} \left(\frac{k_n}{n-t}\right)^{-k_n}\left(1 - \frac{k_n}{n-t}\right)^{k_n - (n-t)},
%\]
%and similarly for $\binom{m_n-s}{k_n}$. Therefore
%\begin{align*}
%	\binom{n-t}{k_n} \binom{m_n - s}{k_n}^{-1}
%	&= (1+\smallO{1}) \left(\frac{n - t}{m_n-s}\right)^{k_n}\left(1 - \frac{k_n}{n-t}\right)^{k_n - (n-t)}
%		\left(1 - \frac{k_n}{m_n-s}\right)^{m_n - s - k_n}
%\end{align*}
%and thus
%\begin{align*}
%	\frac{\Prob{\mathrm{Bin}(n-t,p_n) = k_n}}{\Prob{\mathrm{Bin}(m_n-s,p_n) = k_n}}
%	&= (1+\smallO{1}) \left(\frac{n - t - k_n}{n - t - p_n}\right)^{k_n - (n-t)} 
%		\left(\frac{m_n - s - k_n}{m_n-s - p_n}\right)^{k_n - (m_n - s)}
%\end{align*}
%Let us rewrite the first multiplicative term as 
%\[
%	\left(\frac{n - t - k_n}{n - t - p_n}\right)^{k_n - (n-t)} 
%	= \left(1 - \frac{k_n - p_n}{n - t -p_n}\right)^{k_n - (n-t)} := (1-x_n)^{k_n - (n-t)}.
%\]
%Then, using that for all $-1/2 \le x \le 1/2$, $-x - x^2 \le \log(1-x) \le -x$, we get
%\[
%	e^{-(k_n - (n-t))\left(x_n + x_n^2\right)}
%	\le \left(\frac{n - t - k_n}{n - t - p_n}\right)^{k_n - (n-t)}
%	\le e^{-(k_n - (n-t))x_n}.
%\]
%Similarly,
%\[
%	e^{-(m_n - s- k_n)\left(y_n + y_n^2\right)}
%	\le \left(\frac{m_n - s - k_n}{m_n-s - p_n}\right)^{m_n - s - k_n}
%	\le e^{-(m_n - s - k_n)y_n},
%\]
%where
%\[
%	y_n := \frac{k_n - p_n}{m_n - s -p_n}.
%\]
%
%We first show that 
%\begin{equation}\label{eq:binom_upper_bound_for_proof}
%	\lim_{n \to \infty} e^{-(k_n - (n-t))x_n}e^{-(m_n - s - k_n)y_n} = 1.
%\end{equation}
%With some algebra we get
%\begin{align*}
%	&\hspace{-30pt}-(k_n - (n-t))x_n - (m_n - s - k_n)y_n\\
%	&= \left(1 - \frac{k_n -p_n}{n - t -p_n}\right)(k_n - p_n) 
%		- \left(1 - \frac{k_n - p_n}{m_n - s - p_n}\right)(k_n - p_n)\\
%	&= (k_n - p_n)^2 \left(\frac{1}{m_n - s - p_n} - \frac{1}{n - t - p_n}\right)\\
%	&= (k_n - p_n)^2 \, \frac{n - m_n - t + s}{(m_n - s - p_n)(n - t - p_n)}.
%\end{align*}
%Now by our assumptions
%\[
%	(k_n - p_n)^2 = \bigT{k_n \log(k_n)}
%\]
%while
%\[
%	m_n - s - p_n = \bigT{n - t - p_n} = \bigT{n}
%\]
%and
%\[
%	-C \sqrt{n\log(n)} \le n - m_n \le C\sqrt{n \log(n)}.
%\]
%Therefore we conclude that
%\[
%	(k_n - p_n)^2 \, \frac{n - m_n - t + s}{(m_n - s - p_n)(n - t - p_n)} 
%	= \bigT{\frac{k_n \log(k_n) \sqrt{n\log(n)}}{n^2}}
%\]
%from which~\eqref{eq:binom_upper_bound_for_proof} follows. 
%
%In a similar way it follows that
%\[
%	\lim_{n \to \infty} e^{-(k_n - (n-t))x_n^2}e^{-(m_n - s - k_n)y_n^2} = 1,
%\]
%which implies that
%\[
%	\lim_{n \to \infty} \frac{\Prob{\mathrm{Bin}(n-t,p_n) = k_n}}{\Prob{\mathrm{Bin}(m_n-s,p_n) = k_n}}
%	= 1,
%\] 
%as thus finishes the proof.
%\end{proof}

\section{Concentration of heights for vertices with degree $k$}\label{sec:concentration_argument}

Here we will prove Proposition~\ref{prop:concentration_height_general}. We start by considering integration with respect to the function $\rho(y,k_n) = \Prob{\Po(\mu(y)) = k_n}$ (the degree distribution of a typical point in $\Ginf$). Here we show that we may restrict integration with respect to the \emph{height} $y$ to the interval $\Kcal_C(k_n) = [y_{k_n,C}^-, y_{k_n,C}^+]$ on which $\mu(y) = \bigT{k_n}$. Next we show that if we consider any other measure $\hat{\mu}_n(y)$ that is sufficiently equivalent to $\mu(y)$ on this interval (which will be made precise later), then we may replace $\hat{\rho}_n(y,k_n) := \Prob{\Po(\hat{\mu}_n(y)) = k_n}$ in integrals with $\rho(y,k_n)$. This then implies that we can also restrict integration to the interval $\Kcal_C(k_n)$. We will refer to such results as a \emph{concentration of heights} result.

We start with a concentration of heights result for the infinite model $\Ginf$ (Lemma~\ref{lem:concentration_argument}). We then present a generalization of this result (Lemma~\ref{lem:concentration_argument}) and use this to establish concentration of heights results for the Poissonized KPKVB $\GPo$ and finite box model $\Gbox$. 

Finally we provide a general result that allow to substitute $\hat{\rho}_n(y,k_n)$ in the integrand with $\rho(y,k_n)$ and show that this holds in particular for the degree distributions in $\GPo$ and $\Gbox$, given by, respectively $\rho_{\Po}(y,k_n) := \Prob{\Po(\mu_{\Po}(y)) = k_n}$ and $\rho_{\text{box}}(y,k_n) := \Prob{\Po(\mu_{\mathrm{box}}(y)) = k_n}$.

\subsection{Concentration of heights argument for the infinite model}

The next lemma states that for a large class of functions $h(y)$ and $k_n \to \infty$, to compute the integral 
\[
	\int_{0}^\infty \rho(y,k_n) h(y) e^{-\alpha y} \dd y
\]
it is enough to consider integration over a small interval on which $e^{y/2} \approx k_n$, instead of $\R_+$. 

\begin{lemma}\label{lem:concentration_argument}
Let $\alpha > \frac{1}{2}$, $\nu > 0$, $(k_n)_{n \ge 1}$ be any positive sequence such that $k_n \to \infty$ and $k_n = \smallO{n}$. Then the following holds.

For any continuous function $h : \R_+ \rightarrow  \R$, such that $h(y) = \bigO{e^{\beta y}}$ as $y \to \infty$ for some $\beta < \alpha$, 
\begin{equation}\label{eq:error_bound_int_rho_not_K}
	\int_{\R_+ \setminus \Kcal_C(k_n)} \rho(y,k_n) h(y) \alpha e^{-\alpha y} \dd y
	= \bigO{k_n^{-C^2/2}},
\end{equation}
as $n \to \infty$.
%\item If in addition $C > \sqrt{4\alpha + 1}$ and $h(a_n) \sim h(b_n)$ whenever $a_n \sim b_n$, as $n \to \infty$. Then, 
%\begin{equation}\label{eq:concentration_h_rho}
%	\int_0^\infty h(y) \rho(y,k_n) \alpha e^{-\alpha y} \dd y \sim  
%		2\alpha \xi^{2\alpha} h(2\log(k_n/\xi)) k_n^{-(2\alpha + 1)},
%\end{equation}
%as $n \to \infty$.
%\end{enumerate}
\end{lemma}

\begin{proof}
Since $\mu^\prime(y) = \mu(y)/2$, we get that
\[
	\frac{\partial \rho(y,k)}{\partial y} = \frac{1}{2}\left(k - \mu(y)\right)\rho(y,k),
\]
which implies that $\rho(y,k)$ attains its maximum at $\mu(y) = k$. Moreover we see that the derivative is strictly positive when $\mu(y) < k$ and strictly negative when $\mu(y) > k$. Since $\mu(y_{k,C}^-) < k$ and $\mu(y_{k,C}^+) > k$, we conclude that $\rho(y,k)$, as a function of $y$, is strictly increasing on $[0,y_{k,C}^-]$ and strictly decreasing on $[y_{k,C}^+,\infty)$.

Therefore, by our assumption on $h(y)$,
\begin{align*}
	&\hspace{-20pt}\int_{\R_+ \setminus \Kcal_C(k_n)} h(y) \rho(y,k_n) 
		\alpha e^{-\alpha y} \dd y\\
    &= \bigO{1} \int_0^{y_{k_n,C}^-} e^{\beta y} \rho(y,k_n) \alpha e^{-\alpha y} \dd y 
    	+ \bigO{1}\int_{y_{k_n,C}^+}^{\infty} e^{\beta y} \rho(y,k_n) \alpha e^{-\alpha y} \dd y \\
    &= \bigO{1} \int_0^{y_{k_n,C}^-} \rho(y,k_n) e^{-(\alpha-\beta) y} \dd y 
   		+ \bigO{1} \int_{y_{k_n,C}^+}^{\infty} \rho(y,k_n) e^{-(\alpha-\beta) y} \dd y\\
   	&\le \bigO{1}\rho(y_{k_n,C}^-,k_n)\int_0^{y_{k_n,C}^-} e^{-(\alpha-\beta) y} \, \dd y
   		+ \bigO{1} \rho(y_{k_n,C}^+,k_n) \int_{y_{k_n,C}^+}^{\infty} e^{-(\alpha-\beta) y} \, \dd y.
\end{align*}
Since $\alpha - \beta > 0$, we conclude that
\begin{equation}\label{eq:concentration_lemma_integral_bound}
	\int_{\R_+ \setminus \Kcal_C(k_n)} h(y) \rho(y,k_n) \alpha e^{-\alpha y} \dd y
	= \bigO{1} \left(\rho(y_{k_n,C}^-,k_n) + \rho(y_{k_n,C}^+,k_n)\right).
\end{equation}

We shall now bound the terms $\rho(y_{k_n,C}^\pm,k_n)$. We explicitly show the bound for $\rho(y_{k_n,C}^+,k_n)$, the computation for $\rho(y_{k_n,C}^-,k_n)$ is similar. First note that $\mu(y_{k_n,C}^+) = k_n + C \sqrt{\frac{\log(k_n)}{k_n}}$. Hence we can write
\begin{align*}
	\rho(y_{k_n,C}^+,k_n) &= \Prob{\Po(\mu(y_{k_n,C}^+)) = k_n} \le \Prob{\Po(\mu(y_{k_n,C}^+)) \ge k_n}\\
	&\le \Prob{\left|\Po(\mu(y_{k_n,C}^+)) - \mu(y_{k_n,C}^+)\right| \ge C \sqrt{\frac{\log(k_n)}{k_n}}}.
\end{align*}
Apply the Chernoff bound~\eqref{eq:def_chernoff_bound_poisson_C} then yields
\begin{equation}\label{eq:concentration_lemma_bound_an+}
	\rho(y_{k_n,C}^+,k_n) = \bigO{k_n^{-C^2/2}}.
\end{equation}
A similar analysis yields
\begin{equation}\label{eq:concentration_lemma_bound_an-}
	\rho(y_{k_n,C}^-,k_n) \le \bigO{k_n^{-C^2/2}}.
\end{equation} 
Plugging~\eqref{eq:concentration_lemma_bound_an-} and~\eqref{eq:concentration_lemma_bound_an+}  into~\eqref{eq:concentration_lemma_integral_bound} yields the result. 


%\paragraph{Proof of the second statement.} By the mean value theorem for definite integrals, there exists a $c_n \in (a_n^-, a_n^+)$ such that
%\[
%	\int_{a_n^-}^{a_n^+} h(y) \rho(y,k_n) \alpha e^{-\alpha y} \dd y
%	= h(c_n) \int_{a_n^+}^{a_n^+} \rho(y,k_n) \alpha e^{-\alpha y} \dd y.
%\]
%Since $\int_0^\infty \rho(y,k_n) \alpha e^{-\alpha y} \dd y = \bigT{k_n^{-(2\alpha + 1)}}$, taking any $C > \sqrt{4\alpha + 1}$, \eqref{eq:error_bound_int_rho_not_K} implies that
%\[
%	\int_{a_n^+}^{a_n^+} \rho(y,k_n) \alpha e^{-\alpha y} \dd y
%	= (1 + \smallO{1})\int_0^\infty \rho(y,k_n) \alpha e^{-\alpha y} \dd y,
%\]
%from which we conclude that (see~\eqref{eq:degree_distribution_P_asymptotics}),
%\[
%	\int_{a_n^+}^{a_n^+} \rho(y,k_n) \alpha e^{-\alpha y} \dd y = (1 + \smallO{1}) 2\alpha \xi^{2\alpha} k_n^{-(2\alpha + 1)},
%\]
%as $n \to \infty$. Finally, since $c_n \in (a_n^-, a_n^+)$ it follows that
%\[
%	\left|\frac{c_n}{2\log(k_n/\xi)} - 1\right| \le 2 C \sqrt{\frac{\log(k_n)}{k_n}}, 
%\]
%so that $c_n \sim 2\log(k_n/\xi)$. Therefore, by assumption on $h$ 
%\[
%	\int_{a_n^-}^{a_n^+} h(y) \rho(y,k_n) \alpha e^{-\alpha y} \dd y
%	\sim h(c_n) 2\alpha \xi^{2\alpha} k_n^{-(2\alpha + 1)} 
%	\sim 2\alpha \xi^{2\alpha} h(2\log(k_n/\xi)) k_n^{-(2\alpha + 1),}
%\]
%as $n \to \infty$.
\end{proof}

Note that we can tune the error in~\eqref{eq:error_bound_int_rho_not_K} by selecting an appropriately large $C > 0$, i.e. by restricting the function $h(y)$ inside the integral to an appropriate interval around $2\log(k_n/\xi)$. This makes Lemma~\ref{lem:concentration_argument} very powerful. As an example we give the following corollary, which allows us to bound integrals of functions $h_n(y)$ by considering their maximum of $\Kcal_{C}(k_n)$.

%\begin{corollary}\label{cor:concentration_of_heights_asymptotics}
%Let $h : \R_+ \to \R$ be any continuous function such that for some $\beta < \alpha$, $h(y) = \bigO{e^{\beta y}}$ as $y \to \infty$ and $h(a_n) \sim h(b_n)$ whenever $a_n \sim b_n$. Then for any other continuous function $g: \R_+ \to \R$, such that $g(y) \sim h(y)$ as $y \to \infty$
%\begin{equation}\label{eq:concentration_h_sim_rho}
%	\int_0^\infty g(y) \rho(y,k_n) \alpha e^{-\alpha y} \dd y \sim  
%		2\alpha \xi^{2\alpha} h(2\log(k_n/\xi)) k_n^{-(2\alpha + 1)},
%\end{equation}
%as $n \to \infty$.
%\end{corollary}
%
%\begin{proof}
%By assumption, $g$ satisfies the conditions of the second statement of Lemma~\ref{lem:concentration_argument}. Since in addition $g(2\log(k_n/\xi)) \sim h(2\log(k_n/\xi))$, the result follows.
%\end{proof}

\begin{corollary}\label{cor:concentration_heights_bounds_n}
Let $h_n : \R_+ \to \R_+$ be a sequence of continuous functions which such that for some $s \in \R$ and $\beta < \alpha$, as $n \to \infty$, $h_n(y) = \bigO{k_n^{s} e^{\beta y}}$ and $h_n(y) = \Omega(1)$, uniformly on $0 \le y \le (1-\varepsilon)R$ for some $0 < \varepsilon < 1$. Then for large enough $C > 0$, as $n \to \infty$,
\[
	\int_{\R_+} h_n(y) \rho(y,k_n) e^{-\alpha y} \dd y 
	= (1 + \smallO{1}) \int_{\Kcal_{C}(k_n)} h_n(y) \rho(y,k_n) \alpha e^{-\alpha y} \dd y.
\]
In particular,
\[
	\int_{\R_+} h_n(y) \rho(y,k_n) e^{-\alpha y} \dd y = \bigO{1} k_n^{-(2\alpha + 1)} \max_{y \in \Kcal_{C}(k_n)} h_n(y),
\]
as $n \to \infty$.
\end{corollary}

\begin{proof}
The second result follows immediately from the first. For the first result we note that by Lemma~\ref{lem:concentration_argument}
\begin{align*}
	\int_{\R_+ \setminus \Kcal_C(k_n)} h_n(y) \rho(y,k_n) e^{-\alpha y} \dd y
	&\le \bigO{k_n^s} \int_{\R_+ \setminus \Kcal_C(k_n)} e^{\beta y } \rho(y,k_n) e^{-\alpha y} \dd y\\
	&= \bigO{k_n^{s - C^2/2}}.
\end{align*}
By assumption on $h_n(y)$,
\[
	\int_{\Kcal_{C}(k_n)} h_n(y) \rho(y,k_n) e^{-\alpha y} \dd y 
	= \bigO{k_n^{s+2\beta}} \int_{\Kcal_{C}(k_n)} \rho(y,k_n) e^{-\alpha y} \dd y
	= \bigO{k_n^{s+2\beta - (2\alpha + 1)}},
\]
and
\[
	\int_{\Kcal_{C}(k_n)} h_n(y) \rho(y,k_n) e^{-\alpha y} \dd y 
	= \Omega(1) \int_{\Kcal_{C}(k_n)} \rho(y,k_n) e^{-\alpha y} \dd y
	= \Omega(k_n^{-(2\alpha + 1)}).
\]
Hence, by taking $C > 0$ such that $C^2/2 > \max\{2\alpha + 1 + s, 2\alpha +1 - \beta\}$ we get that
\[
	\int_{\R_+ \setminus \Kcal_C(k_n)} h_n(y) \rho(y,k_n) e^{-\alpha y} \dd y
	= \smallO{1} \int_{\Kcal_{C}(k_n)} h_n(y) \rho(y,k_n) e^{-\alpha y} \dd y.
\]
\end{proof}

%\begin{corollary}\label{cor:concentration_heights_bounds_n}
%Let $h : \R_+ \to \R_+$ be a continuous functions which such that for some $\beta < \alpha$, $h(y) = \bigO{ e^{\beta y}}$ as $y \to \infty$. Moreover, $h(y)$ satisfies that for any $\epsilon_n \to 0$, and $Q > 0$, $h(y \pm Q \log(1+\epsilon_n)) \sim h(y)$ as $n \to \infty$, uniformly on $0 \le y \le (1-\varepsilon)R$ for some $0 < \varepsilon < 1$. Then for large enough $C > 0$, as $n \to \infty$,
%\[
%	\int_{\R_+} h(y) \rho(y,k_n) e^{-\alpha y} \dd y = 
%\]
%as $n \to \infty$.
%\end{corollary}

%For functions $h_n(y) = k_n^s h(y)$ we obtain an asymptotic equivalent expression for the associated integral.
%
%\begin{corollary}\label{cor:concentration_heights_asymptotics_n}
%Let $h : \R_+ \to \R$ be a continuous function which  satisfies the conditions of Lemma~\ref{lem:concentration_argument} and let $h_n$ be a sequence of functions such that, as $n \to \infty$, $h_n(y) = \Omega(1)$ and $h_n(y) = \bigO{k_n^{s}} h(y)\rho(y,k_n)$, uniformly on $0 \le y \le R$. Then,
%\begin{equation}
%	\int_{\Rcal} h_n(y) \rho(y,k_n) f(x,y) \dd x \dd y 
%	\sim 2\alpha \xi^{2\alpha} \, n \, h_n(2\log(k_n/\xi)) k_n^{-(2\alpha + 1)},
%\end{equation}
%as $n \to \infty$.
%\end{corollary}
%
%\begin{proof}
%The result immediately follows by first applying Corollary~\ref{cor:concentration_heights_bounds_n} and then using the second statement from Lemma~\ref{lem:concentration_argument}.
%\end{proof}

%\begin{remark}[Concentration of heights argument]\label{rmk:concentration_argument}
%%\TM{ again, this is a loaded term. I would in the very least change to ``concentration of height'' everywhere }
%All the above corollaries use the same reasoning, namely that when the integrand contains $h_n(y) \rho(y,k_n)$, for some "nice" functions $h_n(y)$, then the main contribution is determined by the integration over $\Kcal_{C}(k_n)$.  This implies, for instance, that we only need to carefully analyze the functions $h_n(y)$ on $\Kcal_{C}(y)$, while for a certain class of functions we can even simply replace it with $h_n(2\log(k_n/\xi))$. We will refer collectively to any of these arguments as a \emph{concentration of heights argument}. For example, suppose we know that $h_n(y) = \Omega(1)$ and we can obtain some general crude bound $h_n(y) = \bigO{k_n^s} e^{\alpha/2 y}$ for some $s > 0$ for $y \in \R_+$. Then, if we can obtain a more precise bound $h_n(y) = (1 + \smallO{1}) k_n^2 e^{\alpha/2 y}$ uniformly on $\Kcal_{C}(k_n)$, by a concentration of heights argument (in this case Corollary~\ref{cor:concentration_heights_asymptotics_n})
%\begin{align*}
%	\int_{\Rcal} h_n(y) \rho(y,k_n) f(x,y) \dd x \dd y 
%	&= (1 + \smallO{1}) 2 \alpha \xi^{2\alpha} \, n \, k_n^2 (k_n/\xi)^{\alpha/2} k_n^{-(2\alpha + 1)} \\
%	&= (1 + \smallO{1}) 2\alpha \xi^{5\alpha/2} \, n \, k_n^{1 + 5\alpha/2}.
%\end{align*}
%\end{remark}
%
%
%
%\begin{remark}[Proof of Proposition~\ref{prop:asymp} revisited]
%Note that due to Proposition~\ref{prop:asymptotics_P}, the function $P(y)$ from Section~\ref{sec:asymptotics_average_clustering_ast_P} satisfies all the necessary conditions in Corollary~\ref{cor:concentration_of_heights_asymptotics} with
%\[
%	h(y) := \begin{cases}
%		e^{-\frac{y}{2}(4\alpha - 2)} c_\alpha \xi^{4\alpha - 2} &\mbox{if } \frac{1}{2} < \alpha < \frac{3}{4},\\
%		\frac{y}{2} e^{-\frac{y}{2}} &\mbox{if } \alpha = \frac{3}{4},\\
%		e^{-\frac{y}{2}} \frac{\alpha - \frac{1}{2}}{\alpha - \frac{3}{4}} &\mbox{if } \alpha > \frac{3}{4}.
%	\end{cases}
%\] 
%Hence, Proposition~\ref{prop:asymp} directly follows from Proposition~\ref{prop:asymptotics_P} and a concentration of heights argument (Corollary~\ref{cor:concentration_of_heights_asymptotics}).
%\end{remark}


%
%
%The next proposition establishes this result and we will spend the rest of this section on its proof.
%
%\begin{proposition}\label{prop:concentration_argument_all}
%The conclusions from Lemmas~\ref{lem:concentration_argument} and Corollaries~\ref{cor:concentration_of_heights_asymptotics}, \ref{cor:concentration_heights_bounds_n} and~\ref{cor:concentration_heights_asymptotics_n} still hold if we replace $\rho(y,k_n)$ with either $\rho_{\Po}(y,k_n)$ or $\rho_{\text{box}}(y,k_n)$.
%\end{proposition}

\subsection{Concentration of heights for the KPKVB and finite box model}\label{ssec:general_concentration_lemma}

Although powerful, the current versions of the concentration of heights argument is only valid for the function $\rho(y,k_n) := \Prob{\Po\left(\Mu{\BallPo{y}}\right) = k_n}$. We want to extend this to the Poissonized KPKVB model $\GPo$ and the finite box model $\Gbox$. To be more precise, recall that $\mu_{Po}(y) = \Mu{\BallHyp{y}}$ and $\mu_{\mathrm{box}}(y) = \Mu{\BallPon{y}}$ and let us define
\[
	\rho_{\Po}(y,k) = \Prob{\Po(\mu_{\Po}(y)) = k}
\] 
and
\[
	\rho_{\mathrm{box}}(y,k) = \Prob{\Po(\mu_{\mathrm{box}}(y) = k}.
\] 
Then we want when Lemma~\ref{lem:concentration_argument} to remain true if we replace $\rho(y,k_n)$ with either the function $\rho_{\Po}(y,k_n)$ or $\rho_{\text{box}}(y,k_n)$. To establish this result we first prove the following technical lemma.

\begin{lemma}\label{lem:concentration_heights_mu_approx}
Let $0 < \delta < 1$ and $k_n \to \infty$ be such that $k_n = \bigO{n^{1-\delta}}$. Let $\hat{\mu}_n(y)$ be a monotone increasing differentiable function such for some $0 < \varepsilon < 1$, $\hat{\mu}_n(y) = (1+\smallO{1})\mu(y)$ holds uniformly for $0 \le y \le (1-\varepsilon)R$. Furthermore, let $h : \R_+ \rightarrow  \R$, be a continuous function such that $h(y) = \bigO{e^{\beta y}}$ as $y \to \infty$ for some $\beta < \alpha$. Then, for $C > 0$ large enough
\[
	\int_0^\infty h(y) \hat{\rho}_n(y,k_n) e^{-\alpha y} \dd y \sim  
		\int_{\Kcal_C(k_n)} h(y) \rho(y,k_n) e^{-\alpha y} \dd y,
\]
as $n \to \infty$.
\end{lemma}

\begin{proof}
Take any $0 < \eta < \min\{\delta, \varepsilon\}$. We first show that we can restrict to integration to the interval $[0, (1-\eta)R)$. By construction $\eta < \varepsilon$, and hence by the assumption on $\hat{\mu}$ we have that $\hat{\mu}_n((1-\eta)R) = \bigT{\mu((1-\eta)R} = \bigT{n^{(1-\delta)}}$. Therefore, since $\eta<\delta$ and $k_n = \bigO{n^{1-\delta}}$, it follows that $\hat{\mu}_n((1-\eta)R)/k_n = \omega\left(n^{\delta - \eta}\right) = \omega(1)$ as $n \to \infty$. Hence $\hat{\rho}_n(y,k_n) \le \hat{\rho}((1-\eta)R,k_n)$ for all $y \ge (1-\eta)R$. It now follow that
\begin{align*}
	\int_{(1-\eta)R}^{R} h(y) \hat{\rho}_n(y,k_n) e^{-\alpha y} \dd y
	&= \bigO{1} \hat{\rho}_n((1-\eta)R,k_n) e^{-(\alpha-\beta)(1-\eta)R} \\
	&= \bigO{\hat{\rho}_n((1-\eta)R,k_n) n^{-2(\alpha-\beta)(1-\eta)}},
\end{align*}
where we used that that $h(y) = \bigO{e^{\beta y}}$. Next we use Stirling's bound $k! \ge \sqrt{2\pi} k^{k+\frac{1}{2}} e^{-k}$, to bound $\hat{\rho}_n((1-\eta)R,k_n)$,
\begin{align*}
	\hat{\rho}_n((1-\eta)R,k_n) &= \Prob{\Po(\hat{\mu}_n((1-\eta)R)) = k_n} \\
	&= \frac{\hat{\mu}_n((1-\eta)R)^{k_n}}{k_n!} e^{-\hat{\mu}_n((1-\eta)R)}\\
	&= \bigO{1} k_n^{-1/2} \left(\frac{\hat{\mu}_n((1-\eta)R)}{k_n}\right)^{k_n} e^{k_n - \hat{\mu}_n((1-\eta)R)}\\
	&= \bigO{1} k_n^{-1/2} e^{k_n\left(1 - \frac{\hat{\mu}_n((1-\eta)R)}{k_n}+ \log\left(\frac{\hat{\mu}_n((1-\eta)R)}{k_n}\right)\right)}\\
	&\le \bigO{1} k_n^{-1/2} e^{-\hat{\mu}_n((1-\eta)R)/2},
\end{align*}
where the last line follows since $\hat{\mu}_n((1-\eta)R)/k_n \to \infty$ and $1 - x + \log(x) \le -x/2$ for large enough $x$. Because $\hat{\mu}_n((1-\eta)R) = \bigT{n^{(1-\delta)}}$ we conclude that for any $C > 0$
\[
	\int_{(1-\delta)R}^{R} h(y) \hat{\rho}_n(y,k_n) e^{-\alpha y} \dd y
	= \bigO{k_n^{-1/2} n^{-2(\beta-\alpha)(1-\delta)} e^{-n^{(1-\delta)}/2}}
	= \bigO{k_n^{-C^2/2}}.
\]

It thus remains to prove that 
\[
	\int_{y_{k_n,C}^+}^{(1-\eta)R} h(y) \hat{\rho}_n(y,k_n) e^{-\alpha y} \dd y
	\le \bigO{k_n^{-C^2/8}}
\]
and
\[
	\int_0^{y_{k_n,C}^-} h(y) \hat{\rho}_n(y,k_n) e^{-\alpha y} \dd y
	\le \bigO{k_n^{-C^2/8}}.
\]

Define $\hat{y}_n^\pm$ to be such that $\hat{\mu}_n(y_n^\pm) = k_n \pm C \sqrt{k_n \log(k_n)}$. Then by assumption on $\hat{\mu}_n$ we have that
\[
	k_n \pm C \sqrt{k_n \log(k_n)} = \hat{\mu}_n(\hat{y}_n^\pm) = (1+\smallO{1}) \mu(\hat{y}_n^\pm)
	= (1+\smallO{1}) \xi e^{\hat{y}_n^\pm/2}.
\]
and hence
\[
	\hat{y}_n^\pm = 2\log\left(\frac{k_n \pm C \sqrt{k_n \log(k_n)}}{\xi}\right) - 2\log(1+\smallO{1})
	= y_{k_n,C}^\pm - 2\log(1+\smallO{1}) := y_{k_n,C}^\pm - \epsilon_n,
\]
with $\epsilon_n \to 0$. Recall that $\hat{\mu}_n(y)$ is monotonic increasing. Now let $n$ be large enough such that  $\hat{\mu}_n(\hat{y}_n^+ - \epsilon_n) > k_n + \frac{C}{2}\sqrt{k_n \log(n)}$. Then 
\begin{align*}
	\int_{y_{k_n,C}^+}^{(1-\eta)R} h(y) \hat{\rho}_n(y,k_n) e^{-\alpha y} \dd y
	&\le \int_{\hat{y}_n^+ - \epsilon_n}^{(1-\eta)R} h(y) \hat{\rho}_n(y,k_n) e^{-\alpha y} \dd y\\
	&\le \hat{\rho}_n(\hat{y}_n^+ - \epsilon_n,k_n) \int_{\hat{y}_n^+ + \eta}^{(1-\eta)R} h(y) e^{-\alpha y} \dd y\\
	&\le \bigO{1} \hat{\rho}_n(\hat{y}_n^+ - \epsilon_n,k_n),
\end{align*}
where we used that $\hat{\mu}_n(y)$ is monotonic increasing and $\hat{\rho}_n(y,k_n)$ is decreasing for all $y \ge \hat{y}_n^+$. Write $\lambda_n = \hat{\mu}_n(\hat{y}_n^+ - \epsilon_n)$. Then, similar to the proof of Lemma~\ref{lem:concentration_argument}, we have that
\[
	\hat{\rho}_n(\hat{y}_n^+ - \epsilon_n,k_n)
	\le \Prob{\left|\Po(\lambda_n) - \lambda_n\right| \ge \frac{C}{2} \sqrt{k_n \log(k_n)}}
	\le \bigO{k_n^{-C^2/8}},
\]
where the last step follows from the Chernoff bound~\eqref{eq:def_chernoff_bound_poisson_C} with $C = C/2$.

In a similar fashion we can let $n$ be large enough such that
$\hat{\mu}_n(\hat{y}_n^- + \epsilon_n) < k_n - \frac{C}{2}\sqrt{k_n \log(n)}$ can show that
\begin{align*}
	\int_0^{y_{k_n,C}^-} h(y) \hat{\rho}_n(y,k_n) e^{-\alpha y} \dd y
	&\le \int_0^{\hat{y}_n^- + \epsilon_n} h(y) \hat{\rho}_n(y,k_n) e^{-\alpha y} \dd y\\
	&\le \bigO{1} \hat{\rho}_n(\hat{y}_n^- + \epsilon_n,k_n) = \bigO{k_n^{-C^2/8}}.
\end{align*}
\end{proof}

The conclusion of Lemma~\ref{lem:concentration_heights_mu_approx} is that as long as $\mu_{Po}(y)$ and $\mu_{\mathrm{box}}(y)$ are $(1+\smallO{1})\mu(y)$, uniformly on $[0,(1-\varepsilon)R]$, then indeed the concentration of height result (Lemma~\ref{lem:concentration_argument}) also holds in both $\GPo$ and $\Gbox$. This was proven in Lemma~\ref{lem:average_degree_P_n} and Lemma~\ref{lem:average_degree_G_box}, respectively. For completeness we give the proof of Proposition~\ref{prop:concentration_height_general}.

\begin{proofof}{Proposition~\ref{prop:concentration_height_general}}
The proof for $\hat{\mu}(y) = \mu(y)$ follows directly from Lemma~\ref{lem:concentration_argument}.

Now consider the case $\hat{\mu}(y) = \mu_{Po}(y)$. Then by Lemma~\ref{lem:average_degree_P_n} $\hat{\mu}(y) = (1+\smallO{1}) \mu(y)$, uniformly on $[0,(1-\varepsilon)R]$ and thus in particular on $\Kcal_C(k_n)$. Finally we note that by Lemma~3.3. in \cite{gugelmann2012random} $\hat{\mu}(y)$ is monotonic increasing. The statement then follows by applying Lemma~\ref{lem:concentration_heights_mu_approx}.

Finally, for $\hat{\mu}(y) = \mu_{\mathrm{box}}(y)$ we recall that by Lemma~\ref{lem:average_degree_G_box} $\hat{\mu}(y) = (1+\smallO{1})\mu(y)$. More precisely,
\[
	\hat{\mu}(y) = \mu(y)(1-\phi_n(y)) = \begin{cases}
		\mu(y)\left(1 - e^{-(\alpha-\frac{1}{2})R}\right) &\mbox{if } 0 \le y \le 2\log(\pi/2) \\
		\mu(y)\left(1 - \phi_n(y)\right) &\mbox{if } 2\log(\pi/2) < y \le (1-\varepsilon)R,
	\end{cases}
\]
where
\[
	\phi_n(y) :=  \left(\frac{\pi}{2}\right)^{-(2\alpha - 1)} \hspace{-3pt} e^{-(\alpha - \frac{1}{2})(R - y)}
				+ \frac{\nu}{\xi}e^{-(\alpha - \frac{1}{2})R - \frac{y}{2}} - \frac{\nu}{\xi}\left(\frac{\pi}{2}\right)^{-2\alpha} e^{-(\alpha-\frac{1}{2})(R - y)}.
\]
Note that $\phi_n(2\log(\pi/2)) = e^{-(\alpha-\frac{1}{2})R}$. In addition, since $|\phi_n(y)| \le \bigO{e^{-(\alpha-\frac{1}{2})\varepsilon R}}$ for $0 \le y \le (1-\varepsilon)R$ we have that $\hat{\mu}(y)$ is monotonic increasing for large enough $n$. The statement now follows by applying Lemma~\ref{lem:concentration_heights_mu_approx}.
\end{proofof}

%If in addition the derivatives of the measures $\mu_{Po}(y)$ and $\mu_{\mathrm{box}}(y)$ are also equivalent to the derivative of $\mu(y)$ we can even replace $\rho_{\Po}(y,k_n)$ and $\rho_{\text{box}}(y,k_n)$ in the integration with $\rho(y,k_n)$. 
%
%\begin{lemma}\label{lem:mu_substitution}
%Let $0 < \delta < 1$ and $k_n \to \infty$ be such that $k_n = \bigO{n^{1-\delta}}$. Let $\hat{\mu}_n(y)$ be a monotone increasing differentiable function such for some $0 < \varepsilon < 1$, 
%\begin{align*}
%	&\mathrm{i)} \quad \hat{\mu}_n(y) = (1+\smallO{1})\mu(y)
%	\quad \text{and}
%	&&\mathrm{ii)} \quad \hat{\mu}_n^\prime(y) = (1+\smallO{1})\mu^\prime(y),
%\end{align*}
%holds uniformly for $0 \le y \le (1-\varepsilon)R$.
%
%Furthermore, let $h : \R_+ \rightarrow  \R$, be a continuous function such that $h(y) = \bigO{e^{\beta y}}$ as $y \to \infty$ for some $\beta < \alpha$. Finally, assume that for any $\epsilon_n \to 0$, and $Q > 0$, $h(y \pm Q \log(1+\epsilon_n)) \sim h(y)$ as $n \to \infty$, uniformly on $y \in \Kcal_C(k_n)$. Then the following holds for $C > 0$ large enough:
%\begin{equation}\label{eq:mu_substitution}
%	\int_0^\infty h(y) \hat{\rho}_n(y,k_n) \alpha e^{-\alpha y} \dd y \sim  
%		\int_0^\infty h(y) \rho(y,k_n) \alpha e^{-\alpha y} \dd y,
%\end{equation}
%as $n \to \infty$.
%\end{lemma}
%
%\begin{proof} 
%
%
%%For the range $(0,Q)$ we have, for any $C > 0$,
%%\begin{align*}
%%	\int_0^Q h(y) \hat{\rho}_n(y,k_n) e^{-\alpha y} \dd y
%%	&= \bigO{1} \hat{\rho}_n(Q, k_n) \\
%%	&= \bigO{1} k_n^{-1/2} e^{-k_n(\log(k_n)-\hat{\mu}_n(Q))} = \bigO{k_n^{-C^2/2}}.
%%\end{align*}
%
%Fix $y_0 > 0$ such that $\hat{\mu}(y_0) \ge \xi$ and let $0 < \eta \le \min\{\delta, \varepsilon,\varepsilon^\prime\}$. Then since, for large enough $n$, $\Kcal_C(k_n) \subseteq [y_0, (1-\eta)R]$, by Lemma~\ref{lem:concentration_heights_mu_approx} we only need to show that,
%\begin{equation}\label{eq:substitution_result_main}
%	 \int_{y_0}^{(1-\eta)R} h(y) \hat{\rho}_n(y,k_n) e^{-\alpha y} \dd y \sim 
%	 \int_0^\infty h(y) \rho(y,k_n) e^{-\alpha y} \dd y.
%\end{equation}
%
%We will do this by a substitution of variables. Define the function $z(y)=2\ln\frac{\hat{\mu}_n(y)}{\xi}$ (note that $z(y)$ is well-defined as $\mu_{Po,n}(y)\geq 0$ and that $z(y)$ is bijective because $\mu_{Po,n}(y)$ is strictly monotone increasing and continuous, see Lemma~3.3. in \cite{gugelmann2012random}). By rearranging, we have that
%\begin{align*}
%\hat{\mu}_{n}(y) = \xi e^{\frac{z(y)}{2}}.
%\end{align*}
%By our assumption on $\hat{\mu}_n$ it follows that uniformly for all $0\leq y\leq (1-\varepsilon^\prime)R$, $\mu(y) = (1+o(1))\hat{\mu}_{n}(y) = (1+o(1))\xi e^{\frac{z(y)}{2}}$. In particular we have $z(y) = y + 2\log(1+\smallO{1})$ and hence $e^{-\alpha y} = (1+o(1))e^{-\alpha z(y)}.$
%
%Next we note that $\mu^\prime(y) = \frac{1}{2} \mu(y)$. Therefore, by the assumptions on $\hat{\mu}_n$ and its derivative we have that uniformly for $0 \le y \le (1-\varepsilon^\prime)R$
%\[
%	z^\prime(y) = \frac{2\hat{\mu}_{n}^\prime(y)}{\hat{\mu}_{n}(y)} = 1+o(1).
%\]
%We now have 
%\begin{align*}
%	&\hspace{-20pt}\int_{y_0}^{(1-\delta)R} h(y) \Prob{\Po(\mu_{Po}(y))=k_n} \alpha e^{-\alpha y}dy \\
%	&= (1+o(1)) \int_{y_0}^{(1-\delta)R} h\left(z(y) + 2\log(1+\smallO{1})\right) \,
%		\Prob{\Po(\xi e^{\frac{z(y)}{2}})=k_n}\alpha e^{-\alpha z(y)} z^\prime(y) dy. \\
%	&= (1+o(1)) \int_{y_0}^{(1-\delta)R} h\left(z(y) + 2\log(1+\smallO{1})\right) \,
%			\rho(z(y),k_n) \alpha e^{-\alpha z(y)} z^\prime(y) dy\\
%	&= (1 + \smallO{1}) \int_{z(y_0)}^{z((1-\delta)R)} h\left(z + 2\log(1+\smallO{1})\right) 
%			\rho(y,k_n)\alpha e^{-\alpha z} dz,
%\end{align*}
%where we applied integration by substitution to the last integral, i.e. used the new variable $z=z(y)$. 
%
%Note that since the function $y \mapsto 2\ln \frac{y}{\xi}$ is monotone increasing it follows that for large enough $n$, 
%$\Kcal_{C}(k_n) \subset [z(y_0),z((1-\epsilon)R)]$. Moreover, by the assumption on $h$ we have that $h\left(z + 2\log(1+\smallO{1})\right) = (1+\smallO{1})h(y)$, uniformly on $\Kcal_{C}(k_n)$. Therefore, by Lemma~\ref{lem:concentration_argument} it follows that
%\begin{align*}
%	&\hspace{-30pt}\int_{z(y_0)}^{z((1-\delta)R)} 
%		h\left(z + 2\log(1+\smallO{1})\right) \rho(y,k_n)\alpha e^{-\alpha z} dz\\
%	&= (1+o(1))\int_{\Kcal_C(k_n)} h\left(z + 2\log(1+\smallO{1})\right) \rho(y,k_n) \alpha e^{-\alpha z}dz\\
%	&= (1+o(1))\int_{\Kcal_C(k_n)} h\left(z\right) \rho(y,k_n) \alpha e^{-\alpha z}dz\\
%	&= (1+\smallO{1}) \int_0^\infty h\left(z\right) \rho(y,k_n) \alpha e^{-\alpha z}dz.
%\end{align*}
%
%\end{proof}


%The following lemma immediately implies that $\Mu{\BallPon{y}}$ satisfies the conditions of Lemma~\ref{lem:concentration_argument_rho_approximation}.

%\begin{lemma}\label{lem:average_degree_P_n}
%For all $y > 2\log(\pi/2)$,
%\[
%	\Mu{\BallPon{p}} = \Mu{\BallPo{p}}\left(1 - \phi_n(y)\right)
%\]
%%\TM{ where did the curly B's go? }\PvdH{Typo. Fixed}
%where $\phi_n(y) \ge 0$ is given by
%\[
%	\phi_n(y) = \left(\frac{\pi}{2}\right)^{-(2\alpha - 1)}e^{-(\alpha-\frac{1}{2})(R - y)}
%	- \frac{(2\alpha - 1)\pi}{4\alpha}\left(\left(\frac{\pi}{2}\right)^{-2\alpha} 
%	e^{-(\alpha - \frac{1}{2})(R - y)} - e^{-(\alpha - \frac{1}{2})R - \frac{y}{2}}\right).
%\]
%On the other hand, if $y \le 2 \log(\pi/2)$ then
%\[
%	\mu(\BallPon{p}) = \mu(\BallPo{p})\left(1 - e^{-(\alpha - \frac{1}{2})R}\right).
%\]
%\end{lemma}
%
%\begin{proof}
%First note that since we have identified the boundaries of $[-\frac{\pi}{2}e^{\frac{R}{2}}, \frac{\pi}{2}e^{\frac{R}{2}}]$ we can assume, without loss of generality, that $p = (0,y)$. We then have that the boundaries of $\BallPon{p}$ are given by the equations $x^\prime = \pm e^{\frac{y+y^\prime}{2}}$, which intersect the left and right boundaries of $[-\frac{\pi}{2}e^{\frac{R}{2}}, \frac{\pi}{2}e^{\frac{R}{2}}]$ at height
%\[
%	h(y) = R + 2 \log\left(\frac{\pi}{2}\right) - y.
%\]
%Therefore, if $y \le 2 \log(\pi/2)$ this intersection occurs above the height $R$ of the box $\Rcal$ while in the other case the full region of the box above $h(y)$ is connected to $p$. 
%
%We will first consider the case where $y > 2 \log(\pi/2)$. Recall that $\mu(\BallPo{p}) = \xi e^{\frac{y}{2}}$ where $\xi = \frac{4\alpha \nu}{(2\alpha - 1)\pi}$. Then, after some simple algebra, we have that
%\begin{align*}
%	\mu(\BallPon{p})
%	&= \int_0^{h(y)} \int_{-\frac{\pi}{2}e^{\frac{R}{2}}}^{\frac{\pi}{2}e^{\frac{R}{2}}} 
%		\ind{|x^\prime| \le e^{\frac{y+y^\prime}{2}}} f_{\alpha,\nu}(x^\prime,y^\prime) \, dx^\prime \, dy^\prime\\
%	&\hspace{10pt}+ \int_{h(y)}^{R} \int_{-\frac{\pi}{2}e^{\frac{R}{2}}}^{\frac{\pi}{2}e^{\frac{R}{2}}} 
%		f_{\alpha,\nu}(x^\prime,y^\prime) \, dx^\prime \, dy^\prime\\
%	&= \frac{2 \alpha \nu}{\pi} e^{\frac{y}{2}} \int_0^{h(y)} e^{-(\alpha - \frac{1}{2})y^\prime} \, dy^\prime
%		+ \alpha \nu e^{\frac{R}{2}} \int_{h(y)}^{R} e^{-\alpha y^\prime} \, dy^\prime \\
%	&= \xi e^{\frac{y}{2}}\left(1 - \left(\frac{\pi}{2}\right)^{-(2\alpha - 1)} 
%		e^{-(\alpha - \frac{1}{2})(R - y)}\right)\\
%	&\hspace{10pt}+ \nu e^{\frac{R}{2}}\left(\left(\frac{\pi}{2}\right)^{-2\alpha} e^{-\alpha(R - y)} 
%		- e^{-\alpha R}\right)\\
%	&= \mu(\BallPo{p})\left(1 - \phi_n(y)\right).
%\end{align*}
%Since, for all $\alpha > \frac{1}{2}$,
%\[
%	\left(\frac{\pi}{2}\right)^{-(2\alpha - 1)} \ge \frac{(2\alpha - 1)\pi}{4\alpha} \left(\frac{\pi}{2}\right)^{-2\alpha}
%\]
%it follows that $\phi_n(y) \ge 0$.
%
%When $y \le 2 \log(\pi/2)$ we have
%\begin{align*}
%	\mu(\BallPon{p})
%	&= \int_0^{R} \int_{-\frac{\pi}{2}e^{\frac{R}{2}}}^{\frac{\pi}{2}e^{\frac{R}{2}}} 
%		\ind{|x^\prime| \le e^{\frac{y+y^\prime}{2}}} f_{\alpha,\nu}(x^\prime,y^\prime) \, dx^\prime \, dy^\prime\\
%	&= \frac{2 \alpha \nu}{\pi} e^{\frac{y}{2}} \int_0^{R} e^{-(\alpha - \frac{1}{2})y^\prime} \, dy^\prime\\
%	&= \mu(\BallPo{p})\left(1 - e^{-(\alpha - \frac{1}{2})R}\right).
%\end{align*}
%\end{proof}


%From the definition of $\phi_n(y)$ in Lemma~\ref{lem:average_degree_P_n} it is immediate that $\rho_{\mathrm{box}}(y,k)$ satisfies the conditions for $\hat{\rho}_n(y,k)$ in Lemma~\ref{lem:concentration_argument_rho_approximation}. We thus have the following corollary.
%
%
%
%We will now show that a concentration of heights argument also applies to the KPKVB model. Due to the hyperbolic distance formula, the computations are however more involved than for the finite box model. Recall that under the coupling between the hyperbolic random graph and the finite box model, for two points $p, p^\prime$ with $y + y^\prime < R$, $p^\prime \in \BallHyp{p}$ exactly when $|x-x^\prime|_{\pi e^{R/2}} \le \Phi(y,y^\prime)$. In this setting, the coupling lemma (Lemma~\ref{lem:asymptotics_Omega_hyperbolic}) gives that  
%\[
%	e^{\frac{1}{2}(y+y^\prime)} - K e^{\frac{3}{2}(y+y^\prime) - R} \leq \Phi(y, y^\prime) 
%		\leq  e^{\frac{1}{2}(y+y^\prime)} + K e^{\frac{3}{2}(y+y^\prime) - R},
%\]
%for some constant $K$. This result enables us to determine the measure of a ball around a given point $p=(0,y)$. Recall that the hyperbolic ball $\BallHyp{p}$ is a subset of $\Rcal$ and not of the hyperbolic disc $\Dcal_{R}$, i.e. the balls $\BallHyp{p}$ ``live" in the finite box and not on the hyperbolic disc. We start with the following preliminary result.
%
%\begin{lemma}\label{lem:hyperbolic_ball_lower_part}
%Let $\Phi(y,y^\prime)$ be defined as in~\eqref{eq:def_Omega_hyperbolic}. Then, for any $0 \le \delta < 1$
%\[
%	\lim_{n \to \infty} \sup_{0 < y \le (1-\varepsilon)R} \left|\Mu{\BallPo{y}}^{-1} \frac{2\nu \alpha}{\pi} \int_0^{(1-\delta)(R-y)} \Phi(y,y^\prime) e^{-\alpha y^\prime} \dd y^\prime  - 1\right| = 0.
%\]
%\end{lemma}
%
%\begin{proof}
%Recall that $\Mu{\BallPo{0,y}} = \xi e^{y/2}$ where $\xi = \frac{4\alpha\nu}{\pi(2\alpha - 1)}$ and that by Lemma~\ref{lem:asymptotics_Omega_hyperbolic}
%\[
%	\left|\Phi(y,y^\prime) - e^{\frac{y+y^\prime}{2}}\right| \le K e^{\frac{3}{2}(y+y^\prime) - R},
%\]
%for all $y + y^\prime < R$. Also,
%\[
%	\Mu{\BallPo{y}} = \frac{2 \nu \alpha}{\pi}\int_0^\infty e^{\frac{y + y^\prime}{2}} e^{-\alpha y^\prime} \dd y^\prime. 
%\]
%
%Therefore we have 
%\begin{align*}
%	&\left|\Mu{\BallPo{y}}^{-1} \frac{2\nu \alpha}{\pi} \int_0^{(1-\delta)(R-y)} \Phi(y,y^\prime) e^{-\alpha y^\prime} 
%		\dd y^\prime  - 1\right|\\
%	&= \frac{1}{\Mu{\BallPo{y}}}\left|\frac{2\nu \alpha}{\pi} \int_0^{(1-\delta)(R-y)} \Phi(y,y^\prime) 
%		e^{-\alpha y^\prime} \dd y^\prime 
%		- \frac{2\nu \alpha}{\pi} \int_0^\infty e^{\frac{y + y^\prime}{2}} e^{-\alpha y^\prime} \dd y^\prime\right|\\
%	&\le \frac{2\nu \alpha}{\pi \Mu{\BallPo{y}}} \int_0^{(1-\delta)(R-y)}\left|\Phi(y,y^\prime) - 
%		e^{\frac{y + y^\prime}{2}} \right| e^{-\alpha y^\prime} \dd y^\prime
%		+ \frac{2\nu \alpha}{\pi \Mu{\BallPo{y}}} \int_{(1-\delta)(R-y)}^\infty e^{\frac{y + y^\prime}{2}} 
%		e^{-\alpha y^\prime} \dd y^\prime\\
%	&\le \frac{2 K \nu \alpha}{\pi \Mu{\BallPo{y}}} e^{\frac{3}{2}y - R} \int_0^{(1-\delta)(R-y)} 	
%		e^{(\frac{3}{2}-\alpha)y^\prime} \dd y^\prime +  e^{-(\alpha-\frac{1}{2})(1-\delta)(R-y)}.
%\end{align*}
%%\begin{align*}
%%	\frac{2\nu \alpha}{\pi} \int_0^{(1-\delta)(R-y)} \Phi(y,y^\prime) e^{-\alpha y^\prime} 
%%	&\le \frac{2\nu \alpha}{\pi} \int_{0}^{(1-\delta)(R-y)} \left(e^{\frac{y + y^\prime}{2}} + K e^{\frac{3}{2}(y + y^\prime) - R}\right)
%%		e^{-\alpha y^\prime} \dd y^\prime\\
%%	&= \Mu{\BallPo{0,y}}\left(1 - e^{-(\alpha - \frac{1}{2})(1-\delta)(R - y)}\right) \\
%%	&\hspace{10pt}+ \frac{2\nu \alpha}{\pi} K e^{\frac{3 y}{2} - R}\int_0^{(1-\delta)(R-y)} e^{(\frac{3}{2} - \alpha)y^\prime} \dd y^\prime
%%\end{align*}
%For the last term it holds that
%\[
%	\lim_{n \to \infty} \sup_{0 < y \le (1-\varepsilon)R}  e^{-(\alpha-\frac{1}{2})(1-\delta)(R-y)} = 0.
%\]
%We first compute the integral in the first term, which depends on the value of $\alpha$,
%\[
%	\int_0^{(1-\delta)(R-y)} e^{(\frac{3}{2} - \alpha)y^\prime} \dd y^\prime
%	= \begin{cases}
%		\frac{2}{3 - 2\alpha}\left(e^{(\frac{3}{2} - \alpha)(1-\delta)(R-y)} - 1\right) &\mbox{if } 1/2 < \alpha < 3/2,\\
%		(1-\delta)(R-y) &\mbox{if } \alpha = 3/2,\\
%		\frac{2}{2\alpha-3}\left(1 - e^{-(\alpha - \frac{3}{2})(1-\delta)(R-y)}\right) &\mbox{if } \alpha > 3/2.
%	\end{cases}
%\]
%This implies that
%\begin{align*}
%	&\frac{2 K \nu \alpha}{\pi \Mu{\BallPo{y}}} e^{\frac{3 y}{2} - R}
%		\int_0^{(1-\delta)(R-y)} e^{(\frac{3}{2} - \alpha)y^\prime} \dd y^\prime\\
%	&= \begin{cases}
%		\frac{(2\alpha - 1)K}{3 - 2\alpha}\left(e^{-(\alpha - \frac{1}{2})(R - y)-(\frac{3}{2}-\alpha)\delta(R-y)} 
%			- e^{-(R - y)}\right)
%		&\mbox{if } 1/2 < \alpha < 3/2,\\
%		\frac{(2\alpha -1)K}{2} (1-\delta)(R - y)e^{-(R - y)} &\mbox{if } \alpha = 3/2,\\
%		\frac{(2\alpha - 1)K}{2\alpha - 3} \left(e^{-(R - y)} - e^{-(\alpha - \frac{1}{2})(R - y) 
%			-(\alpha - \frac{3}{2})(R-y)}\right)
%		&\mbox{if } \alpha > 3/2,
%	\end{cases}
%\end{align*}
%and hence
%\[
%	\lim_{n \to \infty} \sup_{0 < y \le (1-\varepsilon)R}
%	\frac{2 K \nu \alpha}{\pi \Mu{\BallPo{y}}} e^{\frac{3 y}{2} - R}
%			\int_0^{(1-\delta)(R-y)} e^{(\frac{3}{2} - \alpha)y^\prime} \dd y^\prime = 0,
%\]
%which completes the proof.
%\end{proof}
%
%We can now show that the measure of the balls in the KPKVB model and the infinite model are asymptotically equivalent.
%
%\begin{lemma}\label{lem:average_degree_hyperbolic}
%For any $0 < \varepsilon < 1$
%\[
%	\lim_{n \to \infty} \sup_{0 < y \le (1-\varepsilon)R} \left|\frac{\Mu{\BallHyp{y}}}{\Mu{\BallPo{y}}} - 1\right| = 0.
%\]
%\end{lemma}
%
%\begin{proof}
%We perform the computation of $\Mu{\BallHyp{y}}$ by splitting the integration with respect to the height $y^\prime$ into the cases $y^\prime > R - y$ and $y^\prime \le R - y$,
%\[
%	\Mu{\BallHyp{y}} 
%	= \Mu{\BallHyp{y} \cap \Rcal ([0,R - y))} + \Mu{\BallHyp{y} \cap \Rcal ([R - y, R])}.
%\]
%
%For the first part we have that
%\[
%	\Mu{\BallHyp{(0,y)} \cap\Rcal[(0,R-y)]} = \frac{2\nu \alpha}{\pi} \int_0^{R-y} \Phi(y,y^\prime) e^{-\alpha y^\prime}
%	\dd y^\prime.
%\]
%Hence, by applying Lemma~\ref{lem:hyperbolic_ball_lower_part} with $\delta = 0$ we conclude that
%\[
%	\lim_{n \to \infty} \sup_{0 < y \le (1-\varepsilon)R} \left|
%	\frac{\Mu{\BallHyp{(y)} \cap\Rcal[(0,R-y)]}}{\Mu{\BallPo{y}}} - 1\right| = 0.
%\]
%
%For the second part we observer that $\BallHyp{(y)} \cap \Rcal ([R - y, R])= \Rcal([R-y,R])$. 
%Thus, 
%\begin{align*}
%	&\hspace{-30pt}\Mu{\BallHyp{(y)} \cap \Rcal ([R - y, R])} \\
%	&= \int_{R - y}^{R} \int_{I_n} f_{\alpha,\nu}(x^\prime, y^\prime) \dd x^\prime \dd y^\prime
%		= \nu \alpha e^{R/2}\left(e^{-\alpha(R - y)} - e^{-\alpha R}\right)\\
%	&= \Mu{\BallPo{y}} \frac{2\alpha - 1}{4\pi} \left( e^{-(\alpha - \frac{1}{2})(R - y)}
%		- e^{-(\alpha - \frac{1}{2})R - y/2}\right), \numberthis \label{eq:mu_hyperbolic_ball_part_1}
%\end{align*}
%from which we conclude that
%\[
%	\lim_{n \to \infty} \sup_{0 < y \le (1-\varepsilon)R} \frac{\Mu{\BallHyp{(0,y)} \cap \Rcal ([R - y, R])}}{\Mu{\BallPo{y}}} = 0,
%\]
%which finishes the proof.
%\end{proof}

\section{Derivative of $\mu_{Po}(y)$}

Recall that $\mu_{Po}(y) = \Mu{\BallHyp{y}}$ denote the measure of the ball at height $y$ in the KPKVB model and $\mu(y) = \xi e^{\frac{y}{2}}$ denotes the measure of a ball at height $h$ in the infinite model $\Ginf$. In this section we will show that $\mu_{Po}^\prime(y) = (1+\smallO{1})\mu^\prime(y)$, uniformly on $[0,(1-\varepsilon)R]$, for some $0 < \varepsilon < 1$. This is a technical result that is needed in the proof of Lemma~\ref{lem:degree_integral} in Section~\ref{ssec:expected_degrees_GPo}.

First we note that it follows from Lemma~\ref{lem:average_degree_P_n} that $\mu_{Po}(y) = \mu(y)(1 + \phi_n(y))$, where $\phi_n(y) := \mu_{Po}(y)/\mu(y) - 1$. Taking the derivative we have
\[
	\mu_{Po}^\prime(y) = \mu^\prime(y)(1 + \phi_n(y)) + \mu(y)\phi_n^\prime(y)
	= \mu^\prime(y)(1 + \phi_n(y) + 2 \phi_n^\prime(y)),
\] 
where we used that $\frac{\partial}{\partial y} \mu(y) = \frac{1}{2}\mu(y)$. Hence, to show the result for we thus need to show that $\phi_n^\prime(y)) = \smallO{1}$, uniformly on $[0,(1-\varepsilon)R]$. 

Writing out the derivative we have
\[
	\phi_n^\prime(y) = 
	\mu_{Po}(y) ^{-1} \frac{\partial}{\partial y} \mu_{Po}(y) -  \frac{1}{2} \frac{\mu_{Po}(y) }{\mu(y)},
\]
where we used again that $\frac{\partial}{\partial y} \mu(y) = \frac{1}{2}\mu(y)$. For the second term Lemma~\ref{lem:average_degree_P_n} implies that $\frac{1}{2} \frac{\mu_{Po}(y) }{\mu(y)} = (1+\smallO{1})\frac{1}{2}$ uniformly on $[0,(1-\varepsilon)R]$. The following lemma shows that the same holds for the first term from which we conclude that $\phi_n^\prime(y)) = \smallO{1}$ and hence $\mu_{Po}^\prime(y) = (1+\smallO{1})\mu^\prime(y)$, uniformly on $[0,(1-\varepsilon)R]$.

\begin{lemma}\label{lem:derivative_mu_Po}
For any $0 < \varepsilon < 1$,
\[
	\lim_{n \to \infty} \sup_{0 \le y \le (1-\varepsilon)R} \left|\mu(y)^{-1}
	\frac{\partial}{\partial y} \mu_{Po}(y) - \frac{1}{2}\right| = 0.
\]
\end{lemma}

\begin{proof}
We again split $\mu_{\Po}(y)$ over the top and bottom part,
\[
	\mu_{\Po}(y) 
	= \Mu{\BallHyp{y} \cap \Rcal ([0,R - y))} + \Mu{\BallHyp{y} \cap \Rcal ([R - y, R])},
\]
where
\[
	\Mu{\BallHyp{y} \cap \Rcal ([0,R - y))} = \frac{2\alpha \nu}{\pi}\int_0^{R - y} \Phi(y,y^\prime) 
		e^{-\alpha y^\prime} \dd y^\prime,
\]
with $\Phi(y,y^\prime)$ defined as in~\eqref{eq:def_Omega_hyperbolic}. For the second term we have
\[
	\Mu{\BallHyp{y} \cap \Rcal ([R - y, R])}
	= \int_{R-y}^R \int_{I_n} f(x^\prime,y^\prime) \dd x^\prime \dd y^\prime
	= \xi e^{y/2}\frac{2\alpha - 1}{4\pi} \left( e^{-(\alpha - \frac{1}{2})(R - y)}
	- e^{-(\alpha - \frac{1}{2})R - y/2}\right).
\]
Taking the derivative of the last expression gives
\begin{align*}
	&\frac{\partial}{\partial y} \Mu{\BallHyp{y} \cap \Rcal ([R - y, R])}\\
	&= \frac{1}{2}\Mu{\BallHyp{y} \cap \Rcal ([R - y, R])}
		+ \xi e^{y/2}\frac{2\alpha - 1}{4\pi}\left(
		\left(\alpha - \frac{1}{2}\right)e^{-(\alpha - \frac{1}{2})(R - y)} 
		+ \frac{1}{2}e^{-(\alpha - \frac{1}{2})R - y/2}\right)\\
	&= \frac{1}{2}\Mu{\BallHyp{y} \cap \Rcal ([R - y, R])}\left(1 +
		\frac{(2\alpha - 1)e^{-(\alpha - \frac{1}{2})(R - y)} + e^{-(\alpha - \frac{1}{2})R - y/2}}
		{e^{-(\alpha - \frac{1}{2})(R - y)} - e^{-(\alpha - \frac{1}{2})R - y/2}}\right).
\end{align*}
Since, $\lim_{n \to \infty} \sup_{0 < y \le (1-\varepsilon)R} \Mu{\BallPo{y}}^{-1} \Mu{\BallHyp{y} \cap \Rcal ([R - y, R])} = 0$, we are left to show that
\begin{equation}\label{eq:derivative_mu_hyp_ball_main}
	\lim_{n \to \infty} \sup_{0 < y \le (1-\varepsilon)R} \left|\Mu{\BallPo{y}}^{-1} \frac{2\alpha \nu}{\pi} \frac{\partial}{\partial y} \int_0^{R - y} \Phi(y,y^\prime) e^{-\alpha y^\prime} \dd y^\prime
	- \frac{1}{2}\right| = 0.
\end{equation}

We start with some preliminary computations. For convenience we define
\[
	\Xi(y,y^\prime) = 1 - \frac{\cosh(R- y)\cosh(R-y^\prime) - \cosh(R)}{\sinh(R - y) \sinh(R - y^\prime)},
\]
so that
\[
	\Phi(y, y^\prime) = \frac{1}{2}e^{R/2} \arccos\left(1 - \Xi(y,y^\prime)\right).
\]
Next, following the same calculation as in the proof of~\cite[Lemma 28]{fountoulakis2018law}, we write
\begin{align*}
	\Xi(y,y^\prime)
	&= 2 e^{-(R - y - y^\prime)} \frac{\left(1 - e^{y^\prime - y - R}\right)\left(1 - e^{y - y^\prime - R}\right)}
		{\left(1 - e^{-2(R - y^\prime)}\right)\left(1 - e^{-2(R- y)}\right)}\\
	&:= 2 e^{-(R - y - y^\prime)} \frac{h_1(y) h_2(y)}{h_3(y^\prime) h_3(y)},
\end{align*}
with
\[
	h_1(y) = 1 - e^{y^\prime - y - R}, \quad h_2(y) = 1 - e^{y - y^\prime - R}
	\quad \text{and} \quad h_3(y) = 1 - e^{-2(R- y)}.
\]
We suppressed the dependence on $n$ and, in some cases, on $y^\prime$ for notation convenience.

We make two important observations. First, $\Xi(y,y^\prime)$ is an increasing function in both arguments, for $y, y^\prime < R$ and $y + y^\prime < R$. Second, for all $y + y^\prime < R$, $h_1(y) \le h_3(y^\prime)$ and $h_2(y) \le h_3(y)$, while $h_3(y), h_3(y^\prime) < 1$, so that
\begin{equation}\label{eq:derivative_hyp_ball_Xi_bounds}
	2 e^{-(R - y - y^\prime)}h_1(y) h_2(y) \le \Xi(y,y^\prime) \le 2 e^{-(R - y - y^\prime)}.
\end{equation}
In particular, since $R-y$ is an increasing function of $n$ uniformly on $0 < y < (1-\varepsilon)R$, there exists a $0 < \delta < 1$ such that $1/2 \le \Xi(y,y^\prime) < 2$ for all $y + y^\prime < R$ and $(1-\delta)(R-y) < y^\prime < R$ and $n$ large enough.

Next, taking the derivative of $\Xi(y,y^\prime)$ yields,
\begin{align*}
	\frac{\partial}{\partial y} \Xi(y,y^\prime) &= \Xi(y,y^\prime) + 2 e^{-(R - y - y^\prime)}
		\left(\frac{h_1^\prime(y) h_2(y)}{h_3(y^\prime) h_3(y)} + \frac{h_1(y)h_2^\prime(y)}{h_3(y^\prime) h_3(y)}
		- \frac{h_1(y) h_2(y) h_3^\prime(y)}{h_3(y^\prime) h_3(y)^2}\right)\\
	&= \Xi(y,y^\prime)\left(1 + \frac{h_1^\prime(y)}{h_1(y)} + \frac{h_2^\prime(y)}{h_2(y)} 
		- \frac{h_3^\prime(y)}{h_3(y)}\right)\\
	&:= \Xi(y,y^\prime)\left(1 + \varphi_n(y,y^\prime)\right),
\end{align*}
with
\[
	\varphi_n(y,y^\prime) = \frac{e^{y^\prime - y- R}}{1 - e^{y^\prime - y - R}} 
	- \frac{e^{y - y^\prime - R}}{1 - e^{y - y^\prime - R}} - \frac{2e^{-2(R - y)}}{1 - e^{-2(R-y)}}. 
\]
Therefore, by the chain rule,
\begin{align*}
	\frac{\partial}{\partial y} \Phi(y, y^\prime)
	&= \frac{1}{2}e^{R/2} \frac{1}{\sqrt{1 - \left(1 - \Xi(y,y^\prime)\right)^2}} 
		\frac{\partial}{\partial y} \Xi(y,y^\prime)\\
	&=  \frac{ \frac{1}{2}e^{R/2} \Xi(y,y^\prime)}{\sqrt{1 - \left(1 - \Xi(y,y^\prime)\right)^2}}\left(1 + 
		\varphi_n(y,y^\prime)\right). \numberthis \label{eq:derivative_Phi}
\end{align*}
Applying the Leibniz's rule we then get
\begin{align*}
	&\hspace{-30pt}\frac{\partial}{\partial y} \int_0^{R - y} \Phi(y,y^\prime) e^{-\alpha y^\prime} \dd y^\prime\\
	&= - \Phi(y,R - y)e^{-\alpha(R-y)} + \int_0^{R - y} \frac{\partial}{\partial y}  \Phi(y,y^\prime) 
		 e^{-\alpha y^\prime} \dd y^\prime\\
	&= -\frac{1}{2}e^{-(\alpha-\frac{1}{2})R + \alpha y} + \int_0^{R- y} \frac{ \frac{1}{2}e^{R/2} \Xi(y,y^\prime)}{\sqrt{1 - \left(1 - \Xi(y,y^\prime)\right)^2}}
		\left(1 + \varphi_n(y,y^\prime)\right) e^{-\alpha y^\prime} \dd y^\prime\\
	&= -\frac{1}{2}e^{-(\alpha-\frac{1}{2})R + \alpha y}  + \int_0^{(1-\delta)(R- y)} \frac{ \frac{1}{2}e^{R/2} \Xi(y,y^\prime)}
		{\sqrt{1 - \left(1 - \Xi(y,y^\prime)\right)^2}}
		\left(1 + \varphi_n(y,y^\prime)\right) e^{-\alpha y^\prime} \dd y^\prime\\
	&\hspace{10pt}+ \int_{(1-\delta)(R- y)}^{R - y} \frac{ \frac{1}{2}e^{R/2} \Xi(y,y^\prime)}
			{\sqrt{1 - \left(1 - \Xi(y,y^\prime)\right)^2}}
			\left(1 + \varphi_n(y,y^\prime)\right) e^{-\alpha y^\prime} \dd y^\prime\\
	&:= -I_1(y) + I_2(y) + I_3(y),
\end{align*}
with $0 \le \delta < 1$ such that $0 < \Xi(y,y^\prime) < 2$ for all $0 < y < R$ and $(1-\delta)(R-y) < y^\prime < R$.

We proceed by showing that
\begin{equation}\label{eq:derivative_hyp_ball_error_1}
	\lim_{n \to \infty} \sup_{0 < y \le (1-\varepsilon)R} \left|\frac{I_t(y)}{\Mu{\BallPo{y}}}\right| 
	= 0, \quad \text{for } t = 1,3
\end{equation}
while
\begin{equation}\label{eq:derivative_hyp_ball_main_part}
	\lim_{n \to \infty} \sup_{0 \le y \le (1-\varepsilon)R} \left|\frac{2\nu \alpha}{\pi \Mu{\BallPo{y}}} I_2(y) - \frac{1}{2}\right| = 0.
\end{equation}
This then implies~\eqref{eq:derivative_mu_hyp_ball_main} and finishes the proof.

For $I_1(y)$ we have 
\[
	\lim_{n \to \infty} \sup_{0 < y \le (1-\varepsilon)R} \Mu{\BallPo{y}}^{-1} I_1(y) 
	\le \lim_{n \to \infty} \sup_{0 < y \le (1-\varepsilon)R} \frac{1}{2\xi} e^{-(\alpha - \frac{1}{2})(R - y)} = 0.
\]

For $I_3(y)$ we first use that $y^\prime < R - y$ to bound $\varphi(y,y^\prime)$ as follows,
\[
	\varphi_n(y,y^\prime) \le \frac{e^{y^\prime - y - R}}{1 - e^{y^\prime - y - R}} \le \frac{e^{-2y}}{1 - e^{-2y}}.
\]
This then yields that
\[
	I_3(y) \le \frac{1}{2}\left(1 + \frac{e^{-2y}}{1 - e^{-2y}}\right)e^{R/2}
	\int_{(1-\delta)(R- y)}^{R - y} \frac{ \Xi(y,y^\prime)}{\sqrt{1 - \left(1 - \Xi(y,y^\prime)\right)^2}}
	e^{-\alpha y^\prime} \dd y^\prime.
\]
To bound the integral we recall that $0 < \Xi(y,y^\prime) \le 2e^{-(R-y-y^\prime)} < 2$ and for all $1/2 \le x < 2$,
\[
	\frac{1}{\sqrt{1- (1-x)^2}} \le \frac{2}{\sqrt{2-x}},
\]a
where the right hand side is a monotonic increasing function.
Therefore
\begin{align*}
	\int_{(1-\delta)(R- y)}^{R - y} \frac{ \Xi(y,y^\prime)}{\sqrt{1 - \left(1 - \Xi(y,y^\prime)\right)^2}}
		e^{-\alpha y^\prime} \dd y^\prime
	&\le 2 \int_{(1-\delta)(R- y)}^{R - y} \frac{\Xi(y,y^\prime)}{\sqrt{(2-\Xi(y,y^\prime))}} e^{-\alpha y^\prime} 
		\dd y^\prime \\
	&\le \sqrt{2} e^{-\alpha(R - y)} \int_{(1-\delta)(R- y)}^{R - y} 
		\frac{e^{-(R - y - y^\prime)}}{\sqrt{1 - e^{-(R - y - y^\prime)}}} e^{\alpha (R - y - y^\prime)} \dd y^\prime,
\end{align*}
Making the change of variables $z = e^{-(R - y - y^\prime)}$ ($\dd y^\prime = z^{-1} \dd z$) we get that
\begin{align*}
	&\hspace{-20pt}\sqrt{2} e^{-\alpha(R - y)} \int_{(1-\delta)(R- y)}^{R - y} 
			\frac{e^{-(R - y - y^\prime)}}{\sqrt{1 - e^{-(R - y - y^\prime)}}} e^{\alpha (R - y - y^\prime)} \dd y^\prime\\
	&= \sqrt{2} e^{-\alpha(R - y)} \int_{e^{-\delta (R - y)}}^{1} \frac{z^{-\alpha}}{\sqrt{1 - z}} \dd z
		\le \sqrt{2} e^{-\alpha(R - y)} \sqrt{1 - e^{-\delta (R - y)}}
		\le \sqrt{2} e^{-\alpha (R -y)}.
\end{align*}
We therefore conclude that
\[
	I_3(y) \le \frac{1}{\sqrt{2}} \left(1 + \frac{e^{-2y}}{1 - e^{-2y}}\right)e^{-(\alpha -\frac{1}{2})R + \alpha y}.
\]
which implies~\eqref{eq:derivative_hyp_ball_error_1} for $t=3$.

Finally, to show~\eqref{eq:derivative_hyp_ball_main_part} we first write
\begin{align*}
	\left|\frac{2\alpha \nu}{\pi \Mu{\BallPo{y}}} I_2(y) - \frac{1}{2}\right|
	&\le \left|\frac{2\alpha \nu}{\pi \Mu{\BallPo{y}}}  \int_0^{(1-\delta)(R- y)} \frac{\Phi(y,y^\prime)}{2} 
		e^{-\alpha y^\prime} \dd y^\prime - \frac{1}{2}\right| \\
	&\hspace{10pt}+ \frac{2\alpha \nu}{\pi \Mu{\BallPo{y}}}\left|\int_0^{(1-\delta)(R- y)} 
		\frac{\Phi(y,y^\prime)}{2} e^{-\alpha y^\prime} \dd y^\prime - I_2(y)\right|.
\end{align*}

Note that by Lemma~\ref{lem:average_degree_P_n}
\[
	\frac{2\nu \alpha}{\pi} \int_{0}^{(1-\delta)(R-y)} \Phi(y,y^\prime) e^{-\alpha y^\prime} \dd y^\prime
	= (1+\smallO{1})\Mu{\BallPo{y}},
\]
uniformly for all $0 \leq y \leq (1-\varepsilon)R$. Therefore
\[
	\lim_{n \to \infty} \sup_{0 < y \le (1-\varepsilon)R} \left|\frac{2\nu \alpha}{\pi \Mu{\BallPo{y}}} 
	\int_{0}^{(1-\delta)(R-y)} \frac{\Phi(y,y^\prime)}{2} e^{-\alpha y^\prime} \dd y^\prime
	- \frac{1}{2}\right| = 0,
\] 
and thus it suffices to show that the $\lim_{n \to \infty} \sup_{0 < y \le (1-\varepsilon)R}$ of the second term goes to zero.

Recalling the definition of $I_2(y)$ we have
\begin{align*}
	&\hspace{-30pt}\left|\int_0^{(1-\delta)(R- y)} \frac{\Phi(y,y^\prime)}{2} e^{-\alpha y^\prime} \dd y^\prime 
		- I_2(y)\right|\\
	&\le \int_0^{(1-\delta)(R- y)} \left|\frac{\Phi(y,y^\prime)}{2}- \frac{\frac{1}{2}e^{R/2} \Xi(y,y^\prime)}
		{\sqrt{1 - \left(1 - \Xi(y,y^\prime)\right)^2}}(1+\varphi_n(y,y^\prime)\right|e^{-\alpha y^\prime} \dd y^\prime.
		\numberthis \label{eq:derivative_hyp_ball_final_error}
\end{align*}
We will proceed to bound the term inside the integral. For this we first note that for $0 \le y^\prime \le (1-\delta)(R-y)$,
\[
	\varphi_n(y,y^\prime) \le \frac{e^{-\delta(R-y)}}{1 - e^{-\delta(R-y)}}.
\]
and recall that $\Xi(y,y^\prime) \le 2 e^{-(R-y-y^\prime)}$. Moreover, since $x/\sqrt{1-(1-x)^2} = x/\sqrt{2x-x^2}$ is an increasing function and $e^{-(R - y - y^\prime)} \le e^{-\delta(R- y)}$ for $0 < y^\prime < (1-\delta)(R-y)$,
\[
	\frac{\frac{1}{2}e^{R/2} \Xi(y,y^\prime)}{\sqrt{1 - \left(1 - \Xi(y,y^\prime)\right)^2}}
	\le e^{R/2} \frac{e^{-\delta(R-y)}}{\sqrt{2e^{-\delta(R-y)} - e^{-2\delta(R-y)}}}.
\]
Next, recall that $\Phi(y,y^\prime) = \frac{1}{2}e^{R/2}\arccos(1-\Xi(y,y^\prime))$. Then, since $\Xi(y,y^\prime) < 1$ for all $y^\prime < (1-\delta)(R-y)$, $y < R$ and $n$ large enough, we have (see Lemma~\ref{lem:arccos_approx}),
\[
	\left|\frac{1}{2}\Phi(y,y^\prime) - \frac{\frac{1}{2}e^{R/2} \Xi(y,y^\prime)}{\sqrt{1 - \left(1 - \Xi(y,y^\prime)\right)^2}}\right| \le \frac{1}{2}\Phi(y,y^\prime) \Xi(y,y^\prime).
\]
for all $y^\prime < (1-\delta)(R - y)$ and $y < R$. Together these facts imply that for $n$ large enough
\begin{align*}
	&\hspace{-30pt}\left|\frac{\Phi(y,y)}{2} - \frac{\frac{1}{2}e^{R/2} \Xi(y,y^\prime)}{\sqrt{1 - \left(1 - 	
		\Xi(y,y^\prime)\right)^2}}(1+\varphi_n(y,y^\prime))\right|\\
	&\le \frac{\Phi(y,y^\prime)\Xi(y,y^\prime)}{2} 
		+ \frac{\frac{1}{2}e^{R/2} \Xi(y,y^\prime) \varphi_n(y,y^\prime)}
		{\sqrt{1 - \left(1 - \Xi(y,y^\prime)\right)^2}} \\
	&\le e^{-\delta(R-y)} \Phi(y,y^\prime) + \frac{e^{-\delta(R-y)}}{1-e^{-\delta(R-y)}}
		\frac{e^{R/2} e^{-\delta(R-y)}}{\sqrt{2e^{-\delta(R-y)} - e^{-2\delta(R-y)}}}\\
	&\le e^{-\delta(R-y)} \Phi(y,y^\prime) + \frac{e^{\frac{R}{2}}e^{-\frac{3}{2}\delta(R-y)}}
		{\left(1 - e^{-\delta(R-y)}\right)^{3/2}}
\end{align*}

Plugging this into~\eqref{eq:derivative_hyp_ball_final_error} yields
\begin{align*}
	&\hspace{-30pt}\left|\int_0^{(1-\delta)(R- y)} \frac{\Phi(y,y^\prime)}{2} e^{-\alpha y^\prime} \dd y^\prime 
			- I_2(y)\right|\\
	&\le \int_0^{(1-\delta)(R- y)} \left(e^{-\delta(R-y)} \Phi(y,y^\prime) + 
		\frac{e^{\frac{R}{2}}e^{-\frac{3}{2}\delta(R-y)}}{\left(1 - e^{-\delta(R-y)}\right)^{3/2}}\right)
		e^{-\alpha y^\prime} \dd y^\prime \\
	&\le e^{-\delta(R-y)} \Mu{\BallPo{y}} + e^{\frac{y}{2}} \frac{e^{-(\alpha - \frac{1}{2} - (\alpha - \frac{3}{2})\delta)(R-y)}}
		{\alpha \left(1 - e^{-\delta(R-y)}\right)^{3/2}}
\end{align*}
To finish the argument we note that $R-y > 0$ for all $0 < y \le (1-\varepsilon)R$ and observe that $\delta < 1$ implies that $(\alpha - \frac{1}{2} - (\alpha - \frac{3}{2})\delta > 1$. Since $\Mu{\BallPo{y}} = \bigT{e^{\frac{y}{2}}}$ it the
then follows that
\[
	\lim_{n \to \infty} \sup_{0 < y \le (1-\varepsilon)R} \frac{2\alpha \nu}{\pi \Mu{\BallPo{y}}}
	\left|\int_0^{(1-\delta)(R- y)} \frac{\Phi(y,y^\prime)}{2} e^{-\alpha y^\prime} \dd y^\prime - I_2(y)\right| = 0,
\]
which completes the proof.
\end{proof}

\section{Code for the simulations}

The simulations of the clustering coefficient and function in the KPKVB model were done using Wolfram Mathematica 11.1.
The simulation dots for the clustering coefficient in Figure~\ref{fig:gamma} were generated by the following code (where in the second line, the entire script was also run for the values \verb|nu=1| and \verb|nu=0.5|):
\begin{lstlisting}[language=Mathematica,breaklines]
n=10000;
nu=2;
R=2*Log[n/nu];
plotpoints=20;
reps=100;
Plotingdataalpha = ConstantArray[0,{plotpoints,2}];
SeedRandom[1];
For[z=1,z<=plotpoints,z++,a=0.4+z (4.6/plotpoints); sum=0;
	For[r=1,r<=reps,r++,V = ConstantArray[0,{n,2}];
		For[i=1,i<=n,i++,
			V[[i,1]]=RandomReal[{-Pi ,Pi }];
			V[[i,2]]=ArcCosh[RandomReal[{0,1}](Cosh[a*R]-1)+1]/a];
		A= ConstantArray[0,{n,n}];
		For[i=1,i<=n,i++,
			For[j=1,j<=n,j++,
				If[Cosh[V[[i,2]]]Cosh[V[[j,2]]]-Sinh[V[[i,2]]]Sinh[V[[j,2]]]Cos[Abs[V[[i,1]]-V[[j,1]]]] <= Cosh[R] && i != j,A[[i,j]]=1,A[[i,j]]=0]]];
		g = AdjacencyGraph[A];
		sum=sum+MeanClusteringCoefficient[g]];
	Plotingdataalpha[[z,1]]=a;
	Plotingdataalpha[[z,2]]=1.0*sum/reps;]
Print[Plotingdataalpha]
\end{lstlisting}
The simulation dots for the clustering function in Figure~\ref{fig:gammak} were generated by the following code (where in the third line, the entire script was also run for the values \verb|nu=1| and \verb|nu=0.5|):
\begin{lstlisting}[language=Mathematica,breaklines]
n=10000;
a=0.8;
nu=2;
R=2*Log[n/nu];
plotpoints=24;
reps=100;
Plotingdatak = ConstantArray[0,{reps,plotpoints,2}];
SeedRandom[1];
For[r=1,r<=reps,r++,V = ConstantArray[0,{n,2}];
	For[i=1,i<=n,i++,
		V[[i,1]]=RandomReal[{-Pi ,Pi }];
		V[[i,2]]=ArcCosh[RandomReal[{0,1}](Cosh[a*R]-1)+1]/a];
	A= ConstantArray[0,{n,n}];
	For[i=1,i<=n,i++,
		For[j=1,j<=n,j++,
			If[Cosh[V[[i,2]]]Cosh[V[[j,2]]]-Sinh[V[[i,2]]]Sinh[V[[j,2]]]Cos[Abs[V[[i,1]]-V[[j,1]]]] <= Cosh[R] && i != j,A[[i,j]]=1,A[[i,j]]=0]]];
	g = AdjacencyGraph[A];
	For[k=1,k<=plotpoints,k++,
		sum=0;
		result=0;
		nrdegk=0;
		For[v =1,v<=n,v++; 
			If[VertexDegree[g,v]==k+1,
				result=result+LocalClusteringCoefficient[g,v];nrdegk++]];
		Plotingdatak[[r,k,1]]=k+1;
		If[nrdegk>0,Plotingdatak[[r,k,2]]=1.0*result/nrdegk]];]
Print[Mean[Plotingdatak]];
\end{lstlisting}

\end{appendices}