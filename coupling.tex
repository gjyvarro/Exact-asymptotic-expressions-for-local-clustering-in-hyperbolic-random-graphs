\section{Equivalence for local clustering in hyperbolic and Poissonized random graph}\label{sec:coupling_H_P_n}

In this section we establish the equivalence between $c_{\H,n}^\ast(k)$ and $c_{\Pcal,n}^\ast(k)$ as expressed in Proposition~\ref{prop:couling_c_H_P}, using the coupling procedure explained in Section~\ref{ssec:coupling_H_P}. 

Recall that $\mathcal{P}_{\alpha,\nu}$ denotes a Poisson process on $\mathbb{R} \times \mathbb{R}_+$, with intensity $f_{\alpha,\nu}(x,y)$, $I_n = \left(-\frac{\pi}{2}e^{R_n/2}, \frac{\pi}{2}e^{R_n/2}\right)$, $\mathcal{R}_n = I_n \times (0,R_n]$ and $\mathcal{V}_n = \mathcal{P}_{\alpha, \nu}\cap \mathcal{R}_n$. In addition we define for any interval $I \subseteq \mathbb{R}_+$, $\Rcal_n(I) := I_n \times I$ and denote by $\BallPon{p}$ the \emph{ball}
\[
	\BallPon{p} = \left\{p^\prime \in \mathcal{V}_n : |x - x^\prime |_{\pi e^{R_n/2}} < e^{\frac{y+y^\prime}{2}}\right\}.
\]
Note that when $p \in \mathcal{V}_n$ then $\BallPon{p}$ denotes its neighborhood in the graph $G_{\mathcal{P},n}$. 
Note that the above definition implies that for all $y\in [0,R_n]$ we have 
\begin{equation} \label{eq:upper_ballPo_inclusion}
\Rcal_n([R_n-y - 2\ln (\pi/2), R_n]) \subseteq \BallPon{(0,y)}
\end{equation}
- this is a fact which we are going to use several times in our analysis. \PvdH{@All: Maybe we should either add figure to illustrate this or we could refer to a previous figure.}

For any Borel-measurable subset $S \subseteq \mathbb{R} \times \mathbb{R}_+$, we let 
\[
	\mu_{\alpha, \nu} (S) = \int_S f_{\alpha, \nu}(x,y) \, dx \, dy = \frac{\nu \alpha}{\pi}\int_S e^{-\alpha y}dy.
\]
Thus, the number of points of $\Pcal_{\alpha, \nu}$ inside $S$ is distributed as $\Po ({\mu_{\alpha, \nu,} (S)})$.

Finally, we recall the map $\Psi$ from~\eqref{eq:def_Psi}
\[
	\Psi(r,\theta) = \left(\theta \frac{e^{R_n/2}}{2}, R_n - r\right),
\] 
and remind the reader that $\BallHyp{p}$ denotes the image under $\Psi$ of the ball of hyperbolic radius $R_n$ around the point $\Psi^{-1}(p)$ and that under the coupling between the hyperbolic random graph and the finite box model, described in Section~\ref{ssec:coupling_H_P}, two points $p$ and $p^\prime$ are connected if and only if
\[
	|x-x^\prime|_{\pi e^{r_n/2}} \le \Omega(R_n - y, R_n - y^\prime),
\]
where the function $\Omega$ can be approximated, for $y + y^\prime < R_n$, using Lemma~\ref{lem:asymptotics_Omega_hyperbolic} by  
\[
	e^{\frac{1}{2}(y+y^\prime)} - K e^{\frac{3}{2}(y+y^\prime) - R_n} \leq \Omega(R_n - y, R_n - y^\prime) 
		\leq  e^{\frac{1}{2}(y+y^\prime)} + K e^{\frac{3}{2}(y+y^\prime) - R_n}.
\]

%To prove Proposition~\ref{prop:couling_c_H_P} we calculate the error in two steps. First we show in Section~\ref{ssec:coupling_HP_ast_P} that
%\[
%	\lim_{n \to \infty} s_\alpha(k_n) \Exp{\left|c_{\HP,n}^\ast(k_n) - c_{\Pcal,n}^\ast(k_n)\right|} = 0,
%\]
%Then, in Section~\ref{ssec:coupling_H_HP}, we prove Proposition~\ref{prop:clustering_ast_H_Pois}, i.e.
%\[
%	\lim_{n \to \infty} s_\alpha(k_n) \Exp{\left| c_{\H,n}^\ast(k_n) - c_{\HP,n}^\ast(k_n)\right|} = 0.
%\]
%Together these results yield Proposition~\ref{prop:couling_c_H_P}.

\subsection{Some results on the hyperbolic geometric graph}

We start with some basic results for the hyperbolic random geometric graph. Observe that~\eqref{eq:asymp2} from Lemma~\ref{lem:asymptotics_Omega_hyperbolic} implies the following. 
\begin{corollary}\label{cor:balls_inclusion}
\begin{equation*}
 \BallPo{p} \cap \Rcal_n ([K,R_n]) \subseteq \BallHyp{p} \cap \Rcal_n (K,R_n). 
\end{equation*}
\end{corollary}


Furthermore, Lemma~\ref{lem:asymptotics_Omega_hyperbolic} enables us to determine the measure of a ball around a given point $p=(0,y)$ - this is will be fairly useful in our subsequent analysis. 

Let $p \in \Rcal_n$. Then we can see that the curve $x^\prime = e^{\frac{1}{2} (y + y^\prime)}$ with $x^\prime \geq 0$ meets the right boundary of $\Rcal_N$, that is, the line $x^\prime = \frac{\pi}{2} e^{R_n/2}$ at $y^\prime = R_n - y + 2\ln \frac{\pi}{2}$. Hence, any point $p^{\prime} \in \Rcal_n ([R_n - y + 2\ln \frac{\pi}{2}, R_n])$ is included in $\BallPo{p}$. In other words,
\begin{equation*} \label{eq:P_ball_inclusion_lower}
\BallPo{p} \cap \Rcal_n ([R_n - y +2\ln \frac{\pi}{2},R_n]) = \Rcal_n ([R_n - y + 2\ln \frac{\pi}{2},R_n]).
\end{equation*}
This together with \eqref{eq:tail_inclusion_hyperbolic_ball} implies that 
\begin{equation}\label{eq:symm_diff_upper_P} 
(\BallHyp{p} \bigtriangleup \BallPo{p})  \cap \Rcal ([R_n - y + 2 \ln \frac{\pi}{2},R_n]) = \emptyset,
\end{equation}
where $A \bigtriangleup B$ denotes the symmetric difference. We can now compute the expected number of points in $\BallHyp{p} \bigtriangleup \BallPo{p}$, i.e. those that belong are a neighbor of $p$ in only one of the two models.

\begin{lemma}\label{lem:sym_diff_measure_H_P}
Let $0 \le y_n <R_n$ be such that $R_n - y_n \to \infty$ and write $p_n = (x_n, y_n)$. Then we have, as $n \to \infty$,
\[
	\mu_{\alpha,\nu} (\BallHyp{p_n} \bigtriangleup \BallPo{p_n} ) 
	= \Theta (1) \cdot \begin{cases} 
		e^{(1/2-\alpha)R_n + \alpha y_n}, & \mbox{if } \alpha < 3/2 \\
		(R_n-y_n) e^{3y/2 - R_n}, & \mbox{if }\alpha = 3/2\\
		e^{3y_n/2 - R_n}, &  \mbox{if } \alpha > 3/2 
	\end{cases}.
\]
\end{lemma}

\begin{proof}
Let $r_n := R_n - y$. Lemma~\ref{lem:asymptotics_Omega_hyperbolic} implies that for such a $p_n$, if a point $p$ belongs to $\BallHyp{p_n} \bigtriangleup \BallPo{p_n} \cap \Rcal ([0,r_n])$ then 
\[
	|x_n - x| = \Theta(1) \cdot e^{\frac{3}{2} (y_n + y) - R_n}.
\]
Now, if $p \in [r_n, r_n + 2 \ln \frac{\pi}{2})]$ and also $p \in \BallHyp{p_n} \bigtriangleup \BallPo{p_n}$, then 
\[
	|x_n-x| =\frac{\pi}{2} e^{R_n/2} - e^{\frac{1}{2} (y_n + y)}.
\]
Finally,~\eqref{eq:symm_diff_upper_P} implies that no point in $\Rcal ([r_n+2\ln \frac{\pi}{2},R_n])$ belongs to $\BallHyp{p_n} \bigtriangleup \BallPo{p_n}$. We first compute the expected number of points in $p \in \BallHyp{p_n} \bigtriangleup \BallPo{p_n}$ that have $R_n - y \le r_n$. The result depends on the value of $\alpha$, yielding the following three cases
\begin{align*}
	\mu_{\alpha,\nu} (\BallHyp{p_n} \bigtriangleup \BallPo{p_n} \cap \Rcal ([0,r_n])) 
	&= \Theta(1) \cdot e^{3y_n/2 - R_n}\int_0^{r_n} e^{(3/2-\alpha) y} \, dy \\
	&= \Theta(1)\cdot \begin{cases} e^{(1/2-\alpha)R_n + \alpha y_n}, & \mbox{if } \alpha < 3/2 \\
		(R_n - y_n) e^{3y_n/2 - R_n}, & \mbox{if }\alpha = 3/2\\
		e^{3y_n/2 - R_n}, &  \mbox{if } \alpha > 3/2
	\end{cases}.
\end{align*}
Next we compute the number of remaining points in $\BallHyp{p_n} \bigtriangleup \BallPo{p_n}$, 
\begin{align*}
	\mu_{\alpha,\nu} (\BallHyp{p_n} \bigtriangleup \BallPo{p_n} \cap \Rcal ([r_n, R_n])) 
	&= \frac{\nu\alpha}{\pi} \int_{r_n}^{r_n + 2 \ln \frac{\pi}{2}} 
		\left(\frac{\pi}{2} e^{R_n/2} - e^{\frac{1}{2} (y_n + y)}\right)e^{-\alpha y} \, dy \\
	&= O(1) \cdot e^{R_n/2} \int_{r_n}^{r_n + 2 \ln \frac{\pi}{2}} e^{-\alpha y} \, dy 
		= O(1) \cdot e^{R_n/2} e^{-\alpha r_n} \\
	&=O(1) \cdot e^{(1/2 - \alpha) R_n + \alpha y_n}.
\end{align*}
Now note that for any $\alpha > 3/2$, we have 
\[
	\left( (1/2 - \alpha) R_n + \alpha y_n\right) - \left(3y_n/2 - R_n \right)
	= (3/2 -\alpha) (R_n- y_n) \to -\infty,
\]
by our assumption on $y_n$. For $\alpha = 3/2$, these two quantities are equal. From these observations, we deduce that 
\[
	\mu_{\alpha,\nu} (\BallHyp{p_n} \bigtriangleup \BallPo{p_n} ) 
	= \Theta (1) \cdot \begin{cases} 
		e^{(1/2-\alpha)R_n + \alpha y_n}, & \mbox{if } \alpha < 3/2 \\
		r_n e^{3y_n/2 - R_n}, & \mbox{if }\alpha = 3/2\\
		e^{3y_n/2 - R_n}, &  \mbox{if } \alpha > 3/2 
	\end{cases}.
\]
\end{proof}

%\subsection{Figures for proof strategy}
%
%\PvdH{I will create some figures illustrating the proof strategy for the coupling. These will be merged with the computations as soon as Nikolaos finishes the computations.}
%
%\begin{figure}
%
%%\begin{tikzpicture}
%%
%%	\pgfmathsetmacro{\u}{0}
%%	\pgfmathsetmacro{\v}{0.5}
%%	\pgfmathsetmacro{\uu}{1}
%%	\pgfmathsetmacro{\vv}{1.2}
%%	\pgfmathsetmacro{\uuu}{-3}
%%	\pgfmathsetmacro{\vvv}{2}
%%	\pgfmathsetmacro{\e}{0.05}
%%	\pgfmathsetmacro{\K}{1}
%%	\pgfmathsetmacro{\R}{3}
%%	\pgfmathsetmacro{\r}{(pi/2)*exp(\R/2)}
%%
%%	\draw[line width=1pt,dashed] (-\r,0) -- (\r,0) -- (\r,\R) -- (-\r,\R) -- (-\r,0);
%%
%%	\draw[domain=0:{\R-\v},variable=\x, samples=200, black, line width=1pt] plot ({(exp(\R/2)/2)*rad(acos((cosh(\R-\v)*cosh(\R-\x)-cosh(\R))/(sinh(\R-\v)*sinh(\R-\x))))},{\x});
%%
%%\end{tikzpicture}
%
%\begin{tikzpicture}[scale=0.85]
%	%Define the coordinates 
%	%p = (\u,\v), p_1 = (\uu, \vv)
%	%Box \Rcal_n has width 2\r and height \t (\r = \pi/2 e^{R_n/2})
%	\pgfmathsetmacro{\u}{0}
%	\pgfmathsetmacro{\v}{0.5}
%	\pgfmathsetmacro{\uu}{1}
%	\pgfmathsetmacro{\vv}{1.2}
%	\pgfmathsetmacro{\uuu}{5.5}
%	\pgfmathsetmacro{\vvv}{1.7}
%	\pgfmathsetmacro{\epsilon}{0.05}
%	\pgfmathsetmacro{\K}{1}
%	\pgfmathsetmacro{\R}{3}
%	\pgfmathsetmacro{\r}{(pi/2)*exp(\R/2)}
%	\pgfmathsetmacro{\t}{\R}
%	
%	\pgfmathsetmacro{\leftintvandvvv}{-exp((\v + \vvv)/2)}
%	\pgfmathsetmacro{\rightintvandvvv}{exp((\v + \vvv)/2)}
%	\pgfmathsetmacro{\leftintvvandvvv}{\uu-exp((\vv + \vvv)/2)}
%	\pgfmathsetmacro{\rightintvvandvvv}{\uu+exp((\vv + \vvv)/2)}
%		
%	%The box \Rcal_n
%	\draw[line width=1pt,dashed] (0,0) -- (\r,0) -- (\r,\t) -- (0,\t);
%
%	%Dram all three nodes
%    \draw node[fill, circle, inner sep=0pt, minimum size=5pt] (p0) at (\u,\v) {};
%    \path (p0)+(-0.3,0.3) node {$p_0$};
%    
%    \draw node[fill,blue, circle, inner sep=0pt, minimum size=5pt] (p1) at (\uu,\vv) {};
%    \path (p1)+(-0.3,0.3) node {\color{blue}$p_1$};	
%
%	
%	%Boundaries p_0 = (\u,\v)
%	
%	%Limit model
%	%Right boundary
%	\pgfmathsetmacro{\rightbounduv}{\u+exp((\v)/2)}
%	\draw[domain=0:\R,smooth,variable=\y,black, dotted, line width=1pt] plot ({exp((\v+\y)/2)},{\y});
%    
%    %Hyperbolic model
%    %Right boundary
%	\draw[domain=0:{\R-\v},variable=\y, samples=200, black, line width=1pt] plot 		
%		({(exp(\R/2)/2)*rad(acos((cosh(\R-\v)*cosh(\R-\y)-cosh(\R))/(sinh(\R-\v)*sinh(\R-\y))))},{\y});
%
%    
%	%Boundaries p_1 = (\uu,\vv)
%	
%	%Limit model
%	\pgfmathsetmacro{\rboundvv}{2*ln(\r-\uu)-\vv}
%	\pgfmathsetmacro{\lboundvv}{2*ln(\r+\uu)-\vv}
%
%    
%    %Hyperbolic model
%	\pgfmathsetmacro{\rhboundvv}{1.76}
%	\pgfmathsetmacro{\lhboundvv}{\R-\vv}
%
%	%Intersection
%	\pgfmathsetmacro{\regionright}{\uu+exp((\vv+\rboundvv)/2)}
%
%	\draw[red,line width=1pt,pattern=north west lines, pattern color=red] 
%		plot[domain=0.6:\rboundvv,smooth,variable=\y,red] ({\uu+exp((\vv+\y)/2)},{\y}) 
%		--
%		(\regionright,\rboundvv) 
%		--
%		(\regionright,\rhboundvv)
%		--
%		plot[domain={\rhboundvv-0.01}:0.6,smooth,variable=\y,red] ({\uu+(exp(\R/2)/2)*rad(acos((cosh(\R-\vv)*cosh(\R-\y)-cosh(\R))/(sinh(\R-\vv)*sinh(\R-\y))))},{\y});
%		
%	\draw[red,line width=1pt,pattern=north west lines, pattern color=red] 
%		plot[domain=0:0.6,smooth,variable=\y,red] ({\uu+exp((\vv+\y)/2)},{\y}) 
%		--
%		plot[domain=0.6:0,smooth,variable=\y,red] ({\uu+(exp(\R/2)/2)*rad(acos((cosh(\R-\vv)*cosh(\R-\y)-cosh(\R))/(sinh(\R-\vv)*sinh(\R-\y))))},{\y});
%
%	%Limit model
%	%Right boundary
%	\draw[domain=0:\rboundvv,smooth,variable=\y,blue, dotted, line width=1pt] plot ({\uu+exp((\vv+\y)/2)},{\y});
%
%    %Hyperbolic model
%    %Right boundary
%	\draw[domain=0:\rhboundvv,variable=\y, samples=200, blue, line width=1pt] plot 		
%		({\uu+(exp(\R/2)/2)*rad(acos((cosh(\R-\vv)*cosh(\R-\y)-cosh(\R))/(sinh(\R-\vv)*sinh(\R-\y))))},{\y});
%
%
%
%\end{tikzpicture}~\hspace{5pt}~
%\begin{tikzpicture}[scale=0.85]
%	%Define the coordinates 
%	%p = (\u,\v), p_1 = (\uu, \vv)
%	%Box \Rcal_n has width 2\r and height \t (\r = \pi/2 e^{R_n/2})
%	\pgfmathsetmacro{\u}{0}
%	\pgfmathsetmacro{\v}{0.5}
%	\pgfmathsetmacro{\uu}{1}
%	\pgfmathsetmacro{\vv}{2.7}
%	\pgfmathsetmacro{\uuu}{5.5}
%	\pgfmathsetmacro{\vvv}{1.7}
%	\pgfmathsetmacro{\epsilon}{0.05}
%	\pgfmathsetmacro{\K}{1}
%	\pgfmathsetmacro{\R}{3}
%	\pgfmathsetmacro{\r}{(pi/2)*exp(\R/2)}
%	\pgfmathsetmacro{\t}{\R}
%	
%	\pgfmathsetmacro{\leftintvandvvv}{-exp((\v + \vvv)/2)}
%	\pgfmathsetmacro{\rightintvandvvv}{exp((\v + \vvv)/2)}
%	\pgfmathsetmacro{\leftintvvandvvv}{\uu-exp((\vv + \vvv)/2)}
%	\pgfmathsetmacro{\rightintvvandvvv}{\uu+exp((\vv + \vvv)/2)}
%		
%	%The box \Rcal_n
%	\draw[line width=1pt,dashed] (0,0) -- (\r,0) -- (\r,\t) -- (0,\t);
%
%	%Dram all three nodes
%    \draw node[fill, circle, inner sep=0pt, minimum size=5pt] (p0) at (\u,\v) {};
%    \path (p0)+(-0.3,0.3) node {$p_0$};
%    
%    \draw node[fill,blue, circle, inner sep=0pt, minimum size=5pt] (p1) at (\uu,\vv) {};
%    \path (p1)+(-0.3,-0.3) node {\color{blue}$p_1$};	
%
%	
%	%Boundaries p_0 = (\u,\v)
%	
%	%Limit model
%	%Right boundary
%	\pgfmathsetmacro{\rightbounduv}{\u+exp((\v)/2)}
%	\draw[domain=0:\R,smooth,variable=\y,black, dotted, line width=1pt] plot ({exp((\v+\y)/2)},{\y});
%
%    
%    %Hyperbolic model
%    %Right boundary
%	\draw[domain=0:{\R-\v},variable=\y, samples=200, black, line width=1pt] plot 		
%		({(exp(\R/2)/2)*rad(acos((cosh(\R-\v)*cosh(\R-\y)-cosh(\R))/(sinh(\R-\v)*sinh(\R-\y))))},{\y});
%
%    
%	%Boundaries p_1 = (\uu,\vv)
%	
%	%Limit model
%	\pgfmathsetmacro{\rboundvv}{2*ln(\r-\uu)-\vv}
%	\pgfmathsetmacro{\lboundvv}{2*ln(\r+\uu)-\vv}
%
%    
%    %Hyperbolic model
%	\pgfmathsetmacro{\rhboundvv}{0.277}
%	\pgfmathsetmacro{\lhboundvv}{\R-\vv}
%
%	%Intersection
%	\pgfmathsetmacro{\regionright}{\uu+exp((\vv+\rboundvv)/2)}
%	\pgfmathsetmacro{\intersection}{0.15}
%	
%	\draw[red,line width=1pt,pattern=north west lines, pattern color=red] 
%		plot[domain=\intersection:\rboundvv,smooth,variable=\y,red] ({\uu+exp((\vv+\y)/2)},{\y}) 
%		--
%		(\regionright,\rboundvv) 
%		--
%		(\regionright,\rhboundvv)
%		--
%		plot[domain={\rhboundvv-0.01}:\intersection,smooth,variable=\y,red] ({\uu+(exp(\R/2)/2)*rad(acos((cosh(\R-\vv)*cosh(\R-\y)-cosh(\R))/(sinh(\R-\vv)*sinh(\R-\y))))},{\y});
%		
%	\draw[red,line width=1pt,pattern=north west lines, pattern color=red] 
%		plot[domain=0:\intersection,smooth,variable=\y,red] ({\uu+exp((\vv+\y)/2)},{\y}) 
%		--
%		plot[domain=\intersection:0,smooth,variable=\y,red] ({\uu+(exp(\R/2)/2)*rad(acos((cosh(\R-\vv)*cosh(\R-\y)-cosh(\R))/(sinh(\R-\vv)*sinh(\R-\y))))},{\y});
%
%	%Limit model
%	%Right boundary
%	\draw[domain=0:\rboundvv,smooth,variable=\y,blue, dotted, line width=1pt] plot ({\uu+exp((\vv+\y)/2)},{\y});
%
%    %Hyperbolic model
%    %Right boundary
%	\draw[domain=0:\rhboundvv,variable=\y, samples=200, blue, line width=1pt] plot 		
%		({\uu+(exp(\R/2)/2)*rad(acos((cosh(\R-\vv)*cosh(\R-\y)-cosh(\R))/(sinh(\R-\vv)*sinh(\R-\y))))},{\y});
%
%
%\end{tikzpicture}
%
%\begin{tikzpicture}[scale=0.85]
%	%Define the coordinates 
%	%p = (\u,\v), p_1 = (\uu, \vv) and p_2 = (\uuu, \vvv)
%	%Box \Rcal_n has width 2\r and height \t (\r = \pi/2 e^{R_n/2})
%	\pgfmathsetmacro{\u}{0}
%	\pgfmathsetmacro{\v}{0.5}
%	\pgfmathsetmacro{\uu}{1}
%	\pgfmathsetmacro{\vv}{1.2}
%	\pgfmathsetmacro{\uuu}{3}
%	\pgfmathsetmacro{\vvv}{2.5}
%	\pgfmathsetmacro{\epsilon}{0.05}
%	\pgfmathsetmacro{\K}{1}
%	\pgfmathsetmacro{\R}{3}
%	\pgfmathsetmacro{\r}{(pi/2)*exp(\R/2)}
%	\pgfmathsetmacro{\t}{\R}
%	
%	\pgfmathsetmacro{\leftintvandvvv}{-exp((\v + \vvv)/2)}
%	\pgfmathsetmacro{\rightintvandvvv}{exp((\v + \vvv)/2)}
%	\pgfmathsetmacro{\leftintvvandvvv}{\uu-exp((\vv + \vvv)/2)}
%	\pgfmathsetmacro{\rightintvvandvvv}{\uu+exp((\vv + \vvv)/2)}
%		
%	%The box \Rcal_n
%	\draw[line width=1pt,dashed] (0,0) -- (\r,0) -- (\r,\t) -- (0,\t);
%
%	%Dram all three nodes
%    \draw node[fill, circle, inner sep=0pt, minimum size=5pt] (p0) at (\u,\v) {};
%    \path (p0)+(-0.3,0.3) node {$p_0$};
%    
%    \draw node[fill,blue, circle, inner sep=0pt, minimum size=5pt] (p1) at (\uu,\vv) {};
%    \path (p1)+(-0.3,-0.3) node {\color{blue}$p_1$};	
%
%	
%	%Boundaries p_0 = (\u,\v)
%	
%	%Limit model
%	%Right boundary
%	\pgfmathsetmacro{\rightbounduv}{\u+exp((\v)/2)}
%	\draw[domain=0:\R,smooth,variable=\y,black, dotted, line width=1pt] plot ({exp((\v+\y)/2)},{\y});
%
%    
%    %Hyperbolic model
%    %Right boundary
%	\draw[domain=0:{\R-\v},variable=\y, samples=200, black, line width=1pt] plot 		
%		({(exp(\R/2)/2)*rad(acos((cosh(\R-\v)*cosh(\R-\y)-cosh(\R))/(sinh(\R-\v)*sinh(\R-\y))))},{\y});
%
%    
%	%Boundaries p_1 = (\uu,\vv)
%	
%	%Limit model
%	\pgfmathsetmacro{\rboundvv}{2*ln(\r-\uu)-\vv}
%	\pgfmathsetmacro{\lboundvv}{2*ln(\r+\uu)-\vv}
%
%    
%    %Hyperbolic model
%	\pgfmathsetmacro{\rhboundvv}{1.76}
%	\pgfmathsetmacro{\lhboundvv}{\R-\vv}
%
%	%Intersection
%	\pgfmathsetmacro{\regionright}{\uu+exp((\vv+\rboundvv)/2)}
%	\pgfmathsetmacro{\intersection}{0.6}
%	
%	\draw[red,line width=1pt,pattern=north west lines, pattern color=red] 
%		plot[domain=\intersection:\rboundvv,smooth,variable=\y,red] ({\uu+exp((\vv+\y)/2)},{\y}) 
%		--
%		(\regionright,\rboundvv) 
%		--
%		(\regionright,\rhboundvv)
%		--
%		plot[domain={\rhboundvv-0.01}:\intersection,smooth,variable=\y,red] ({\uu+(exp(\R/2)/2)*rad(acos((cosh(\R-\vv)*cosh(\R-\y)-cosh(\R))/(sinh(\R-\vv)*sinh(\R-\y))))},{\y});
%		
%	\draw[red,line width=1pt,pattern=north west lines, pattern color=red] 
%		plot[domain=0:\intersection,smooth,variable=\y,red] ({\uu+exp((\vv+\y)/2)},{\y}) 
%		--
%		plot[domain=\intersection:0,smooth,variable=\y,red] ({\uu+(exp(\R/2)/2)*rad(acos((cosh(\R-\vv)*cosh(\R-\y)-cosh(\R))/(sinh(\R-\vv)*sinh(\R-\y))))},{\y});
%
%	%Limit model
%	%Right boundary
%	\draw[domain=0:\rboundvv,smooth,variable=\y,blue, dotted, line width=1pt] plot ({\uu+exp((\vv+\y)/2)},{\y});
%
%    %Hyperbolic model
%    %Right boundary
%	\draw[domain=0:\rhboundvv,variable=\y, samples=200, blue, line width=1pt] plot 		
%		({\uu+(exp(\R/2)/2)*rad(acos((cosh(\R-\vv)*cosh(\R-\y)-cosh(\R))/(sinh(\R-\vv)*sinh(\R-\y))))},{\y});
%
%
%\end{tikzpicture}
%\end{figure}



\subsection{Equivalence clustering $G_{\HP,n}(\alpha,\nu)$ and $G_{\Pcal,n}(\alpha,\nu)$}\label{ssec:coupling_HP_ast_P}


Here we prove Proposition~\ref{prop:couling_c_H_P}. We first establish a few results regarding the number of nodes of degree $k_n$ in both the \TM{ Poissonized? } hyperbolic random graph $G_{\HP,n}$ and the finite box model $G_{\Pcal,n}$.

\begin{lemma}\label{lem:N_k_HP_P}
Let $\alpha > 1/2$, $\nu > 0$ and $\{k_n\}_{n\ge 1}$ be a sequence such that $k_n = \bigO{n^{1/(2\alpha + 1)}}$. Then
\begin{equation} \label{eq:n_k_Hyp}
	\Exp{N_{\HP,n}(k_n)} = \bigT{1} n k_n^{-(2\alpha + 1)},
\end{equation}
and
\begin{equation} \label{eq:n_k_Po}
	\Exp{N_{\Pcal,n}(k_n)} = \bigT{1} n k_n^{-(2\alpha + 1)}.
\end{equation}
Moreover,
\begin{equation}\label{eq:equivalence_N_HP_P}
	\lim_{n \to \infty} \left|\frac{\Exp{N_{\HP,n}(k_n)}}{\Exp{N_{\Pcal,n}(k_n)}} - 1\right| = 0.
\end{equation}
\end{lemma}

\begin{proof}
Recall that
\[
	\Exp{N_{\H,n}(k_n)} = \int_{\Rcal_n} \rho_{\HP,n}(y,k_n) f_{\alpha,\nu}(x,y) \dd x \dd y. 
\]
Then by Lemma~\ref{lem:concentration_argument_rho_approximation} and a concentration argument \TM{ Please provide details for the ``concentration argumnent''. }
\begin{align*}
	\Exp{N_{\H,n}(k_n)} &= (1 + \smallO{1})\int_{\Rcal_n} \rho(y,k_n) f_{\alpha,\nu}(x,y) \dd x \dd y\\
	&= (1 + \smallO{1}) n \int_{0}^{R_n} \rho(y,k_n) f_{\alpha,\nu}(x,y) \dd x \dd y = \bigT{1}n k_n^{-(2\alpha + 1)}.
\end{align*}
Similarly,
\[
	\Exp{N_{\Pcal,n}(k_n)} = (1 + \smallO{1})\int_{\Rcal_n} \rho(y,k_n) f_{\alpha,\nu}(x,y) \dd x \dd y
\]
From which both~\eqref{eq:n_k_Po} and~\eqref{eq:equivalence_N_HP_P} follow.
\end{proof}

Recall that Proposition~\ref{prop:couling_c_H_P} states
\[
	\lim_{n \to \infty} s_\alpha(k_n)^{-1} \, \Exp{\left|c_{\HP,n}^\ast(k_n) - c_{\Pcal,n}^\ast(k_n)\right|} = 0.
\]
Since for $\alpha > 3/4$, $s_{3/4}(k_n) = \log(k_n)^{-1} s_{\alpha}(k_n) = \smallO{s_{\alpha}(k_n)}$ it suffices to prove the following two cases:
\begin{enumerate}
\item if $1/2 < \alpha \leq 3/4$, then
\[
	\lim_{n\to \infty} k_n^{4\alpha -2}\cdot \Exp{\left|  c_{\HP}^\ast(k_n) - c_\Pcal^\ast(k_n)\right|}=0,
\]
\item if $3/4 < \alpha$, then
\[ 
	\lim_{n\to \infty} k_n \cdot \Exp{\left|  c_{\HP}^\ast(k_n) - c_\Pcal^\ast(k_n)\right|}=0.
\]
\end{enumerate}

Recall the definition of $\Kcal_{C}(k_n)$
\[
	\Kcal_C(k_n) = \left\{p \in \R : \frac{k_n - C \kappa_n}{\xi_{\alpha,\nu}} \vee 0 \le e^{\frac{y}{2}}
	\le \frac{k_n + C \kappa_n}{\xi_{\alpha,\nu}} \wedge e^{R_n/2} \right\},
\]
with $C > 0$ and 
\[
	\kappa_n := \begin{cases}
		\log(n) &\mbox{if } k_n = \bigT{1},\\
		\sqrt{k_n \log(k_n)} &\mbox{else.}
	\end{cases}
\]

The following lemma will be frequently used in the proof of Proposition~\ref{prop:couling_c_H_P}.

\begin{lemma} \label{lem:gamma_approx}
Let $t, r \in \mathbb{R}$ be fixed and let $\hat{\rho}(y,k)$ be any of the three probability functions $\rho_{\HP,n}(y,k), \rho_{\Pcal,n}(y,k)$ or $\rho(y,k)$. Then for any sequence $k_n$ of positive integers with $k_n = \bigO{n^{\frac{1}{2\alpha + 1}}}$ and $C > 0$ large enough,
\[
	\int_{\Kcal_{C}} e^{t y} \hat{\rho}_n(y,k_n-r) e^{-\alpha y} \dd y = \bigO{1} n k_n^{-2\alpha - 1 + 2t}
\]
as $n \to \infty$.
\end{lemma}
\begin{proof}
Note that on $\Kcal_{C}(k_n)$ we have that $e^{ty} = \bigT{k_n^{2t}}$. Hence, by a concentration argument
\begin{align*}
	\int_{\Kcal_{C}} e^{t y} \hat{\rho}_n(y,k_n-r) e^{-\alpha y} \dd y
	&= \bigT{k_n^{2t}} \int_{\Kcal_{C}} \hat{\rho}_n(y,k_n-r) e^{-\alpha y} \dd y\\
	&= \bigO{k_n^{2t}} n \Exp{N_{\Pcal}(k_n)} = \bigO{1} n k_n^{-2\alpha - 1 + 2t}.
\end{align*}
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:couling_c_H_P}] 

To keep notations concise we abbreviate $\Exp{N_{\HP} (k_n)}$ and $\Exp{N_{\Pcal} (k_n)}$ by $\expH$ and $\expP$, respectively. We will also suppress the subscripts $n$ in most expression regarding the graphs $G_{\HP,n}$ and $G_{\Pcal,n}$. Then we have
\begin{align*} 
	&\Exp{\left|  c_{\HP}^\ast(k_n) - c_{\Pcal}^\ast(k_n)\right|}= 
	\binom{k_n}{2}^{-1}\Exp{\left|\sum_{p \in \Pcal} 
    	\frac{\ind{D_{\HP}(p) = k_n}}{\expH} \Delta_{\HP}(p )
        - \frac{\ind{D_\Pcal(p) = k_n}}{\expP}  \Delta_\Pcal(p)\right|} \\
    &\le\binom{k_n}{2}^{-1} \expH^{-1} \Exp{\left|\sum_{p \in \Pcal} \ind{D_{\HP}((0,y)) = k_n} \Delta_{\HP}(p) 
    	- \ind{D_\Pcal(p) = k_n} \Delta_\Pcal(p)\right|} \\
    &\hspace{10pt}+ \binom{k_n}{2}^{-1} \left|\frac{1}{\expH} - \frac{1}{\expP}\right|\Exp{
        	\sum_{p \in \Pcal} \ind{D_\Pcal(p) = k_n} \Delta_\Pcal(p) }
\end{align*}
The last term can be rewritten as
\begin{align*}
	\left|1 - \frac{\expH}{\expP}\right| \Exp{c_\Pcal^\ast(k_n)} = \left|1 - \frac{\expH}{\expP}\right|c_\infty(k_n)(1+o(1)),
\end{align*}
where we used Proposition~\ref{prop:convergence_average_clustering_P_n} (See Section~\ref{sec:clustering_Pn_to_P}). The first term in this product converges to zero by Lemma~\ref{lem:N_k_HP_P} while the second term scales as $s_\alpha(k_n)$ by Theorem~\ref{thm:asymptotics_average_clustering_P}. Hence
\[
	\left|1 - \frac{\expH}{\expP}\right| \Exp{c_\Pcal^\ast(k_n)} = \smallO{s_\alpha(k_n)},
\]
and therefore we are left to analyze the other term. By the Campbell-Mecke formula~\eqref{eq:def_Campbell-Mecke} we have that
\begin{align*}
	    &\Exp{\left|\sum_{p \in \Pcal} \frac{\ind{D_\H((0,y)) = k_n}}{\expH} \Delta_\H(p) 
	        	- \frac{\ind{D_\Pcal(p) = k_n}}{\expH} \Delta_\Pcal(p)\right|} \\
	    &= \int_{\Rcal_n} 
	        \Exp{\left|\frac{\ind{D_\H((0,y)) = k_n}}{\expH} \Delta_\H ((0,y) ) 
	        - \frac{\ind{D_\Pcal((0,y)) = k_n}}{\expH} \Delta_\Pcal ((0,y))\right|} 
	        	f_{\alpha,\nu}(x,y) \dd y \dd x.
\end{align*}
Since 
\begin{align*}
	\Exp{\frac{\ind{D_\H((0,y)) = k_n}}{\expH} \Delta_\H ((0,y) )}
	&\le \binom{k_n}{2} \rho_{\HP}(y,k_n)\expH^{-1} \\
	&= \binom{k_n}{2} \rho_{\HP}(y,k_n)\bigT{\expP^{-1}}\\
	&= \bigT{n^{-1} k_n^{2\alpha + 3}}\rho_{\HP,n}(y,k_n)
\end{align*}
and similar for the other term, it follows that
\begin{align*}
	&\hspace{-60pt}\Exp{ \left| \frac{\ind{D_\H((0,y)) = k_n}}{\expH} \Delta_\H ((0,y) )
		- \frac{\ind{D_\Pcal((0,y)) = k_n}}{\expH}  \Delta_\Pcal ((0,y))\right|} \\
	&\le \bigT{n^{-1} k_n^{2\alpha + 3}}\left(\rho_{\HP,n}(k,n) + \rho_n(y,k_n)\right).
\end{align*}
Therefore, by a concentration argument (c.f. Corollary~\ref{cor:concentration_argument_other_models}), it is enough to consider the integral
\begin{equation} \label{eq:expectation_total}
	\int_{\Kcal_{C}(k_n)} \Exp{\left|\frac{\ind{D_\H((0,y)) = k_n}}{\expH} \Delta_\H ((0,y) ) 
		- \frac{\ind{D_\Pcal((0,y)) = k_n}}{\expH} \Delta_\Pcal ((0,y))\right|} e^{-\alpha y} \dd y \dd x,
\end{equation}
where we also used that $f_{\alpha,\nu}(x,y)$ is simply a constant multiple of the function $e^{-\alpha y}$. We shall proceed by expanding the integrand and analyzing the individual terms. With a slight abuse of notation we shall write $y$ instead of $(0,y)$ in expression such as $\BallHyp{y}$. In addition we write $D_{\H}(y,k_n;\Pcal)$ for the indicator which is equal to 1 if and only if $\BallHyp{(0,y)}$ contains $k_n$ points from $\Pcal \setminus \{(0,y)\}$. We define $D_{\Pcal}(y,k_n;\Pcal)$ analogously for the ball $\BallPo{(0,y)}$.

We shall divide the analysis into the two case $k_n = \bigO{1}$ and $k_n \to \infty$, the proof of the latter case being the most involved one. 

\subsubsection*{The case $\bm{k_n = \bigT{1}}$}

In this case we split the integrand as follows:
\begin{align*}
	&\Exp{\left|\frac{\ind{D_\H((0,y)) = k_n}}{\expH} \Delta_\H ((0,y)) 
		- \frac{\ind{D_\Pcal((0,y)) = k_n}}{\expH} \Delta_\Pcal ((0,y))\right|}\\
	&\le \expH^{-1} {\mathbb{E}}\left[\sum_{p_1, p_2\in \Pcal_n \setminus \{(0,y)\}}^{\ne} \hspace{-3pt} 
		\left(D_{\H}(y,k_n-2;\Pcal \setminus \{p_1,p_2\})+ D_{\Pcal}(y,k_n-2;\Pcal \setminus \{p_1,p_2\})\right) \right.\\
	&\hspace{75pt}\left. \times	\left|\Delta_\H (y,p_1,p_2) - \Delta_\Pcal (y,p_1,p_2)\right| 
		\vphantom{\sum_{p_1, p_2\in \Pcal_n \setminus \{(0,y)\}}^{\ne}}\right]\\
	&\le \expH^{-1} \Exp{\sum_{p_1 \in \Pcal_n \setminus \{(0,y)\}} \hspace{-3pt} 
		D_{\H}(y,k_n-2;\Pcal \setminus \{p_1,p_2\}) \ind{p_1 \in \BallSym{y}}}^2\\
	&\hspace{10pt}+ \expH^{-1} \Exp{\sum_{p_1 \in \Pcal_n \setminus \{(0,y)\}} \hspace{-3pt} 
			D_{\Pcal}(y,k_n-2;\Pcal \setminus \{p_1,p_2\}) \ind{p_1 \in \BallSym{y}}}^2
\end{align*}
We will proceed with analyzing the first term with $D_{\H}(y,k_n-2;\Pcal \setminus \{p_1,p_2\})$. The analysis for the other term is analogous.

We write
\begin{align*}
	&\Exp{\sum_{p_1 \in \Pcal_n \setminus \{(0,y)\}} \hspace{-3pt} 
		D_{\H}(y,k_n-2;\Pcal \setminus \{p_1,p_2\}) \ind{p_1 \in \BallSym{y}}}\\
	&\le \Exp{\sum_{p_1 \in \Pcal_n \setminus \{(0,y)\}} \hspace{-3pt} 
		D_{\H}(y,k_n-2;\Pcal \setminus \{p_1,p_2\}) \ind{y_1 > R_n/4 - y} \ind{p_1 \in \BallSym{y}}}\\
	&\hspace{10pt}+ \Exp{\sum_{p_1 \in \Pcal_n \setminus \{(0,y)\}} \hspace{-3pt} 
		\ind{y_1 \le R_n/4 - y} \ind{p_1 \in \BallSym{y}}}.
\end{align*}
Next we note that since $k_n = \bigO{1}$ we have that $\kappa_n = \log(n)$ and hence, since $y \in \Kcal_C(k_n)$ it follows that $y = \bigO{\log\log(n)}$. Now let $n$ be large enough such that $R_n/4 - y > 0$. Then for any $y^\prime \le R_n/4 - y$ it follows from \cite[Lemma 30]{fountoulakis2018law} that under the coupling we consider any edge between $p^{\prime} = (x^\prime, y^\prime)$ and $(0,y)$ is present a.a.s. in both $G_{\HP,n}$ and $G_{\Pcal,n}$. In particular we have that
\[
	\int_{\Kcal_{C}(k_n)} \Exp{\sum_{p_1 \in \Pcal_n \setminus \{(0,y)\}} \hspace{-3pt} \ind{y_1 \le R_n/4 - y} 
		\ind{p_1 \in \BallSym{y}}} \alpha e^{-\alpha y} \dd x \dd y = \smallO{1}.
\]
Therefore, since $k_n^a \expH^{-1} = \bigO{n^{-1}}$ for any $a > 0$ it follows that
\[
	\lim_{n \to \infty} k_n^a \, \expH^{-1} \int_{\Kcal_{C}(k_n)} \Exp{\sum_{p_1 \in \Pcal_n \setminus \{(0,y)\}} \hspace{-3pt} \ind{y_1 \le R_n/4 - y} \ind{p_1 \in \BallSym{y}}} \alpha e^{-\alpha y} \dd x \dd y = 0
\]
and hence it is enough to consider the case where $y_1 > R_n/4 - y$, i.e. to analyze the term
\[
	\Exp{\sum_{p_1 \in \Pcal_n \setminus \{(0,y)\}} \hspace{-3pt} 
			D_{\H}(y,k_n-2;\Pcal \setminus \{p_1,p_2\}) \ind{y_1 > R_n/4 - y} \ind{p_1 \in \BallSym{y}}}
\]
By the Campbell-Mecke formula and Lemma~\ref{lem:asymptotics_Omega_hyperbolic} we have
\begin{align*}
	&\Exp{\sum_{p_1 \in \Pcal_n \setminus \{(0,y)\}} \hspace{-3pt} 
				D_{\H}(y,k_n-2;\Pcal \setminus \{p_1,p_2\}) \ind{y_1 > R_n/4 - y} \ind{p_1 \in \BallSym{y}}}\\
	&\le \int_{R_n/4 - y}^{R_n} \int_{-I_n}^{I_n} \left(\ind{p_1 \in \BallHyp{y}} + \ind{p_1 \in \BallPon{y}}\right)
		f_{\alpha,\nu}(x_1, y_1) \dd x_1 \dd y_1\\
	&= \bigO{1} e^{y/2} \int_{R_n/4 - y}^{R_n - y} e^{-(\alpha - \frac{1}{2})y_1} \dd y_1
		+ \bigO{1} n \int_{R_n - y}^{R_n} e^{-\alpha y_1} \dd y_1\\
	&= \bigO{e^{(1 - \alpha)y} e^{-(2\alpha - 1) R_n/8} + e^{\alpha y} n e^{-\alpha R_n}}\\
	&= \bigO{n^{-\frac{2\alpha - 1}{4}} e^{(1 - \alpha)y} + n^{-(2\alpha - 1)} e^{\alpha y}}.
\end{align*}
Integrating this term over $\Kcal_C(k_n)$ yields
\begin{align*}
	&\expH^{-1} \int_{\Kcal_C(k_n)} \Exp{\sum_{p_1 \in \Pcal_n \setminus \{(0,y)\}} \hspace{-3pt} 
		D_{\H}(y,k_n-2;\Pcal \setminus \{p_1,p_2\}) \ind{y_1 > R_n/4 - y} \ind{p_1 \in \BallSym{y}}} 
		e^{-\alpha y} \dd y \dd x \\
	&\le \bigO{1} n^{-1} \int_{\Kcal_C(k_n)} 
		\left(n^{-\frac{2\alpha - 1}{4}} e^{(1 - \alpha)y} + n^{-(2\alpha - 1)} e^{\alpha y}\right) 
		e^{-\alpha y} \dd y \dd x\\
	&= \bigO{\log(n)^2\left(n^{-\frac{2\alpha - 1}{4}} + n^{-(2\alpha - 1)}\right)} = \smallO{k_n^a},
\end{align*}
for any $a \in \R$, since $k_n = \bigT{1}$. We therefore conclude that
\[
	\int_{\Kcal_{C}(k_n)} \Exp{\left|\frac{\ind{D_\H((0,y)) = k_n}}{\expH} \Delta_\H ((0,y) ) 
		- \frac{\ind{D_\Pcal((0,y)) = k_n}}{\expH} \Delta_\Pcal ((0,y))\right|} e^{-\alpha y} \dd y \dd x
	= \smallO{s_\alpha(k_n)},
\]
as required.

\subsubsection*{The case $\bm{k_n \to \infty}$}

For this case we need to split the integrand over several terms and then analyze each of these separately. Applying the Campbel-Mecke formula~\eqref{eq:def_Campbell-Mecke} yields
\begin{align*} 
 &\hspace{-30pt}\Exp{ \left| \frac{\ind{D_\H((0,y)) = k_n}}{\expH} \Delta_\H ((0,y) )
        - \frac{\ind{D_\Pcal((0,y)) = k_n}}{\expH}  \Delta_\Pcal ((0,y))
        \right|}\leq \\
 & {\mathbb E} \left[ \sum_{(p_1,p_2) \in \Pcal \setminus \{(0,y)\}}^{\not =} 
  \left| \frac{D_{\H}(y,k_n-2; \Pcal \setminus \{p_1,p_2 \})}{\expH} \Delta_\H (y,p_1,p_2) \right. \right.\\
  & \hspace{5cm} 
\left. \left. - \frac{D_{\Pcal} (y,k_n-2;\Pcal \setminus \{p_1,p_2\})}{\expH} \Delta_{\Pcal} (y,p_1,p_2)
   \right| \vphantom{\sum_{p_1,p_2 \in \Pcal \setminus \{(0,y)\}}^{\not =}}\right],
\end{align*}
where the sum ranges over all distinct pairs of points in $\Pcal \setminus \{ (0,y)\}$. In what follows, we will set $\BallSym{p'} = \BallHyp{p'} \bigtriangleup \BallPo{p'}$ and $\BallInter{p'} =\BallHyp{p'} \cap \BallPo{p'}$. 
We will now bound the sum that is inside the expectation. Note that each summand is the absolute value of the difference between two quantities that are either equal to 0 or of order $\expH^{-1}$ ($\expP^{-1}$). We will split these summands into 5 classes which are all combinations of $p_1, p_2\in \Pcal \setminus \{(0,y)\}$ for which only one of the two terms of this difference is non-zero. 
\begin{enumerate} 
\item both $p_1$ and $p_2$ have $y_1,y_2 < (1-\eps ) R_n \wedge (R_n-y)$ and 
\begin{enumerate}
\item $p_1$ is in $\BallInter{y}$ but $p_2 \in \BallHyp{p_1} \setminus \BallPo{p_1}$ 
and $\BallHyp{y}$ contains exactly $k_n-2$ or $k_n-1$ other points (depending on whether 
$p_2 \in \BallHyp{y}$ or not).
\item $p_1$ is in $\BallInter{y}$ but $p_2 \in \BallPo{p_1} \setminus \BallHyp{p_1}$ 
and $\BallPo{y}$ contains exactly $k_n-2$ or $k_n-1$ other points (depending on whether 
$p_2 \in \BallPo{y}$ or not).
\end{enumerate}
\item the above cases but with $y_1 \geq (1-\eps) R_n \wedge (R_n -y)$. 
\item $y_1 \geq K$ and $p_1 \in \BallHyp{y} \setminus 
\BallPo{y}$ and $p_2 \in \BallInter{y}$ - here we use 
Corollary~\ref{cor:balls_inclusion} which implies that if $p_1 \in \BallSym{y}$ and $y_1 \geq K$, then in fact $p_1 \in \BallHyp{y} \setminus \BallPo{y}$. 
\item $y_1 < K$ and $p_1 \in \BallSym{y}$ and $p_2 \in \BallInter{y}$. 
%\item $p_1$ and $p_2$ are such that $\Delta_\H ((0,y),p_1,p_2)=\Delta_\Pcal ((0,y),p_1,p_2)=1$. 
\end{enumerate}
We bound this sum by the following expression:
\begin{align} 
&\sum_{(p_1,p_2) \in \Pcal \setminus \{(0,y)\}}^{\not =} 
  \left| \frac{D_{\H}(y,k_n-2; \Pcal \setminus \{p_1,p_2 \})}{\expH} \Delta_\H (y,p_1,p_2) \right. \notag \\
  & \hspace{5cm} \left. - \frac{D_{\Pcal} (y,k_n-2;\Pcal \setminus \{p_1,p_2\})}{\expH} \Delta_{\Pcal} (y,p_1,p_2)
   \right|  \notag \\
&\le \expH^{-1} \hspace{-15pt} \sum_{p_1,p_2\in \Pcal \setminus \{(0,y)\} \atop  y_1, y_2 < (1-\eps)R_n \wedge (R_n-y)} 
	\hspace{-10pt} \ind{p_1 \in \BallInter{(0,y)}} \cdot \ind{p_2 \in \BallSym{p_1}} 
	\, D_\H (y,k_n-2;\Pcal \setminus \{ p_1,p_2\}) \label{eq:sum_1}\\
&\le \expH^{-1} \hspace{-15pt} \sum_{p_1,p_2\in \Pcal \setminus \{(0,y)\} \atop  y_1, y_2 < (1-\eps)R_n \wedge (R_n-y)} 
	\hspace{-10pt} \ind{p_1 \in \BallInter{(0,y)}} \cdot \ind{p_2 \in \BallSym{p_1}} 
	\, D_\Pcal (y,k_n-2;\Pcal \setminus \{ p_1,p_2\}) \label{eq:sum_2}\\
&\hspace{10pt}+ \expH^{-1} \hspace{-15pt} \sum_{p_1,p_2 \ \in \Pcal \setminus \{(0,y) \} 
	\atop y_1 \geq (1-\eps) R_n \wedge (R_n-y)} \hspace{-10pt}
	\ind{p_1 \in \BallInter{y}} \cdot \ind{p_2 \in \BallSym{p_1}\cap \BallHyp{y}} 
	\, D_\H (y,k_n-2;\Pcal \setminus \{ p_1,p_2\}) \label{eq:sum_3} \\
&\hspace{10pt}+ \expH^{-1} \hspace{-15pt} \sum_{p_1,p_2 \ \in \Pcal \setminus \{(0,y) \} 
	\atop y_1 \geq (1-\eps) R_n \wedge (R_n-y)} \hspace{-10pt}
	\ind{p_1 \in \BallInter{y}} \cdot \ind{p_2 \in \BallSym{p_1}\cap \BallHyp{y}} 
	\, D_\Pcal (y,k_n-2;\Pcal \setminus \{ p_1,p_2\}) \label{eq:sum_4} \\
&\hspace{10pt}+ 2 \expH^{-1} \hspace{-10pt} \sum_{p_1,p_2 \in\Pcal \setminus \{(0,y)\} 
	\atop y(p_1) \geq K} \hspace{-10pt} \ind{p_1\in \BallHyp{y}\setminus \BallPo{y}} \ind{p_2\in \BallHyp{y}\cap \BallPo{y}} 
	\, D_\H (y,k_n-2;\Pcal \setminus \{ p_1,p_2\}) \label{eq:sum_5}\\
&\hspace{10pt}+ (\expH^{-1} + \expP^{-1}) \hspace{-10pt} \sum_{p_1,p_2 \in\Pcal \setminus \{(0,y)\} \atop \ y(p_1) < K}
	\ind{p_1\in \BallSym{y}} \ind{p_2\in \BallHyp{y}\cap \BallPo{y}} \label{eq:sum_6}
%& +  \sum_{p_1,p_2 \in \Pcal \setminus \{(0,y)\}}^{\not =} \ind{p_1,p_2 \in \BallHyp{(0,y)}\cup \BallPo{(0,y)}} \cdot 
%D_{\H}(y,k_n-2; \Pcal \setminus \{ p_1,p_2\})  \cdot 
%\left| \expH^{-1} - \expP^{-1} \right|.  \label{eq:sum_7}
\end{align}
In the following sections we will give upper bounds on the expected values of each one of these partial sums. 

\paragraph{The sums~\eqref{eq:sum_1} and~\eqref{eq:sum_2}}

We will analyze~\eqref{eq:sum_1}. The analysis of the other sum~\eqref{eq:sum_2} is similar.
Note first that for any two points $p_1,p_2$ the following holds: $p_1 \in \BallHyp{y}$ and $p_2 \in \BallSym{p_1}\cap \BallHyp{y}$, then $p_2 \in \BallHyp{y}$ and $p_1 \in \BallSym{p_2}\cap \BallHyp{y}$.
Using this symmetry, it suffices to consider distinct pairs $(p_1,p_2) \in \Pcal \setminus \{(0,y)\}$ with $0\leq y_2 \leq y_1 \leq R- y$. Let $\dom{}$ denote the set of these pairs. 

We are going to consider several sub-cases and, thereby, split the domain $\dom{}$ into the corresponding sub-domains. 
Let $\omega =\omega (n) \to \infty$ as $n\to \infty$ be a slowly growing function and set $y_\omega := y +\omega$. 
We let 
\begin{align*}
	\dom{1} &= \{(p_1,p_2) \in \dom{} \ : \ y \leq y_1 \leq R_n/2, \, y_\omega \leq y_2 \leq y_1 \},\\
	\dom{2} &= \{(p_1,p_2) \in \dom{} \ : \ y_1 \leq R_n/2, \, y_2 \leq y_\omega \} \text{ and }\\
	\dom{3} &=  \{(p_1,p_2) \in \dom{} \ : R_n/2 < y_1 \leq R_n -y, \, y_2 \leq y_1 \}.
\end{align*} 
Note that $\dom{} \subseteq \dom{1} \cup \dom{2}\cup \dom{3}$.
Hence, we can write 
\begin{equation} \label{eq:1sum-rewriting}
\begin{split} 
	&\Exp{\sum_{p_1, p_2\in \Pcal \setminus \{(0,y)\} 
	\atop y_1, y_2 \leq (1-\eps) R_n\wedge (R_n-y)} \hspace{-10pt} \ind{p_1 \in \BallHyp{y}} 
	\ind{p_2 \in \BallSym{p_1}\cap \BallHyp{y}} 
	\, D_\H (y,k_n-2;\Pcal \setminus \{p_1,p_2\})} \\ 
	&= \sum_{i=1}^3 \Exp{\sum_{(p_1, p_2)\in \dom{i}} \ind{p_1 \in \BallHyp{y}} 
	\ind{p_2 \in \BallSym{p_1}\cap \BallHyp{y}} \cdot D_\H (y,k_n-2;\Pcal \setminus \{p_1,p_2\})}.
\end{split}
\end{equation}
We bound each one of the above three summands as follows:  
\begin{equation} \label{eq:term1}
\begin{split}
	&\Exp{\sum_{(p_1, p_2)\in \dom{1}} \ind{p_1 \in \BallHyp{y}} \cdot \ind{p_2 \in \BallSym{p_1}\cap \BallHyp{y}} 
		\, D_\H (y,k_n-2;\Pcal \setminus \{p_1,p_2\})} \\
	&\le \Exp{\sum_{(p_1, p_2)\in \dom{1}} \ind{p_1 \in \BallHyp{y}} \cdot \ind{p_2 \in  \BallHyp{y}} 
		\, D_\H (y,k_n-2;\Pcal \setminus \{p_1,p_2\})} := \mathcal{I}_n^{(1)}(y),
\end{split}
\end{equation}

\begin{equation} \label{eq:term2}
\begin{split}
	&\Exp{\sum_{(p_1, p_2)\in \dom{2}} \ind{p_1 \in \BallHyp{y}} \cdot \ind{p_2 \in \BallSym{p_1}\cap \BallHyp{y}} 
		\, D_\H (y,k_n-2;\Pcal \setminus \{p_1,p_2\})} \\
	&\Exp{\sum_{(p_1, p_2)\in \dom{2}} \ind{p_1 \in \BallHyp{y}} \cdot \ind{p_2 \in \BallSym{p_1}} 
		\, D_\H (y,k_n-2;\Pcal \setminus \{p_1,p_2\})} := \mathcal{I}_n^{(2)}(y)
\end{split}
\end{equation}
and 
\begin{equation}\label{eq:term3}
\begin{split}
	&\Exp{\sum_{(p_1, p_2)\in \dom{3}} \ind{p_1 \in \BallHyp{y}} \cdot \ind{p_2 \in \BallSym{p_1}\cap \BallHyp{y}} 
		\, D_\H (y,k_n-2;\Pcal \setminus \{p_1,p_2\})} \\
	&\le \Exp{\sum_{(p_1, p_2)\in \dom{3}} \ind{p_1 \in \BallHyp{y}} \cdot \ind{p_2 \in \BallHyp{y}} 
		\, D_\H (y,k_n-2;\Pcal \setminus \{p_1,p_2\})} := \mathcal{I}_n^{(3)}.
\end{split}
\end{equation}
We will bound each term using the Campbell-Mecke formula~\eqref{eq:def_Campbell-Mecke} and show for $i = 1,2,3$ that for $1/2 < \alpha < 3/4$
\begin{equation}
	\lim_{n \to \infty} k_n^{4\alpha - 2} \binom{k_n}{2}^{-1} \expH^{-1} 
		\int_{\Kcal_{C}(k_n)} \mathcal{I}_n^{(i)} e^{-\alpha} \dd y = 0,
\end{equation}
and for $\alpha \ge 3/4$
\begin{equation}
	\lim_{n \to \infty} k_n \binom{k_n}{2}^{-1} \expH^{-1} 
		\int_{\Kcal_{C}(k_n)} \mathcal{I}_n^{(i)} e^{-\alpha} \dd y = 0.
\end{equation}

For the first term~\eqref{eq:term1}, we note that
\[ 
\Prob{D_\H (y)=k_n-2;\Pcal \setminus \{p_1, p_2\}} =\rho_{\HP}(y,k_n-2).
\] 
and hence $\mathcal{I}_n^{(1)}(y)$ becomes
\begin{equation} \label{eq:1sum-expansion} 
\begin{split}
	\rho_{\HP}(y,k_n-2) \int_{-I_n}^{I_n} \int_y^{R_n/2}\int_{-I_n}^{I_n} \int_{y_\omega}^{y_1}
  	\ind{p_1 \in \BallInter{y}} \, \ind{p_2 \in \BallHyp{y}} 
  	e^{-\alpha (y_1 + y_2)} \dd y_2 \dd x_2 \dd y_1 \dd x_1.
\end{split}
\end{equation}

Next, Lemma~\ref{lem:asymptotics_Omega_hyperbolic} implies that for $y'\leq R_n -y$, we have that if $(x',y') \in \BallHyp{y}$, then $|x'| < (1+ K) e^{y/2 + y'/2}$, where $K >0$ is as in Lemma~\ref{lem:asymptotics_Omega_hyperbolic}. Using these observations, we obtain: 
\begin{align*}
	&\hspace{-30pt} \Exp{\sum_{p_1, p_2\in \dom{1}} \ind{p_1 \in \BallInter{(0,y)}} \cdot \ind{p_2 \in \BallHyp{y}} \cdot 
	D_\H (y,k_n-2;\Pcal \setminus \{p_1, p_2\})} \\
	&= \rho_{\HP}(y,k_n-2)
	e^{y}\int_y^{R_n/2} e^{y_1/2}\int_{y_\omega}^{y_1} e^{y_2/2} e^{-\alpha y_2} \cdot e^{-\alpha y_1} dy_2 dy_1.
\end{align*}

Now, the double integral becomes
\begin{equation}
\begin{split}
& \int_y^{R_n/2} e^{y_1/2}\int_{y_\omega}^{y_1} e^{y_2/2} e^{-\alpha y_2} \cdot e^{-\alpha y_1} dy_2 dy_1 = \\
&  O(1) \cdot  \int_y^{R_n/2} e^{y_1/2 - \alpha y_1} \cdot 
e^{(1/2 - \alpha) y_\omega} dy_1 \\
& =O(1) \cdot e^{(1/2 - \alpha) y_\omega} \cdot \int_y^{R_n/2} e^{y_1/2 - \alpha y_1} d y_1 \\
& =O(1) \cdot e^{(1/2 - \alpha) y_\omega + (1/2 - \alpha) y} \\ 
& \ll e^{(1 - 2\alpha) y},
\end{split}
\end{equation}
since $y_\omega = y + \omega$ and $\omega \to \infty$. 
We then deduce that 
\begin{equation}
\begin{split}
&\hspace{-50pt}\Exp{\sum_{p_1, p_2\in \dom{1}} \ind{p_1 \in \BallInter{(0,y)}} \cdot \ind{p_2 \in \BallHyp{y}} \cdot 
D_\H (y,k_n-2;\Pcal \setminus \{p_1, p_2\})}\\
& \ll \rho_{\HP}(y,k_n-2) e^{(1-2\alpha)y}.
\end{split}
\end{equation}
We now integrate this with respect to $y$ and determine its contribution to~\eqref{eq:expectation_total} is 
\begin{align*} 
	&{k_n \choose 2}^{-1} \expH^{-1} \int_{\Kcal_{C}(k_n)} \rho_{\HP}(y,k_n-2) e^{(1-2\alpha)y} e^{-\alpha y} \dd y \dd x \\
	&= \bigO{k^{2\alpha - 1} k_n^{-6\alpha + 1}} = \bigO{k_n^{-4\alpha}},
\end{align*}
where we used Lemma~\ref{lem:gamma_approx} with $s = 1 - 2\alpha$.

Since $k_n^{-4\alpha} = \smallO{\min\{k_n^{-4\alpha + 2}, k_n^{-1}\}}$ for all $\alpha > 1/2$ we deduce that for $1/2 < \alpha < 3/4$
\[
	\lim_{n \to \infty} k_n^{4\alpha - 2} \binom{k_n}{2}^{-1} \expH^{-1} 
	\int_{\Kcal_{C}(k_n)} \mathcal{I}_n^{(1)}(y) e^{-\alpha y} \dd y = 0,
\]
while for $\alpha \ge 3/4$
\[
	\lim_{n \to \infty} k_n \binom{k_n}{2}^{-1} \expH^{-1} 
		\int_{\Kcal_{C}(k_n)} \mathcal{I}_n^{(1)}(y) e^{-\alpha y} \dd y = 0.
\]

We will now bound the term in~\eqref{eq:term2}. Using similar observations as for the previous term we get that $\mathcal{I}_n^{(2)}(y)$ equals
\[
	 \rho_{\HP}(y,k_n-2) \int_{-I_n}^{I_n} \int_0^{R_n/2}\int_{-I_n}^{I_n} \int_0^{y_\omega}
	 \ind{p_1 \in \BallHyp{y}} \, \ind{p_2 \in \BallSym{(0,y)}} e^{-\alpha(y_1 + y_2)} \dd y_2 
	 dd x_2 \dd y_1 \dd x_1.
\]

Now, Lemma~\ref{lem:asymptotics_Omega_hyperbolic} implies that for 
$y_2 \leq R_n -y_1$, we have that if 
$(x_2,y_2) \in \BallSym{(x_1,y_1)}$, then $x_2$ lies in an interval of length 
$Ke^{3 y/2 + 3y'/2 - R_n}$, where $K >0$ is again the constant in Lemma~\ref{lem:asymptotics_Omega_hyperbolic}. 
%For $y> R_n - y$, we will simply take $|x'| < I_n$. 
Using these observations we obtain: 
\begin{equation} \label{eq:term2_intermediate}
	\mathcal{I}_n^{(2)}(y) = \rho_{\HP}(y,k_n-2) e^{y/2}\int_0^{R_n/2} e^{y_1/2 + 3y_1/2}
	\int_0^{y_\omega} e^{3y_2/2 - R_n} e^{-\alpha y_2} \cdot e^{-\alpha y_1} \dd y_2 \dd y_1. 
\end{equation} 
Now the latter integral is
\begin{align*}
	&\hspace{-30pt} e^{-R_n}  \left(\int_0^{R_n/2} e^{(2-\alpha )y_1} \dd y_1\right) 
		\left( \int_0^{y} e^{(3/2 - \alpha) y_2} \dd y_2 \right)\\
	&= \bigO{1} e^{-R_n} \left(\begin{cases}
		e^{(1-\alpha/2)R_n} &\mbox{if } \frac{1}{2} < \alpha < 2\\
		R_n &\mbox{if } \alpha \ge 2
	\end{cases}\right)
	\left(\begin{cases}
		e^{(3/2 - \alpha)y} &\mbox{if } \frac{1}{2} < \alpha < \frac{3}{2}\\
		y &\mbox{if } \alpha \ge \frac{3}{2}
	\end{cases}\right)\\
	&= \bigO{1} \begin{cases}
		e^{-\frac{\alpha}{2}R_n} e^{(3/2 - \alpha)y} &\mbox{if } \frac{1}{2} < \alpha < \frac{3}{2}\\
		y e^{\frac{\alpha}{2} R_n} &\mbox{if } \frac{3}{2} \le \alpha < 2\\
		y R_n e^{-R_n} &\mbox{if } \alpha \ge 2.
	\end{cases} 
\end{align*}
Since $y \le R_n = \bigO{\log(n)}$ we conclude that
\[
	\mathcal{I}_n^{(2)}(y) = \bigO{1} \rho_{\HP}(y,k_n-2) \begin{cases}
		n^{-\alpha} e^{(3/2 - \alpha) y} &\mbox{if } \frac{1}{2} < \alpha < \frac{3}{2}\\
		n^{-\alpha} \log(n) &\mbox{if } \frac{3}{2} \le \alpha < 2\\
		n^{-2} \log(n)^{2} &\mbox{if } \alpha \ge 2.
	\end{cases} 	
\]

We proceed with the integration of this with respect to $y$ and determine its contribution to~\eqref{eq:expectation_total} is 
\begin{align*}
	&\bigO{1} \binom{k_n}{2}^{-2} \expH^{-1} \int_{\Kcal_{C}(k_n)} \rho_{\HP}(y,k_n-2) e^{-\alpha y} \dd y
	\cdot 
	\begin{cases}
			n^{-\alpha} k_n^{3 - 2\alpha} &\mbox{if } \frac{1}{2} < \alpha < \frac{3}{2}\\
			n^{-\alpha} \log(n) &\mbox{if } \frac{3}{2} \le \alpha < 2\\
			n^{-2} \log(n)^{2} &\mbox{if } \alpha \ge 2,
	\end{cases}\\
	&= \bigO{1} \cdot \begin{cases}
				n^{-\alpha} k_n^{1 - 2\alpha} &\mbox{if } \frac{1}{2} < \alpha < \frac{3}{2}\\
				n^{-\alpha} \log(n) k_n^{-2} &\mbox{if } \frac{3}{2} \le \alpha < 2\\
				n^{-2} \log(n)^{2} k_n^{-2} &\mbox{if } \alpha \ge 2.
		\end{cases}
\end{align*}

Now for $1/2 < \alpha < 3/4$ it holds that $4\alpha^2 - \alpha + 1 > 0$. Hence since $k_n = \bigO{n^{\frac{1}{2\alpha + 1}}}$, we have
\[
	k_n^{4\alpha - 2} n^{-\alpha} k_n^{1 - 2\alpha} = n^{-\alpha} k_n^{2\alpha - 1} = \bigO{n^{-\alpha + \frac{2\alpha - 1}{2\alpha + 1}}} = \bigO{k_n^{-\frac{4\alpha^2 - \alpha + 1}{2\alpha + 1}}} = \smallO{1},
\]
from which we deduce that
\[
	\lim_{n \to \infty} k_n^{4\alpha -2} \binom{k_n}{2}^{-1} \expH^{-1} \int_{\Kcal_{C}(k_n)} \mathcal{I}_n^{(2)}(y)
	e^{-\alpha y} \dd y = 0.
\]
For $\alpha \ge 3/4$ we have that both $n^{-\alpha} \log(n) k_n^{-1}$ and $n^{-2} \log(n)^2 k_n^{-1}$ converge to zero as $n \to \infty$ and hence in this case
\[
	\lim_{n \to \infty} k_n \binom{k_n}{2}^{-1} \expH^{-1} \int_{\Kcal_{C}(k_n)} \mathcal{I}_n^{(2)}(y)
	e^{-\alpha y} \dd y = 0.
\]

We will now consider the term in~\eqref{eq:term3}. 
Recall that $\dom{3}$ consists of all pairs $(p_1, p_2 ) \in \dom{}$ such that $R_n/2 < y_1 \leq (1-\eps)R_n \wedge (R_n-y)$ and $y_2 \leq y_\omega$ with the property that 
$p_1 \in \BallHyp{y}$ and $p_2 \in \BallSym{p_1} \cap \BallHyp{y}$.    
So, in particular, $p_2 \in (\BallHyp{p_1} \cup \BallPo{p_1}) \cap \BallHyp{y}$.

We will consider this intersection more closely. We use Lemma~\ref{lem:asymptotics_Omega_hyperbolic} to define a ball around $p_1$ that contains both 
$\BallHyp{p_1}$ and $\BallPo{p_1}$.
For $K > 0$, we define, for any point $p_1=(x_1,y_1) \in \R \times \R_+$,
\begin{equation}\label{eq:def_fatball}
	\FatBallHyp{p_1} : = \{ (x',y') \ : \  y' < R_n - y_1, \ | x_1 - x'| < (1+K) e^{\frac{1}{2} (y_1 + y')}  \}.
\end{equation}
%Note that $\Delta (r(p),r') = \frac12 e^{R/2} \theta_R (p)$.
It is an implication of Lemma~\ref{lem:asymptotics_Omega_hyperbolic}  that 
\begin{equation*} %\label{eq:ball_inclusion} 
(\BallHyp{p_1} \cup \BallPo{p_1}) \cap \Rcal([0, R_n - y_1]) \subseteq \FatBallHyp{p_1}
\end{equation*}
Therefore, any point $p_2 = (x_2,y_2) \in \BallSym{p_1} \cap \BallHyp{y}$ with 
$y_2 \leq R-y_1$ must belong to $\FatBallHyp{p_1} \cap \FatBallHyp{(0,y)}$.

We will use this in order to derive a lower bound on $y_2$ as a function of $x_1, y_1$. 
Let us suppose without loss of generality that $x_1 < 0$. 
The left boundary of $\FatBallHyp{(0,y)}$ is given by the equation 
$x^\prime = (1+K)e^{\frac{1}{2} (y + y^\prime)}$ whereas the right boundary of $\FatBallHyp{p_1}$ is given by the curve having equation $x^\prime = x_1 + (1+ K)e^{\frac{1}{2} (y_1 + y^\prime)}.$
The equation that determines the intersection point $(\hat{x},\hat{y})$ of these curves  is
\[
	x_1 + (1+K)e^{(y_1 + \hat{y})/2}= (1+K) e^{(y + \hat{y})/2}.
\]
We can solve the above for $\hat{y}$  
\begin{equation*} 
\begin{split}
|x_1| &=(1+K) e^{\hat{y}/2} \left( e^{y_1/2} + e^{y/2} \right).
\end{split}
\end{equation*}
But $y_1 > R_n/2$ and $y< (1+\eps) R_n /(2\alpha +1)$. So if $\eps$ is small enough depending on $\alpha$, we have 
$$ |x_1| =(1+K) e^{\hat{y}/2} \left( e^{y_1/2} + e^{y/2} \right) = (1+K+o(1))e^{\hat{y}/2 + y_1/2}. $$
Let $c_K$ denote the multiplicative term $1+ K+o(1)$, which appears in the above.
The above yields
\begin{equation} \label{eq:to_use_I}
\hat{y}= \left(2 \log(|x_1|e^{-y_1/2}) - \log c_K \right) \vee 0 := c(x_1,y_1). 
\end{equation}
In particular, note that $\hat{y} = 0$ if and only if $|x_1| \leq c_K e^{y_1/2}$.  
Moreover, since $p_1 \in \BallHyp{y}$ and $x_1 \leq R_n - y$, we also have that 
$|x_1| \leq e^{(y+y_1)/2} (1+o(1))$. This upper bound on $|x_1|$ together with~\eqref{eq:to_use_I}, imply that for $n$ sufficiently large, we have $\hat{y} \leq y$. This observation will be used below, where 
we integrate over $y_2$, thus ensuring that the integrals are non-zero. 

We conclude that 
\begin{equation*}\label{eq:intersex_approx}
	p^\prime \in \FatBallHyp{y}\cap \FatBallHyp{(x_1,y_1)} \Rightarrow y^\prime \ge c(x_1,y_1),
\end{equation*}

Therefore we have 
\begin{equation} \label{eq:symdiff_loc}
\begin{split} 
 \ind{p_2 \in \BallSym{p_1}\cap \BallHyp{y}} \leq \ind{y_2 \geq c(x_1,y_1), p_2 \in \FatBallHyp{(0,y)}}.
\end{split}
\end{equation}
If we integrate this over $x_2, y_2$ we get 
\begin{equation*}
\begin{split}
&\int_{-I_n}^{I_n} \int_{0}^{y_\omega}  \ind{p_2 \in \BallSym{p_1}\cap \BallHyp{y}}  
e^{-\alpha y_2} dy_2 dx_2\\
&  \leq 
\int_{-I_n}^{I_n} \int_{0}^{y_\omega}  \ind{y_2 \geq c(x_1,y_1), p_2 \in \FatBallHyp{y}}  
e^{-\alpha y_2} dy_2 dx_2 
\leq 
(1+K) \cdot e^{y/2} \int_{c(x_1,y_1)}^{y_\omega} e^{y_2/2 - \alpha y_2} dy_2 \\
&=O(1) \cdot e^{y/2 + (1/2 -\alpha) c(x_1,y_1)}.
\end{split}
\end{equation*}
Note also that 
\[
	\Exp{D_\H (y,k_n-2;\Pcal \setminus \{p_1, p_2\})} = \rho_{\HP}(y,k_n-2),
\]
uniformly over all $(p_1, p_2) \in \dom{3}$. 

So the Campbell-Mecke formula yields that $\mathcal{I}_n^{(3)}(y)$ equals: 
\begin{equation}
\begin{split}
 	&O(1) \rho_{\HP}(y,k_n-2) \, e^{y/2} \int_{-I_n}^{I_n} \int_{R_n/2}^{(R_n - y) \wedge (1-\eps)R_n}  
		\ind{p_1 \in \BallHyp{y}} e^{(1/2 -\alpha) c(x_1,y_1) - \alpha y_1} dy_1dx_1 \\
	&= O(1) \rho_{\HP}(y,k_n-2) \, e^{y/2} \int_{-I_n}^{I_n} \int_{R_n/2}^{(R_n - y) \wedge (1-\eps)R_n}  
		\ind{p_1 \in \FatBallHyp{y}} e^{(1/2 -\alpha) c(x_1,y_1) - \alpha y_1} dy_1dx_1.
\end{split}
\end{equation}
Due to the symmetry of $\FatBallHyp{y}$, the integration over $x_1$ is: 
\[
	O(1) \cdot e^{y/2} \cdot \int_0^{(1+K)e^{y/2 + y_1/2}} e^{c(x_1,y_1) (1/2 -\alpha)} dx_1
\]
We will split this integral into two parts according to the value of $c(x_1,y_1)$:
\[
\int_0^{(1+K) e^{y/2 + y_1/2}} e^{c(x_1,y_1) (1/2 -\alpha)} dx_1 = 
\int_{c_K e^{y_1/2}}^{(1+K)e^{y/2 + y_1/2}} e^{c(x_1,y_1) (1/2 -\alpha)} dx_1 + \int_0^{c_K e^{y_1/2}} dx_1.
\]
The first integral becomes: 
\begin{equation*}
\begin{split} 
&\int_{c_K e^{y_1/2}}^{(1+K)e^{y/2 + y_1/2}} e^{c(x_1,y_1) (1/2 -\alpha)} dx_1  = 
\int_{c_K e^{y_1/2}}^{(1+K)e^{y/2 + y_1/2}} e^{c(x_1,y_1)/2 (1 -2\alpha)} dx_1  \\
&= O(1)\cdot \int_{c_K e^{y_1/2}}^{(1+K)e^{y/2 + y_1/2}} x_1^{1 -2\alpha} 
e^{-\frac{y_1}{2} (1-2\alpha)} dx_1 \\
&= O(1) \cdot e^{-y_1/2 + \alpha y_1} \cdot e^{\frac{(y+y_1)}{2} 2(1-\alpha)} \\
&=O(1) \cdot e^{y_1/2 +y(1-\alpha)}.  
\end{split}
\end{equation*}
The second integral trivially gives: 
\[
	\int_0^{c_K e^{y_1/2}} dx_1 = O(1) \cdot e^{y_1/2} = O(1) \cdot e^{y_1/2 +y(1-\alpha)}.
\]
We conclude that 
\[
	e^{y/2} \cdot \int_0^{(1+K)e^{y/2 + y_1/2}} e^{c(x_1,y_1) (1/2 -\alpha)} dx_1 = 
	O(1) \cdot e^{y_1/2 +y(3/2-\alpha)}.
\]
Now, we integrate this with respect to $y_1$ and get 
\[
	e^{y(3/2 -\alpha)} \int_{R_n/2}^{R_n-y} e^{(1/2-\alpha)y_1} dy_1 =  O(1) \cdot e^{y(3/2 -\alpha)} 
	e^{(1/2 -\alpha) R_n/2} = O(1) \cdot n^{1/2 -\alpha} \cdot e^{y(3/2 - \alpha)},
\]
from which we deduce
\begin{equation} \label{eq:term3_intermediate}
	\mathcal{I}_n^{(3)}(y) = 
	O(1) \cdot n^{1/2 -\alpha} e^{y(3/2 - \alpha)} \, \rho_{\HP}(y,k_n-2). 
\end{equation}
We now apply Lemma~\ref{lem:gamma_approx} and get 
\begin{align*}
	&\hspace{-30pt}\binom{k_n}{2}^{-2} \expH^{-1} \int_{\Kcal_{C}(k_n)} \mathcal{I}_n^{(3)}(y) e^{-\alpha y} \dd y\\
	&= \bigO{1} k_n^{2\alpha - 1} n^{-1} \int_{\Kcal_{C}(k_n)} e^{(3/2 - \alpha)y} \rho_{\HP}(y,k_n-2) e^{-\alpha y} \dd y\\
	&= \bigO{1} k_n^{1 - 2\alpha} n^{-(\alpha - 1/2)}.
\end{align*}

Since for $\alpha > 1/2$, $k_n = \bigO{n^{\frac{1}{2\alpha +1}}} = \smallO{n^{1/2}}$ we have that $k_n^{4\alpha - 2} k_n^{1 - 2\alpha} n^{-(\alpha - 1/2)} = \smallO{1}$ and hence for $1/2 < \alpha < 3/4$.
\[
	\lim_{n \to \infty} k_n^{4\alpha - 2} \binom{k_n}{2}^{-2} \expH^{-1} \int_{\Kcal_{C}(k_n)} \mathcal{I}_n^{(3)}(y) e^{-\alpha y} \dd x \dd y = 0,
\]
For $\alpha \ge 3/4$ we observe that $2\alpha^2 + 2\alpha - 2 < 0$. Hence, since
\[
 	k_n n^{-(\alpha - 1/2)} k_n^{1-2\alpha} = \bigO{n^{-(\alpha - 1/2)} n^{\frac{2 - 2\alpha}{2\alpha + 1}}}
 	= \bigO{n^{- \frac{2\alpha^2 + 2\alpha - 2}{2\alpha + 1}}} = \smallO{1}.
\]
we get for $\alpha \ge 3/4$
\[
	\lim_{n \to \infty} k_n \binom{k_n}{2}^{-2} \expH^{-1} \int_{\Kcal_{C}(k_n)} \mathcal{I}_n^{(3)}(y) e^{-\alpha y} \dd x \dd y = 0.
\]



\paragraph{The sums~\eqref{eq:sum_3} and~\eqref{eq:sum_4}}
Again, we will only consider~\eqref{eq:sum_3} since the analysis for the other term is similar. Recall that in this case, we consider pairs $(p_1,p_2)$, with $p_1 = (x_1,y_1)$ satisfying 
$y_1 \geq (R_n - y) \wedge (1-\eps)R_n$, and $p_1 \in \BallHyp{y}$, $p_2 \in \BallSym{p_1} \cap \BallHyp{y}$. 
We split this into three sub-domains:  i. $y_2 \geq R_n - y$; ii. $R_n -y_1 \leq y_2 \leq R_n -y$ and iii. $y_2 < R_n - y_1$. Similar to the analysis above we define
\begin{align*}
	\dom{1} &:= \{(p_1, p_2) \ : \  p_1,p_2 \ \in \Pcal \setminus \{(0,y) \}, \ y_1 \geq (1-\eps) R_n \wedge (R_n-y), 
		\ R_n -y \leq y_2 \leq R_n \}\\
	\dom{2} &:= \{(p_1, p_2) \ : \  p_1,p_2 \ \in \Pcal \setminus \{(0,y) \}, \ y_1 \geq (1-\eps) R_n \wedge (R_n-y), 
		\ R_n -y_1 \leq y_2 \leq R_n - y \}\\
	\dom{3} &:= \{(p_1, p_2) \ : \  p_1,p_2 \ \in \Pcal \setminus \{(0,y) \}, \ y_1 \geq (1-\eps) R_n \wedge (R_n-y), 
		\ y_2 \leq R_n - y_1 \}
\end{align*}
and write, for $i = 1,2,3$,
\[
	\mathcal{I}_n^{(i)}(y) := \Exp { \sum_{(p_1,p_2)  \in \dom{i}} 
	\ind{p_1 \in \BallHyp{y}} \cdot \ind{p_2 \in \BallSym{p_1} \cap \BallHyp{y}}
	\cdot D_\H (y,k_n-2;\Pcal \setminus \{ p_1, p_2\})}.
\]


In the first case, note that $y_1 + y_2 \geq 2(R_n - y) > R_n$, since $2y < 2(1+\eps)
\frac{R_n}{2\alpha +1} < R_n$. Thus, $p_2 \in \BallHyp{p_1}$. Furthermore, 
$y_2 > R_n - y_1 + 2\ln (\pi/2)$, which implies that $p_2 \in \BallPo{p_1}$ too. 
Hence, the contribution from these pairs is zero.   

The Campbell-Mecke formula yields that: 
\begin{equation*}
\begin{split} 
\mathcal{I}_n^{(1)}(y) 
&=O(1) \int_{-I_n}^{I_n} \int_{(1-\eps) R_n \wedge (R_n-y)}^{R_n} \ind{p_1 \in \BallHyp{y}} \times\\
& \hspace{15pt}\int_{-I_n}^{I_n} \int_{R_n - y}^{R_n} 
\ind{p_2 \in \BallSym{p_1} \cap \BallHyp{y}}
  \rho_{\HP}(y,k_n-2) \cdot
e^{-\alpha( y_2 + y_1)} \dd y_2 \dd x_2 \dd y_1 \dd x_1.
\end{split}
\end{equation*}

We proceed to bound the integral: 
\begin{align*}
	&\int_{-I_n}^{I_n} \int_{(1-\eps) R_n \wedge (R_n-y)}^{R_n} \hspace{-5pt} \ind{p_1 \in \BallHyp{y}}
		\int_{-I_n}^{I_n} \int_{R_n - y}^{R_n } \ind{p_2 \in \BallSym{p_1} \cap \BallHyp{y}} 
		e^{-\alpha(y_1 + y_2)} \dd y_2 \dd x_2 \dd y_1 \dd x_1 \\
	&\leq \int_{-I_n}^{I_n} \int_{(1-\eps) R_n \wedge (R_n-y)}^{R_n} 
		\int_{-I_n}^{I_n} \int_{R_n - y}^{R_n}  e^{-\alpha(y_1 + y_2)} \dd y_2 \dd x_2 \dd y_1 \dd x_1\\
	&= \left( \int_{-I_n}^{I_n} \int_{(1-\eps) R_n \wedge (R_n-y)}^{R_n} e^{- \alpha y_1} \dd y_1 \dd x_1 \right) 
	\left(\int_{-I_n}^{I_n} \int_{R_n - y}^{R_n}  e^{-\alpha y_2} \dd y_2 \dd x_2 \right).
\end{align*}
We evaluate
$$  \int_{-I_n}^{I_n} \int_{(1-\eps) R_n \wedge (R_n-y)}^{R_n} 
e^{- \alpha y_1} dy_1 dx_1= O(1) \cdot n \cdot e^{-\alpha R_n + ((\eps R_n) \vee y))\alpha}
=O(1) \cdot n \cdot e^{-\alpha R_n + \alpha y + \alpha \eps R_n }
$$
and 
$$\int_{-I_n}^{I_n} \int_{R_n - y}^{R_n}  e^{-\alpha y_2} dy_2 dx_2 
=O(1) \cdot n \cdot e^{-\alpha R_n +\alpha y}.
$$
Also, $n \cdot e^{-\alpha R_n} = O(1) \cdot e^{(1/2 -\alpha) R_n}$, whereby we deduce that 
\begin{equation*}
\begin{split}
&\hspace{-30pt}\int_{\dom{1}} \ind{p_1 \in \BallHyp{y}} \ind{p_2 \in \BallSym{p_1} \cap \BallHyp{y}} 
 e^{-\alpha(y_1 + y_2)} dy_2 dx_2 dy_1 dx_1 \\
&=O(1) \cdot e^{(1-2\alpha)R_n + 2\alpha y + \alpha \eps R_n} =O(1) \cdot n^{2(1-2\alpha) + 2\alpha \eps} \cdot e^{2\alpha y}.
\end{split}.
\end{equation*}

With these computations we obtain
\begin{align*} 
	&\binom{k_n}{2}^{-1} \expH^{-1} \int_{\Kcal_{C}(k_n)} \mathcal{I}_n^{(1)}(y) e^{-\alpha y} \dd x \dd y\\
	&= O(1) n^{(2(1-2\alpha) + 2\alpha \eps} {k_n \choose 2}^{-1} \expH^{-1} 
		\int_{\Kcal_{C}(k_n)} e^{2\alpha y} \rho_{\HP}(y,k_n-2) e^{-\alpha y} \dd y \dd x \\ 
	&=O(1) n^{(2(1-2\alpha) + 2\alpha \eps} \, k_n^{-2} \, \expH^{-1} \, n \, k_n^{2\alpha - 1}
		=\bigO{1} n^{(2(1-2\alpha) + 2\alpha \eps} k_n^{4\alpha - 2}.
\end{align*}
Thus, for $1/2 < \alpha < 3/4$, we have 
\begin{equation*}
k_n^{4 \alpha -2} \,  n^{2(1-2\alpha) + 2\alpha \eps} \, k_n^{4\alpha -2} = 
n^{2\alpha \eps} \cdot \left( \frac{k_n^2}{n} \right)^{2(2\alpha -1)} = o(1), 
\end{equation*}
provided that $\eps = \eps (\alpha)>0$ is small enough, and hence for such $\eps$
\[
	\lim_{n \to \infty} k_n^{4\alpha - 2} \binom{k_n}{2}^{-1} \expH^{-1} \int_{\Kcal_{C}(k_n)} \mathcal{I}_n^{(1)}(y) e^{-\alpha y} \dd x \dd y = 0.
\]

When $\alpha > 3/4$ we have $2\alpha -1 > 1/2$ and we get
\begin{equation*} 
k_n \cdot n^{2(1-2\alpha) + 2\alpha \eps} \cdot k_n^{4\alpha -2}
\ll k_n \cdot n^{-1/2 + 2\alpha \eps}  \cdot k_n^{4\alpha -2} \cdot n^{1-2\alpha}  = o(1),
\end{equation*}
provided that $\eps$ is small enough, depending on $\alpha$, so that
\[
	\lim_{n \to \infty} k_n \binom{k_n}{2}^{-1} \expH^{-1} \int_{\Kcal_{C}(k_n)} \mathcal{I}_n^{(1)}(y) e^{-\alpha y} \dd x \dd y = 0.
\]


We now consider the second sub-domain $\dom{2}$. The Campbell-Mecke formula yields that: 
\begin{align*}
	\mathcal{I}_n^{(2)}(y) 
	&= \Exp { \sum_{(p_1,p_2)  \in \dom{2}} \ind{p_1 \in \BallHyp{y}} \ind{p_2 \in \BallSym{p_1} \cap \BallHyp{y}}
		D_\H (y,k_n-2;\Pcal \setminus \{ p_1\})} \\
	&= O(1) \rho_{\HP}(y,k_n-2) \cdot \int_{-I_n}^{I_n} \int_{(1-\eps) R_n \wedge (R_n-y)}^{R_n} 
		\ind{p_1 \in \BallHyp{y}} \times \\
	&\hspace{15pt} \int_{-I_n}^{I_n} \int_{R_n - y_1}^{R_n-y} \ind{p_2 \in \BallSym{p_1} \cap \BallHyp{y}}
		e^{-\alpha(y_1 + y_2)} \dd y_2 \dd x_2 \dd y_1 \dd x_1.
\end{align*}

We bound the integral as follows: 
\begin{align*}
	&\int_{-I_n}^{I_n} \int_{(1-\eps) R_n \wedge (R_n-y)}^{R_n} \hspace{-5pt} \ind{p_1 \in \BallHyp{y}}
		\int_{-I_n}^{I_n} \int_{R_n - y_1}^{R_n - y} \ind{p_2 \in \BallSym{p_1} \cap \BallHyp{y}}
		e^{-\alpha(y_1 + y_2)} \dd y_2 \dd x_2 \dd y_1 \dd x_1 \\
	&\leq \int_{-I_n}^{I_n} \int_{(1-\eps) R_n \wedge (R_n-y)}^{R_n} \ind{p_1 \in \BallHyp{y}}
		\int_{-I_n}^{I_n} \int_{R_n - y_1}^{R_n - y} 
		\ind{p_2 \in  \BallHyp{y}} e^{-\alpha(y_1 + y_2)} \dd y_2 \dd x_2 \dd y_1 \dd x_1.
\end{align*}
Now, by Lemma~\ref{} \PvdH{@Nikolaos: Which lemma are you referring to here?}
\begin{align*}
&\int_{-I_n}^{I_n} \int_{R_n - y_1}^{R_n - y} \ind{p_2 \in  \BallHyp{y}} \cdot  
e^{-\alpha y_2} dy_2 dx_2 = O(1) \cdot e^{y/2} \int_{R_n - y_1}^{R_n - y} e^{(1/2 - \alpha) y_2} dy_2 \\
&= O(1) \cdot e^{y/2 + (1/2 - \alpha) (R_n - y_1)}.
\end{align*}
We then integrate with respect to $y_1$:
\begin{align*}
	O(1) \cdot e^{y/2} \cdot 
	&\int_{-I_n}^{I_n} \int_{(1-\eps) R_n \wedge (R_n-y)}^{R_n} \ind{p_1 \in \BallHyp{y}} 
		e^{(1/2 - \alpha) (R_n - y_1)} e^{-\alpha y_1} dy_1 dx_1 \\
	&\le O(1) \cdot e^{y/2 + (1/2 -\alpha) R_n} \cdot \int_{-I_n}^{I_n} \int_{(1-\eps) R_n \wedge (R_n-y)}^{R_n} 
		e^{(\alpha -1/2) y_1} e^{-\alpha y_1} dy_1dx_1 \\
	&=O(1) \cdot e^{y/2 + (1 -\alpha) R_n -((1-\eps) R_n \wedge (R_n-y))/2} \\
	&=O(1) \cdot e^{y/2 + (1/2 -\alpha) R_n + ((\eps R_n) \vee y)/2}\\
	&=O(1) \cdot e^{y + (1/2 -\alpha) R_n + \eps R_n}
		= O(1) \cdot n^{1- 2\alpha+ \eps} \cdot e^{y}. 
\end{align*}

Therefore, the contribution of this term to~\eqref{eq:expectation_total} is:
\begin{align*} 
	&\binom{k_n}{2}^{-1} \expH^{-1} 	\int_{\Kcal_{C}(k_n)}\mathcal{I}_n^{(2)}(y) e^{-\alpha y} \dd x \dd y\\
	&= O\left( n^{1-2\alpha + \eps} \right) {k_n \choose 2}^{-1} \expH^{-1} 
		\int_{\Kcal_{C}(k_n)} \rho_{\HP}(y,k_n-2) e^{y} e^{-\alpha y} \dd x \dd y\\
	&= \bigO{1} n^{1-2\alpha + \eps},
\end{align*}
where we used Lemma~\ref{lem:gamma_approx} with $s = 1$.

For $1/2 < \alpha < 3/4$, we have
\[
	k_n^{4 \alpha -2} \cdot  n^{1-2\alpha + \eps} = n^{\eps} \left( \frac{k_n^2}{n}\right)^{2\alpha -1} = o(1),
\]
provided that $\eps = \eps (\alpha)>0$ is small enough yielding
\[
	\lim_{n \to \infty} k_n^{4\alpha - 2}  {k_n \choose 2}^{-1} \expH^{-1} 
			\int_{\Kcal_{C}(k_n)}\mathcal{I}_n^{(2)}(y) e^{-\alpha y} \dd x \dd y = 0.
\]
Similarly, for $\alpha > 3/4$ we have $2\alpha -1 > 1/2$ and we get
\[
	k_n \cdot n^{1-2\alpha + \eps} \ll n^{-1/2 + \eps}  \cdot k_n  = o(1),
\]
provided that $\eps$ is small enough, so that
\[
	\lim_{n \to \infty} k_n  {k_n \choose 2}^{-1} \expH^{-1} 
			\int_{\Kcal_{C}(k_n)} \mathcal{I}_n^{(2)}(y) e^{-\alpha y} \dd x \dd y = 0.
\]

For the third sub-domain $\dom{3}$ we shall use~\eqref{eq:symdiff_loc} which states that if 
$p_2=(x_2,y_2) \in \BallSym{p_1}\cap \BallHyp{y}$ and $y_2\leq R_n - y_1$, then 
$y_2 \geq c(x_1,y_1)$, where $c(x_1,y_1) = \left(2 \log(|x_1|e^{-y_1/2}) - \log c_K \right) \vee 0$ (cf.~\eqref{eq:to_use_I}). Moreover, $p_2 \in \FatBallHyp{p_1}$.


Again, we will use the Campbell-Mecke formula: 
\begin{align*}
	\mathcal{I}_n^{(3)}(y) &= \Exp { \sum_{(p_1,p_2)  \in \dom{3}} 
		\ind{p_1 \in \BallHyp{y}} \cdot \ind{p_2 \in \BallSym{p_1} \cap \BallHyp{y}}
		\cdot D_\H (y,k_n-2;\Pcal \setminus \{ p_1, p_2\})} \\
	&= O(1) \rho_{\HP}(y,k_n-2) \int_{-I_n}^{I_n} \int_{(1-\eps) R_n \wedge (R_n-y)}^{R_n} \ind{p_1 \in \BallHyp{y}}
		\times\\
	&\hspace{25pt} \int_{-I_n}^{I_n} \int_{0}^{R_n-y_1} 
		\ind{p_2 \in \BallSym{p_1} \cap \BallHyp{y}} 
		e^{-\alpha(y_1 + y_2)} dy_2 dx_2 dy_1 dx_1 \\
\end{align*}

The inner integral with respect to $p_2 := (x_2,y_2)$ is 
\begin{align*}
	&\hspace{-30pt} \int_{-I_n}^{I_n} \int_{0}^{R_n - y_1}  \ind{p_2 \in \BallSym{p_1}\cap \BallHyp{y}}  
		e^{-\alpha y_2} dy_2 dx_2\\
	&\leq \int_{-I_n}^{I_n} \int_{0}^{R_n-y_1}  \ind{y_2 \geq c(x_1,y_1), p_2 \in \FatBallHyp{(0,y)}}  
		e^{-\alpha y_2} dy_2 dx_2 \\
	&= O(1) e^{y/2} \int_{c(x_1,y_1)}^{R_n - y_1} e^{y_2/2 - \alpha y_2} dy_2 \\
	&= O(1) e^{y/2 + (1/2 -\alpha) c(x_1,y_1)}.
\end{align*}

Thus, we get
\begin{align*}
	&\int_{-I_n}^{I_n} \int_{(1-\eps) R_n \wedge (R_n-y)}^{R_n} \ind{p_1 \in \BallHyp{y}}
		\int_{-I_n}^{I_n} \int_{0}^{R_n-y_1} 
		\ind{p_2 \in \BallSym{p_1} \cap \BallHyp{y}} \times \\ 
	& \hspace{2cm}  e^{-\alpha(y_1 + y_2)} dy_2 dx_2 dy_1 dx_1 \\
	&\leq O(1) \int_{-I_n}^{I_n} \int_{(1-\eps) R_n \wedge (R_n-y)}^{R_n} 
		e^{y/2 + (1/2 -\alpha) c(x_1,y_1)} e^{-\alpha y_1} dy_1 dx_1. 
\end{align*}
Due to symmetry, to bound the integral it is enough to integrate this with respect to $x_1$ from 0 to $I_n$.
We will split this integral into two parts according to the value of $c(x_1,y_1)$:
\[
	\int_0^{I_n} e^{c(x_1,y_1) (1/2 -\alpha)} dx_1 = 
	\int_{c_K e^{y_1/2}}^{I_n} e^{c(x_1,y_1) (1/2 -\alpha)} dx_1 + \int_0^{c_K e^{y_1/2}} dx_1.
\]
The first integral becomes: 
\begin{align*}
	&\hspace{-30pt}\int_{c_K e^{y_1/2}}^{I_n} e^{c(x_1,y_1) (1/2 -\alpha)} dx_1  = 
 		O(1)\cdot \int_{c_K e^{y_1/2}}^{I_n} x_1^{1 -2\alpha} 
		e^{-\frac{y_1}{2} (1-2\alpha)} dx_1 \\
	&= \begin{cases}
		O(R_n) \cdot e^{-y_1/2 + \alpha y_1} \cdot e^{\frac{R_n}{2} 2(1-\alpha)} & \ \mbox{if $\alpha \leq 1$}\\
		O(1) \cdot e^{-y_1/2 + \alpha y_1 + 2(1-\alpha)y_1/2} & \ \mbox{if $\alpha > 1$}
		\end{cases}\\
	&=\begin{cases}
		O(R_n) \cdot e^{(\alpha -1/2) y_1} \cdot n^{2(1-\alpha)} & \ \mbox{if $\alpha \leq 1$}\\
		O(1) \cdot e^{y_1/2} &\ \mbox{if $\alpha > 1$}
	\end{cases}.  
\end{align*}
The second integral trivially gives: 
\[
	\int_0^{c_K e^{y_1/2}} dx_1 = O(1) \cdot e^{y_1/2}.
\]
Putting these two together we conclude that 
\[
	e^{y/2} \cdot \int_0^{I_n} e^{c(x_1,y_1) (1/2 -\alpha)} dx_1
	= O(1) \cdot e^{y_1/2 +y(3/2-\alpha)}.
\]

Now, we integrate these with respect to $y_1$:
\begin{align}
	&n^{2(1-\alpha)} \cdot \int_{(1-\eps) R_n \wedge (R_n-y)}^{R_n} e^{(\alpha -1/2)y_1 - \alpha y_1} dy_1 
		=O(1) \cdot n^{2(1-\alpha)} \cdot e^{-R_n/2 + \eps R_n/2  + y/2}  \\
	&= O(1) \cdot n^{1-2\alpha + \eps} \cdot e^{y/2}.
\end{align}
%Also,
%\begin{equation}
%\begin{split}
%&\int_{(1-\eps) R_n \wedge (R_n-y)}^{R_n} e^{(1/2 - \alpha) y_1} dy_1 
%= O(1) \cdot e^{(1/2-\alpha) R_n + y(\alpha -1/2) + \eps R_n (\alpha -1/2)} \\
%&= O(1) \cdot n^{1-2\alpha +\eps(2\alpha -1)} \cdot e^{y(\alpha -1/2)}. 
%\end{split}
%\end{equation}
Therefore, we conclude that
\[
	\mathcal{I}_n^{(3)}(y) = \bigO{R_n} n^{1-2\alpha +\eps(2\alpha -1)} \, e^{y/2} \rho_{\HP}(y,k_n-2)
\]
and hence, using again Lemma~\ref{lem:gamma_approx},
\begin{align*}
	&\hspace{-30pt}\binom{k_n}{2}^{-1} \expH^{-1} \int_{\Kcal_{C}(k_n)} \mathcal{I}_n^{(3)}(y) e^{-\alpha y} \dd x \dd y\\
	&= \bigO{R_n} n^{1-2\alpha +\eps(2\alpha -1)} k_n^{-2} \expH^{-1} \int_{\Kcal_{C}(k_n)} e^{y/2} 
		\rho_{\HP}(y,k_n-2) e^{-\alpha y} \dd x \dd y\\
	&= \bigO{R_n} n^{1-2\alpha +\eps(2\alpha -1)}.
\end{align*}

It follows that for $\eps = \eps(\alpha)$ small enough
\[
	k_n^{4\alpha - 2} R_n n^{1-2\alpha +\eps(2\alpha -1)}
	= R_n n^{\eps(2\alpha -1)} \left(\frac{k_n^2}{n}\right)^{2\alpha - 1} = \smallO{1}
\]
and hence for $\alpha > 1/2$,
\[
	\lim_{n \to \infty} k_n^{4\alpha - 2} \binom{k_n}{2}^{-1} \expH^{-1} \int_{\Kcal_{C}(k_n)} \mathcal{I}_n^{(3)}(y) e^{-\alpha y} \dd x \dd y = 0.
\]
Since $4\alpha - 2 \ge 1$ when $\alpha \ge 3/4$ it immediately follows that
\[
	\lim_{n \to \infty} k_n \binom{k_n}{2}^{-1} \expH^{-1} \int_{\Kcal_{C}(k_n)} \mathcal{I}_n^{(3)}(y) e^{-\alpha y} \dd x \dd y = 0.
\]

\paragraph{The sum of~\eqref{eq:sum_5}}

We will give an upper bound for
\[
	\Exp{ \sum_{p_1,p_2 \in\Pcal \setminus \{(0,y)\} \atop y(p_1) \geq K} 
	\hspace{-10pt} \ind{p_1\in \BallHyp{y}\setminus \BallPo{y}} \ind{p_2\in \BallHyp{y}\cap \BallPo{y}} 
	\, D_\H (y,k_n-2;\Pcal \setminus \{ p_1,p_2\})}.
\]
Let us set $p=(0,y)$. Recall that $\BallSym{y}\cap \Rcal([R_n - y + 2 \log\left(\frac{\pi}{2}\right),R_n]) = \emptyset$. Thus, the summand in the above sum is equal to 0, when 
$y_1 > R_n - y + 2 \log (\pi/2)$. 

Recall the definition of the extended ball $\FatBallHyp{p}$ around $p$~\eqref{eq:def_fatball} that contains both $\BallHyp{p}$ and $\BallPo{p}$ 
\[
	\FatBallHyp{p} : = \{ p^\prime : y^\prime < R_n - y, \ |x^\prime| < (1+K) e^{\frac{1}{2} (y + y^\prime)}  \},
\]
and that we have $\Exp{D_\H (y,k_n-2;\Pcal \setminus \{ p_1,p_2\})} = \rho_{\HP}(y,k_n-2)$.

Observe that,
\begin{equation*} %\label{eq:ball_inclusion} 
\BallHyp{p} \cap \Rcal ([0,r(p))) \subseteq \FatBallHyp{p}
\end{equation*}
and 
\begin{equation*} %\label{eq:ball_inclusion_lower}
\BallHyp{p} \cap \Rcal ([r(p),R_n]) = \Rcal ([r(p),R_n]).
\end{equation*}
We thus conclude that 
\begin{equation} \label{eq:ball_inclusions}
\BallHyp{p} \subseteq \FatBallHyp{p} \cup \Rcal([r(p),R_n]).
\end{equation}
Hence, if we set 
\[
h_y(p_1, \Pcal) := \ind{p_1 \in \BallHyp{p}\setminus \BallPo{p}} \cdot    
\left( \mu_{\alpha,\nu} \left( \FatBallHyp{p_1} \cap \FatBallHyp{p} \right)
+ \mu_{\alpha,\nu} \left( \Rcal([R_n-y,R_n]) \right) \right),
\]
then 
\begin{align*}
&\ind{p_1\in \BallHyp{p}\setminus \BallPo{p}} \cdot \Exp{ \left(\sum_{p_2 \in \Pcal \setminus 
\{p,p_1\}} \ind{p_2 \in \BallHyp{p} \cap \BallPo{p_1}}\right) \cdot 
D_\H (y,k_n-2;\Pcal \setminus \{ p_1, p_2\})
} \\
&=O(1)\cdot
\ind{p_1\in \BallHyp{p}\setminus \BallPo{p}} \cdot \mu_{\alpha,\nu} (\BallHyp{p}\cap \BallHyp{p_1}) \rho_{\HP}(y,k_n-2) \\
& \leq O(1) \cdot  h_y (p_1, \Pcal) \rho_{\HP}(y,k_n-2) . 
\end{align*}
To calculate the expectation of the above function we need to approximate the 
intersection of the two balls $\FatBallHyp{p}$ and $\FatBallHyp{p_1}$, 
where $p_1= (x_1,y_1)$. 
Let us assume without loss of generality that $x_1 > 0$. 
The right boundary of $\FatBallHyp{p}$ is given by the equation 
$x = x(y_1) = (1+K)e^{\frac{1}{2} (y + y_1)}$ whereas the left boundary of $\FatBallHyp{p_1}$ is given by the curve $x = x(y_1)= x_1 - (1+ K)e^{\frac{1}{2} (y + y_1)}.$ 

The equation that determines the intersecting point of the two curves is
\[
	x_1 - (1+K)e^{(\hat{y} + y_1)/2}= (1+K) e^{(\hat{y} + y)/2},
\]
where $\hat{y}$ is the $y$-coordinate of the intersecting point. 
We can solve the above for $\hat{y}$  
\begin{equation*} 
\begin{split}
x_1 &=(1+K) e^{\hat{y}/2} \left( e^{y/2} + e^{y_1/2} \right).
\end{split}
\end{equation*}
But since $p_1=(x_1,y_1)  \in \BallSym{p}$, we also have $x_1 > e^{\frac{y + y_1}{2}}$. Therefore, 
\begin{equation*}
\begin{split}
 e^{\hat{y}/2}& > \frac{1}{1+K}~\frac{e^{\frac{y + y_1}{2}}}{ e^{y/2}+ e^{y_1/2}} \geq 
\frac{1}{2(1+K)}~\frac{e^{\frac{y_1 + y}{2}}}{ e^{\max \{y, y_1\} /2}} 
> \frac{1}{2(1 + K)} ~ e^{\min\{y, y_1\}/2}. 
 \end{split}
\end{equation*}
The above yields
\begin{equation} \label{eq:to_use_I}
\hat{y} > \min\{y, y_1\} - 2\log(2(1+K)) := c(y_1, y). 
\end{equation}
which, in turn, implies the following 
\begin{equation}\label{eq:intersex_approx}
	p \in \FatBallHyp{(0,y)}\cap \FatBallHyp{p_1} \Rightarrow y(p) \ge c(y_1,y).
\end{equation}
We thus conclude that 
\[ 
	\BallHyp{p_1} \cap \BallHyp{p} \subseteq \left(\FatBallHyp{p} \cap \Rcal([c(y_1,y), R_n])\right)
	\, \cup \, \Rcal ([R_n - y,R_n]),
\]
which in turn implies that
\[
	\mu_{\alpha,\nu} \left( \FatBallHyp{p_1} \cap \BallHyp{p} \right) \leq 
	\mu_{\alpha,\nu}\left( \FatBallHyp{p} \cap  \Rcal([c(y_1,y), R_n]\right) + 
	\mu_{\alpha,\nu} (\Rcal ([R_n - y, R_n]) ).
\]
Therefore, 
\begin{align*} 
	h_y(p_1, \Pcal) &\leq \ind{p_1 \in \BallHyp{p}\setminus \BallPo{p}} 
    	\mu_{\alpha,\nu}  \left( \FatBallHyp{p} \cap  \Rcal([c(y_1,y), R_n])\right)
        \\
	&\hspace{10pt}+ \ind{p_1 \in \BallHyp{p}\setminus \BallPo{p}}
    	\mu_{\alpha,\nu}  \left( \Rcal ([R_n - y, R_n]) \right).
\end{align*}



Now, the Campbell-Mecke formula~\eqref{eq:def_Campbell-Mecke} gives
\begin{align*}
	&\Exp{ \sum_{p_1,p_2 \in\Pcal \setminus \{(0,y)\} \atop y(p_1) \geq K} 
		\hspace{-10pt} \ind{p_1\in \BallHyp{y}\setminus \BallPo{y}} \ind{p_2\in \BallHyp{y}\cap \BallPo{y}} 
		\, D_\H (y,k_n-2;\Pcal \setminus \{ p_1,p_2\})}\\
	&\le \Exp{\left( \sum_{p_1 \in \Pcal} 
		h_y (p_1, \Pcal \setminus \{ p_1 \})\right)} \\
	&=\frac{\nu \alpha}{\pi} \int_{\Rcal_n} \Exp{h_y(p_1, \Pcal \setminus \{p_1\})}
		e^{-\alpha y_1} \dd x_1 \dd y_1\\
	&\le \frac{\nu \alpha}{\pi} \int_{\Rcal_n} \ind{p_1 \in \BallHyp{p}\setminus \BallPo{p}} 
	    	\mu_{\alpha,\nu}  \left( \FatBallHyp{p} \cap  \Rcal([c(y_1,y), R_n])\right)
	    	e^{-\alpha y_1} \dd x_1 \dd y_1 \numberthis \label{eq:h_upper_bound_1}\\
	&\hspace{10pt}+ \frac{\nu \alpha}{\pi} \int_{\Rcal_n} \ind{p_1 \in \BallHyp{p}\setminus \BallPo{p}}
	    	\mu_{\alpha,\nu}  \left( \Rcal ([R_n - y, R_n]) \right) e^{-\alpha y_1} \dd x_1 \dd y_1.
	    	\numberthis \label{eq:h_upper_bound_2}
\end{align*}

Recall that $(\BallSym{(0,y)})\cap \Rcal([R_n - y + 2 \log\left(\frac{\pi}{2}\right),R_n]) = \emptyset$. 
We will first calculate the measures $\mu_{\alpha, \nu}$ appearing in~\eqref{eq:h_upper_bound_1} and~\eqref{eq:h_upper_bound_2}. The first one is:
\begin{align*}
	\mu_{\alpha,\nu}\left( \FatBallHyp{y} \cap  \Rcal([c(y_1,y), R_n])\right) 
	&\leq (1+ K) \frac{\nu \alpha}{\pi} \cdot e^{y/2}  \int_{c(y_1,y)}^{R_n} e^{-(\alpha - \frac{1}{2}) y'} \, dy' \\
	&=  \bigO{e^{\frac{y}{2} - (\alpha-\frac{1}{2}) \min \{y,y_1\}}}.
\end{align*}

The second term is: 
\begin{align*}
	\mu_{\alpha,\nu} \left( \Rcal([R_n - y,R_n]) \right) 
    &= \frac{\nu \alpha}{\pi} \int_{R_n - y}^{R_n} \pi e^{\frac{R_n}{2}} e^{-\alpha y'} \, dy' 
    	= \bigO{e^{\frac{R_n}{2}} e^{-\alpha (R_n-y)}} = \bigO{e^{\alpha y - (\alpha - \frac{1}{2})R_n}}. 
\end{align*}

Using these, we get
\begin{align} 
	&\int_{\Rcal_n ([0, R_n - y_n + 2 \ln \frac{\pi}{2}])} \Exp{h_y(p_1, \Pcal \setminus \{p_1\})} 
    e^{-\alpha y_1} \, dx_1 \, dy_1 \notag \\
	&= \bigO{1} \int_{\Rcal_n ([0, R_n - y+ 2 \ln \frac{\pi}{2}])} \ind{p_1 \in \BallSym{p}} 
		e^{\frac{y}{2} - (\alpha - \frac{1}{2}) \min \{y,y_1\} - \alpha y_1} \, dx_1 \, dy_1
		\label{eq:Mecke_sum_1}\\ 
	&\hspace{10pt}+ \bigO{1} \int_{\Rcal_n ([0, R_n - y + 2 \ln \frac{\pi}{2}])} 
    	\ind{p_1 \in \BallHyp{(0,y)}} 
    	e^{\alpha y - (\alpha - \frac{1}{2})R_n - \alpha y_1} \, dx_1 \, dy_1.\label{eq:Mecke_sum_2}
\end{align}
Now, Lemma~\ref{lem:asymptotics_Omega_hyperbolic} implies that 
for any $y \in [0, R_n - y_n + 2 \ln \frac{\pi}{2}]$, we have 
\[ 
	\int_{-I_n}^{I_n} \ind{p_1 \in \BallSym{y}} \, dx_1 \leq 2 K e^{\frac{3}{2} (y_1 + y) - R_n}.
\]

Therefore, \eqref{eq:Mecke_sum_1} is 
\begin{align*}
	&\hspace{-20pt}O(1) \cdot e^{2 y - R_n} \int_{0}^{R_n - y + 2 \ln \frac{\pi}{2}} 
    	e^{\frac{3y_1}{2} - (\alpha - \frac{1}{2})\min\{y_1,y\} - \alpha y_1} \, dy_1 \\
 	&=  O(1) \cdot e^{2 y - R_n} \left( \int_{0}^{y} e^{\frac{3y_1}{2} - (2\alpha - \frac{1}{2})y_1} \, dy_1 
 		+ e^{-(\alpha-\frac{1}{2}) y}\int_{y}^{R_n - y + 2 \ln \frac{\pi}{2}} e^{(\frac{3}{2} - \alpha) y_1} \, dy_1 \right)\\
  	&= O(1) \left( 
	\begin{cases}
	e^{(4-2\alpha) y - R_n}, & \mbox{if $\alpha < 2$} \\
	R_n \cdot e^{2y - R_n}, & \mbox{if $\alpha \geq 2$}
	\end{cases}
	+
	\begin{cases}
	e^{-(\alpha - \frac{1}{2})R_n +y}, & \mbox{if $\alpha < 3/2$} \\
	R_n \cdot  e^{2(2-\alpha)y - R_n}, & \mbox{if $\alpha \geq 3/2$}
	\end{cases}\right).
\end{align*}

Similarly, for~\eqref{eq:Mecke_sum_2} we have
\begin{align*}
	&\hspace{-30pt}\int_{\Rcal ([0, R_n - y + 2 \ln \frac{\pi}{2}])} \ind{p_1 \in \BallSym{(0,y)}} e^{\alpha y - (\alpha - \frac{1}{2})R_n - \alpha y_1} \, dx_1 \, dy_1\\
	&= e^{\frac{3y}{2} - R_n + \alpha y - (\alpha - \frac{1}{2})R_n} 
    	\cdot \int_{0}^{R_n - y + 2 \ln \frac{\pi}{2}} e^{\frac{3y_1}{2}-\alpha y_1} \, dy_1\\
	&= O(1)\cdot 
	\begin{cases} 
	e^{\frac{3y}{2} - R_n + \alpha y - (\alpha - \frac{1}{2})R_n + (\frac{3}{2} - \alpha)(R_n-y)} 
	, & \mbox{if $\alpha < 3/2$} \\ 
	R_n \cdot e^{(\frac{3}{2} +\alpha)y -  (\alpha + \frac{1}{2})R_n}, & \mbox{if 
	$\alpha \geq 3/2$}
	\end{cases}	\\
	&= O(1) \cdot 
	\begin{cases}
	  e^{-(2\alpha-1) R_n + 2 \alpha y}, & \mbox{if $\alpha < 3/2$} \\
	  R_n \cdot e^{(\frac{3}{2} +\alpha)y -  (\alpha + \frac{1}{2})R_n}, & \mbox{if 
	$\alpha \geq 3/2$}
	\end{cases}.
\end{align*}

We thus conclude, using $2(2 - \alpha)y \le y$ for $\alpha > 3/2$, that 
\begin{equation} \label{eq:upper_bound_faulty_edges} 
\Exp{\left( \sum_{p_1 \in \Pcal \setminus\{p\}} 
		h_y (p_1, \Pcal \setminus \{ p_1 \})\right)} \le \bigO{1} \cdot 
\left( \mathcal{I}_n^{(1)}(y) + \mathcal{I}_n^{(2)}(y) + \mathcal{I}_n^{(3)}(y) \right),
\end{equation}
where 
\begin{align*}
 \mathcal{I}_n^{(1)}(y) &= \begin{cases}
	e^{(4-2\alpha) y - R_n}, & \mbox{if $\alpha < 2$} \\
	R_n \cdot e^{2y - R_n}, & \mbox{if $\alpha \geq 2$}
	\end{cases},  \\
	\mathcal{I}_n^{(2)}(y) &= 
	\begin{cases}
	e^{-(\alpha - \frac{1}{2})R_n +y}, & \mbox{if $\alpha < 3/2$} \\
	R_n \cdot  e^{y - R_n}, & \mbox{if $\alpha \geq 3/2$}
	\end{cases}\\
	\mathcal{I}_n^{(3)}(y) &= 
\begin{cases}
	  e^{-(2\alpha-1) R_n + 2 \alpha y}, & \mbox{if $\alpha < 3/2$} \\
	  R_n \cdot e^{(\frac{3}{2} +\alpha)y -  (\alpha + \frac{1}{2})R_n}, & \mbox{if 
	$\alpha \geq 3/2$}
	\end{cases}.
\end{align*}

We now need to calculate:
\begin{align*}
&{k_n\choose 2}^{-1}\cdot \expH^{-1} \int_{\Kcal_{C}(k_n)} \Exp{\left( \sum_{p_1 \in \Pcal} h_y (p_1, \Pcal \setminus \{ p_1 \})\right)} \cdot 
 \rho_{\HP}(y,k_n-2) e^{-\alpha y} \dd y \dd x.
\end{align*}
Firstly, note that as $\expH = \Theta (1) \cdot n \cdot k_n^{-(2\alpha +1)}$, we have 
$$ {k_n\choose 2}^{-1}\cdot \expH^{-1} = O(1) \cdot \frac{k_n^{2\alpha-1}}{n}.$$
Also, $\Exp{\left( \sum_{p_1 \in \Pcal} h_y (p_1, \Pcal \setminus \{ p_1 \})\right)}$ is given 
as the sum of $\mathcal{I}_n^{(1)}(y), \mathcal{I}_n^{(2}(y)$ and $\mathcal{I}_n^{(3)}(y)$ (cf.~\eqref{eq:upper_bound_faulty_edges}). Setting 
\[
	J_3 = \frac{k_n^{2\alpha-1}}{n}\cdot  \left(M_1+ M_2 + M_3 \right)
\]
with
\[
	M_i = \int_{\Kcal_{C}(k_n)} \mathcal{I}_n^{(i)}(y) \rho_{\HP}(y,k_n-1) e^{-\alpha y} \dd y \dd x
\]
it follows that 
\begin{align*}
&{k_n\choose 2}^{-1} \expH^{-1} \int_{\Kcal_{C}(k_n)} \Exp{\left( \sum_{p_1 \in \Pcal \setminus \{(0,y)\}} h_y (p_1, \Pcal \setminus \{ p_1 \})\right)} \rho_{\HP}(y,k_n-1) e^{-\alpha y} \dd y \dd x \\
&= O(1) \cdot J_3
\end{align*}

Computing each of the integral separately we obtain, using Lemma~\ref{lem:gamma_approx} and the fact that $n = \nu e^{R_n/2}$,
\begin{align*} 
M_1:= \int_{\Kcal_{C}(k_n)} \mathcal{I}_n^{(1)}(y) \rho_{\HP}(y,k_n-1) e^{-\alpha y} \dd y \dd x
&= O(1) \cdot 
\begin{cases} 
	\frac{k_n^{7-6\alpha}}{n}, & \mbox{if $\alpha <2$} \\
	R_n^2 \frac{k_n^{3-2\alpha}}{n}, & \mbox{if $\alpha \geq 2$}
\end{cases}. 
\end{align*}
\begin{align*} 
M_2:= \int_{\Kcal_{C}(k_n)} \mathcal{I}_n^{(2)}(y) \rho_{\HP}(y,k_n-1) e^{-\alpha y} dy
&= O(1) \cdot 
\begin{cases}
e^{-(\alpha - 1)R_n} k_n^{-2\alpha+1}, & \mbox{if $\alpha < 3/2$} \\
R_n   \frac{k_n^{7-6\alpha }}{n}, & \mbox{if $\alpha \geq 3/2$}
\end{cases} 
\\
&=\begin{cases}
\frac{k_n^{1-2\alpha}}{n^{2(\alpha-1)}}, & \mbox{if $\alpha < 3/2$} \\
R_n   \frac{k_n^{7-6\alpha}}{n}, & \mbox{if $\alpha \geq 3/2$}
\end{cases}
\end{align*}
and finally 
\begin{align*} 
M_3:= \int_{\Kcal_{C}(k_n)} \mathcal{I}_n^{(3)}(y) \rho_{\HP}(y,k_n-1) e^{-\alpha y} dy
&= O(1) \cdot 
\begin{cases} 
e^{-(2\alpha - 3/2) R_n} k_n^{2\alpha - 1}, & \mbox{if $\alpha <3/2$} \\ 
R_n  e^{-\alpha R_n} k_n^2, &\mbox{if $\alpha \geq 3/2$}
\end{cases} \\
&=O(1) \cdot 
\begin{cases} 
\frac{k_n^{2\alpha - 1}}{n^{4\alpha -3}}, & \mbox{if $\alpha <3/2$} \\ 
R_n \cdot \frac{k_n^2}{n^{2\alpha}}, &\mbox{if $\alpha \geq 3/2$}
\end{cases}  .
\end{align*}

Now, we will consider the two cases according to the value of $\alpha$. 
Assume first that $1/2 < \alpha < 3/4$. 
In this case, we want to show that 
\begin{equation} \label{eq:int3_to_prove_I}
\lim_{n \to \infty} k_n^{4\alpha -2} \cdot J_3 = 0. 
\end{equation}
Using the above expression for $J_3$, we have 
\begin{align*} 
 k_n^{4\alpha -2} \cdot J_3 &= O(1) \cdot  
 \frac{k_n^{6\alpha -3}}{n} \cdot 
\left( 
\frac{k_n^{7-6\alpha}}{n} + \frac{k_n^{2(1-\alpha)}}{n^{2(\alpha-1)}} 
+\frac{k_n^{2\alpha-1}}{n^{4\alpha - 3}}.
\right) 
\end{align*}
We wish to show that each one of the above three terms is $o(1)$ for $k_n = O(n^{\frac{1}{2\alpha +1}})$. 
For the first one we have 
\[ 
\frac{k_n^{6\alpha -3}}{n} \cdot \frac{k_n^{7-6\alpha}}{n} = \frac{k_n^{4}}{n^2} = O(1) \cdot n^{\frac{4}{2\alpha +1} -2} = \smallO{1}, 
\]
since $4 < 4\alpha + 2$ when $1/2 < \alpha$.
The second one yields: 
\[
	\frac{k_n^{6\alpha -3}}{n} \cdot  \frac{k_n^{-2\alpha+1}}{n^{2(\alpha-1)}} =\frac{k_n^{4\alpha -2}}{n^{2\alpha -1}} =O(1) \frac{n^{\frac{4\alpha -2}{2\alpha+1}}}{n^{2\alpha -1}}.
\]
We need to show that $\frac{4\alpha -2}{2\alpha+1}< 2\alpha -1$. Indeed, rearranging this 
yields, $4\alpha -2 < 4\alpha^2 -1$, which is equivalent to $0< 4\alpha^2 - 4\alpha +1=(2\alpha- 1)^2$. This holds for all $\alpha >1/2$.  
 
Finally, the third one yields: 
\[
	\frac{k_n^{6\alpha -3}}{n} \cdot \frac{k_n^{2\alpha -1}}{n^{4\alpha - 3}}  
	= \frac{k_n^{8\alpha -4}}{n^{2(2\alpha -1)}} = \frac{k_n^{4(2\alpha -1)}}{n^{2(2\alpha-1)}}.
\]
But $k_n^4 \leq O(1)\cdot n^{\frac{4}{2\alpha+1}} = o(n^2)$, as $2\alpha +1 >2$.
 
For $\alpha \ge 3/4$, we would like to show that 
\begin{equation} \label{eq:int3_to_prove_II}
\lim_{n \to \infty} k_n \cdot J_3 = 0. 
\end{equation}
Firstly, if $3/4 < \alpha < 3/2$ we have, 
\begin{align*} 
 k_n \cdot J_3 &= O(1) \cdot  
 \frac{k_n^{2\alpha}}{n} \cdot 
\left( 
\frac{k_n^{7-6\alpha}}{n} + \frac{k_n^{-2\alpha +1}}{n^{2(\alpha-1)}} 
+\frac{k_n^{2\alpha -1}}{n^{4\alpha - 3}}.
\right) 
\end{align*}
As above we will deal with the three term of this. 
For the first one we have 
\[
	\frac{k_n^{2\alpha}}{n} \frac{k_n^{7-6\alpha}}{n}  = \frac{k_n^{7-4\alpha }}{n^2} \le   
	\frac{k_n^{4}}{n^2} =o(1). 
\]
The second one yields: 
\[
	\frac{k_n^{2\alpha}}{n} \frac{k_n^{-2\alpha +1}}{n^{2(\alpha-1)}} =
	\frac{k_n}{n^{2\alpha-1}} \le \frac{k_n}{n^{1/2}} = O(1) 
	\frac{n^{\frac{1}{2\alpha+1}}}{n^{1/2}} =o(1).
\]
Finally, the third one yields: 
\[
	\frac{k_n^{2\alpha}}{n} \cdot \frac{k_n^{2\alpha -1}}{n^{4\alpha - 3}}
	= \frac{k_n^{4\alpha -1}}{n^{2(2\alpha -1)}}=
O(1) \frac{n^{\frac{4\alpha-1}{2\alpha+1}}}{n^{2(2\alpha -1)}}.
\]
We need to show that $\frac{4\alpha-1}{2\alpha+1}< 2(2\alpha -1)$, which is 
equivalent to $8\alpha^2 - 4 \alpha -1>0$; this is indeed the case for any $\alpha \geq 3/4$. 

 
For $3/2 \leq \alpha <2$, it is only $M_2$ and $M_3$ that change values. In particular, for any $\alpha \geq 3/2$ we have 
\[
	\frac{k_n}{n} \cdot M_2 =O(1)\cdot R_n \cdot \frac{k_n^{2\alpha}}{n} \cdot 
\frac{k_n^{7-6\alpha}}{n} =o(1),
\]
as above. Also, 
\[
	\frac{k_n}{n} \cdot M_3 = O(1)\cdot
	R_n \cdot \frac{k_n}{n} \cdot \frac{k_n^{2}}{n^{2\alpha}}
	= R_n\cdot  \frac{k_n^{3}}{n^{2\alpha +1}} = o(1),
\]
since $k_n = o(n^{1/2})$ (and, therefore, $k_n^3 = o(n^{3/2})$) but $2\alpha +1 >2$. 

If $\alpha \geq 2$ too, then $M_1$ changes value and we have 
\[
	\frac{k_n}{n} \cdot M_1 =O(1) \cdot R_n^2 \cdot 
\frac{k_n}{n}  \cdot \frac{k_n^{3-2\alpha}}{n} = \frac{k_n^{4 - 2\alpha}}{n^2}= o(1),
\] 
since $\alpha \geq 2$. 



\paragraph{The sum of~\eqref{eq:sum_6}}
Now, we will first give an upper  bound on the term
\begin{equation*}
\Exp { \sum_{p_1,p_2 \in\Pcal \setminus \{(0,y)\} \atop y_1 < K}
\ind{p_1\in \BallSym{y}} \ind{p_2\in \BallHyp{y}\cap \BallPo{y}}}.
\end{equation*}
Using the Campbell-Mecke formula~\eqref{eq:def_Campbell-Mecke}, we write 
\begin{align*}
&\Exp { \sum_{p_1,p_2 \in\Pcal \setminus \{(0,y)\}, \ y_1 < K}
\ind{p_1\in \BallSym{y}} \ind{p_2\in \BallHyp{y}\cap \BallPo{y}}} =\\
&\leq \int_{-I_n}^{I_n} \int_0^K \int_{-I_n}^{I_n} \int_0^{R_n} 
\ind{p_1 \in \BallSym{y}} 
 \ind{p_2 \in \BallHyp{y}\cap \BallPo{y}} e^{-\alpha y_2} e^{-\alpha y_1} 
 dx_2 dy_2 dx_1 dy_1 \\
 &\leq  \mu_{\alpha, \nu} (\BallHyp{y})\cdot
 \int_{-I_n}^{I_n} \int_0^K \ind{p_1 \in \BallSym{y}} 
e^{-\alpha y_1}  dx_1 dy_1.
\end{align*}
By Lemma~\ref{lem:average_degree_hyperbolic} and a concentration argument,  the first factor is 
\[
	\mu_{\alpha, \nu} (\BallHyp{y}) =O(1) e^{y/2}.
\]
We bound the second factor using Lemma~\ref{lem:asymptotics_Omega_hyperbolic}. 
In particular,~\eqref{eq:asymp1} implies that 
if $(x_1,y_1) \in \BallSym{(0,y)}$, then because $y_1 < K$
\[
 |x_1 - e^{(y+y_1)/2} |\leq e^{(y+y_1)/2} \cdot K e^{y+y_1- R_n} = O(1) 
e^{(y+y_1)/2} \cdot e^{y- R_n}.
\]
Therefore, 
\[
	\int_{-I_n}^{I_n} \int_0^K \ind{(x_1,y_1) \in \BallSym{(0,y)}} 
	e^{-\alpha y_1}  dx_1 dy_1 = O(1) \cdot e^{y-R_n} 
	\cdot \int_0^K e^{(y+y_1)/2} 
	e^{-\alpha y_1}   dy_1 = O(1)\cdot e^{3y/2 - R_n}, 
\]
and hence
\begin{align*}
	\Exp { \sum_{p_1,p_2 \in\Pcal \setminus \{(0,y)\} \atop y_1 < K}
	\ind{p_1\in \BallSym{y}} \ind{p_2\in \BallHyp{y}\cap \BallPo{y}}} 
	= O(1) \cdot e^{2y - R_n}.
\end{align*}
Now, we integrate this over $y$: 
\begin{align*}
	&e^{-R_n} \int_{\Kcal_{C}(k_n)} e^{2y -\alpha y} dy dx = O(1) n e^{-R_n} \int_{I_\eps (k_n)} e^{2y -\alpha y} dy \\
	&= O(1) \cdot n^{-1} \int_{I_\eps (k_n)} e^{2y -\alpha y} dy \\
	&= O(1) \cdot n^{-1} \cdot 
		\begin{cases}
		k_n^{2(2-\alpha) (1+\eps)}, & \mbox{if $\alpha < 2$} \\
		\log k_n, & \mbox{if $\alpha =2$} \\
		1, & \mbox{if $\alpha >2$}
		\end{cases}.
\end{align*}
We deduce that
\[
	\binom{k_n}{2}^{-1} \expH^{-1} \Exp { \sum_{p_1,p_2 \in\Pcal \setminus \{(0,y)\} \atop y_1 < K}
	\ind{p_1\in \BallSym{y}} \ind{p_2\in \BallHyp{y}\cap \BallPo{y}}}
	= O(1) \cdot n^{-1} \cdot k_n^{2\alpha -1}.
\]

To finish the argument assume first that $1/2 <\alpha \leq 3/4$. In this case, we will consider
\[
	k_n^{4\alpha -2} \cdot n^{-2} \cdot k_n^{2\alpha -1 + 4 - 2\alpha} = 
	n^{-2} \cdot k_n^{4\alpha +1}.
\]
But, $\alpha \leq 3/4$, we have $4\alpha +1 \leq 4$ and $k_n = o(n^{1/2})$, whereby $k_n^{4\alpha +1} = o(n^{2})$. 

Now, suppose that $3/4 < \alpha < 2$. Here, we will consider 
\[
	k_n \cdot n^{-2} \cdot k_n^{2\alpha -1 + 4 - 2\alpha} =\frac{k_n^2}{n^2} = o(1).
\] 
When $\alpha \geq 2$, we will bound $\log k_n$ and 1 by $k_n$ and we will consider 
\[
	k_n \cdot n^{-2} \cdot k_n^{2\alpha -1 +1}= n^{-2} k_n^{2\alpha +1}.
\] 
But $k_n = O(1)\cdot n^{\frac{1}{2\alpha +1}}$, whereby $k_n^{2\alpha +1} =O(n)$ and 
the above term is therefore $o(1)$.

\end{proof}

\subsection{Coupling $G_{\H,n}$ to $G_{\widetilde{\H},n}$}\label{ssec:coupling_H_HP}

Now that we have established the equivalence of the local clustering function between the Poisson hyperbolic graph $G_{\HP,n}$ and $G_{\Pcal,n}$ the final step is to relate local clustering in $G_{\HP,n}$ to the original hyperbolic random graph $G_{\H,n}$. As mentioned in Section~\ref{ssec:coupling_H_P}, this is done by moving from $c_{\H,n}(k)$ to the adjusted local clustering $c_{\H,n}^\ast(k)$ (Lemma~\ref{lem:clustering_ast_H}) and then to $c_{\HP,n}^\ast(k)$ (Proposition~\ref{prop:clustering_ast_H_Pois}). We prove Proposition~\ref{prop:clustering_ast_H_Pois} first and will end this section with the proof of Lemma~\ref{lem:clustering_ast_H}.

To achieve the results we consider the standard coupling between the binomial and Poisson process. That is, we take a sequence of i.i.d. random elements $z_1, z_2, \dots$ uniformly on the hyperbolic disk of radius $R_n$, i.e. according to the distribution \eqref{eq:def_hyperbolic_point_distribution}. Then the original hyperbolic random graph consists of the first $n$ points and the poissonized version of the first $N \stackrel{d}{=} \Po(n)$ many points ($N$ is a Poisson random variable with mean $n$). Under this coupling $N_{\H,n}(k) = \sum_{j=1}^n \ind{D_\H(z_j)=k}$ denotes the number of degree $k$ vertices in the original hyperbolic random graph model with $n$ vertices and $N_{\HP,n}(k)=\sum_{j=1}^{N} \ind{D_{\HP}(z_j)=k}$ denotes the (random) number of degree $k$ vertices in the Poisson version of the hyperbolic random graph.

We start with a result that relates the number of nodes with degree $k_n$ in both models. 

\begin{lemma}\label{lem:diff_Nk_hyperbolic_binomial_poisson}
Let $\{k_n\}_{n \ge 1}$ be sequence of natural numbers with $0 \leq k_n \leq n-1$ and $k_n = o(n^{\frac{1}{2\alpha+1}})$. Then
\[
	\Exp{\left|N_{\H,n}(k_n)-N_{\HP,n}(k_n)\right|} = \smallO{\Exp{N_{\HP,n}(k_n)}} = \smallO{n k^{-(2\alpha+1)}},
\]
and in particular
\[
	\Exp{N_{\H,n}(k_n)} = \bigT{k_n^{-(2\alpha + 1)} n}.
\]
\end{lemma}

\TM{ I thought it might be a good idea to show somewhere in the paper that the result of Gugelmann et al.~that
$N_k = (1+o(1)) p_k n$ a.a.s., extends for all 
$k \ll n^{1/(2\alpha+1)}$ -- where $p_k$ the expression in terms on $\Gamma^+, \xi$ etc. 
Gugelmann et al.~had show it only until some small power of $n$.
Adding this just means we have to 
compute the leading constants for the expectation $\Ee N_k$, and bound the variance, in addition to what we do here.
Somehow I had expected we would include that. In fact Markus had prepared a short write up of the variance part of the argument. 
Should not be too hard to add the rest, no? \\
We already have the degree sequence for the infinite model somewhere in the paper.
}

\begin{proof}
The second result follows directly from the first and Lemma~\ref{lem:N_k_HP_P}. Therefore we will only need to prove the first statement of the lemma.

We use the Chernoff concentration result for a Poisson random~\eqref{eq:def_chernoff_bound_poisson}, which states that with probability $n^{-C^2/2}$ the Poisson random variable $N$ with expectation $n$ is contained in the interval $[n-C\sqrt{n\log n},n+C\sqrt{n \log n}]$. We proceed by bounding the effect on the number of degree $k_n$ vertices by adding or removing $C\sqrt{n\log n}$ many vertices to $G_{\H,n}(\alpha,\nu$ and from $G_{\HP,n}(\alpha,\nu)$, respectively.

Define the events
\begin{align*}
	A_n^{\pm} := \{N \in [n, n \pm C\sqrt{n \log n}]\},
\end{align*}
\TM{ weird notation }
and let $A_n = A_n^{-} \cup A_n^{+}$. Then,
\begin{align*}
\Exp{\left|N_{\H,n}(k_n)-N_{\HP,n}(k_n)\right|} 
&\le \CExp{\left|N_{\H,n}(k_n)-N_{\HP,n}(k_n)\right|}{A_n} + \bigO{n^{1 - C^2/2}}\\
&= \CExp{\left|N_{\H,n}(k_n)-N_{\HP,n}(k_n)\right|}{A_n} + \smallO{\Exp{N_{\HP,n}(k_n)}}
\end{align*}
by choosing $C$ large enough, e.g. $C > \sqrt{2}$. What is left to show is that for any $C > 0$
\[
	 \CExp{\left|N_{\H,n}(k_n)-N_{\HP,n}(k_n)\right|}{A_n} = \smallO{\Exp{N_{\HP,n}(k_n)}}.
\]

Let $V_{\H,n}(k_n)$ be the set of degree $k_n$ vertices in the binomial graph $G_{\H,n}$ and $V_{\HP,n}(k_n)$ be the set of degree $k_n$ vertices in the Poisson graph $G_{\HP,n}$. Then 
\[
	\left|N_{\H,n}(k_n)-N_{\HP,n}(k_n)\right| = |V_{\H,n}(k_n) \Delta V_{\HP,n}(k_n)|, 
	%= |V_{\H,n}(k_n) \setminus V_{\HP,n}(k_n) | + |V_{\HP,n}(k_n) \setminus V_{\H,n}(k_n) |,
\]
where $A \Delta B$ denotes the symmetric difference between two sets $A$ and $B$.

We first consider the case $N \in [n,n+C\sqrt{n\log n}]$, i.e. the event $A_n^{+}$. For $z \in V_{\HP,n}(k_n) \setminus V_{\H,n}(k_n)$, $z$ has degree $k_n$ in the Poisson graph, but not in the binomial graph; \TM{ I dont particularly like the name ``binomial graph''. Maybe just use ``standard KPKVB graph'' or ``fixed number of node graph'' } so as $N \geq n$, either $z$ or one of its $k_n$ neighbors must have been removed during the transition from the Poisson graph to the binomial graph. On the event $A_n$, at most $C\sqrt{n\log n}$ many vertices are removed. The probability of hitting a degree $k_n$ vertex or one of its neighbors is at most $\frac{k_n+1}{N} \leq \frac{k_n+1}{n}$. Therefore, by the union bound, the probability that a particular degree $k_n$ vertex of the Poisson graph is removed is upper bounded by $C\sqrt{n\log n}\frac{k_n+1}{n}$. Hence, the expected number of degree $k_n$ vertices that disappear in the transition from the Poisson graph to the binomial graph is bounded by 
\[
	\CExp{|V_{\HP,n}(k_n) \setminus V_{\H,n}(k_n)}{A_n^{(1)}}\leq \E[N_{\HP,n}(k_n)] C\sqrt{n \log n}\frac{k_n+1}{n}  
	= \smallO{\Exp{N_{\HP,n}(k_n)}},
\] 
\TM{ absolute bar missing above }
where the last line follows since for $\alpha > 1/2$,
\[
	k_n \sqrt{\frac{\log(n)}{n}} = \smallO{n^{\frac{1}{2\alpha + 1}}\sqrt{\frac{\log(n)}{n}}} 
	= \smallO{n^{-\frac{2\alpha - 1}{4\alpha + 2}} \sqrt{\log(n)}} = \smallO{1}.
\]

For $z \in V_{\H,n}(k_n) \setminus V_{\HP,n}(k_n)$, $z$ is a degree $k_n$ vertex in the binomial graph, but 
must have degree $k_n+\ell$ in the Poisson graph (where $1 \leq \ell \leq c\sqrt{n\log n}$). By linearity of expectation the expected number of degree $k_n+\ell$ vertices of the Poisson graph which turn into degree $k_n$ vertices of the binomial graph is equal to the expected number of degree $k_n+\ell$ vertices in the Poisson graph times the probability that a degree $k_n+\ell$ vertex turns into a degree $k_n$ vertex in the transition back, from the Poisson graph to the binomial graph. The probability of choosing uniformly a set of $\ell$ neighbors of a degree $k_n+\ell$ vertex of the Poisson graph is given by $\frac{k_n+\ell}{N}\cdots \frac{k_n+1}{N-\ell + 1}$. \TM{ what exactly is the chance experiment here? You are doing ordered, which is not what I would have expected. Maybe that can spelled out. } Now, using $k_n = \smallO{n^{\frac{1}{2\alpha+1}}} = \smallO{C\sqrt{n\log n}}$ for $\alpha > \frac{1}{2}$, $\ell \leq C\sqrt{n \log n}$ and $N-\ell + 1 \geq n$, this probability is bounded from above by $(C+1)^\ell (\frac{\sqrt{n \log n}}{n})^\ell =((C+1) \sqrt{\frac{\log n}{n}})^\ell$ which is upper bounded by $(\frac{1}{2})^\ell$ for $n$ large enough, i.e. $n \geq n_0$. Therefore, using the geometric series, we conclude
\begin{align*}
\CExp{|V_{\H,n}(k_n) \setminus V_{\HP,n}(k_n)|}{A_n^{+}} 
&\le  \sum_{\ell=1}^{\sqrt{n\log n}} \Exp{N_{\HP,n}(k_n+\ell)} \left((c+1)\sqrt{\frac{\log n}{n}}\right)^{\ell}\\
&\le \sum_{\ell=1}^{\sqrt{n\log n}} \bigT{n (k_n+\ell)^{-2\alpha-1}} \left((c+1)\sqrt{\frac{\log n}{n}}\right)^{\ell}\\
&= \bigO{\Exp{N_{\HP,n}(k_n)}} \sum_{\ell=1}^{\sqrt{n\log n}} \left((c+1)\sqrt{\frac{\log n}{n}}\right)^{\ell}
= \smallO{\Exp{N_{\HP,n}(k_n)}},
\end{align*}
and hence
\[
	\CExp{\left|N_{\H,n}(k_n)-N_{\HP,n}(k_n)\right|}{A_n^{+}} = \smallO{\Exp{N_{\HP,n}(k_n)}}.
\]

The case $N \in [n-C\sqrt{n\log n},n)$ (event $A_n^{-}$) follows by similar arguments. As $N < n$, a vertex $z \in V_{\HP,n}(k_n) \setminus V_{\H,n}(k_n)$ with degree $k_n$ in the Poisson graph must have a strictly larger degree in the binomial graph, i.e. in the transition from the Poisson graph to the binomial graph, a vertex must have been dropped in the neighborhood of $z$. By the union bound, this can be upper bounded by the number of additional vertices (of the binomial graph) times the probability that a random point falls into the neighborhood of a degree $k_n$ vertex. We obtain
\begin{align*}
	\CExp{|V_{\HP,n}(k_n) \backslash V_{\H,n}(k_n)|}{A_n^{(2)}} = \bigO{ \sqrt{n\log n}\frac{k_n}{n} \Exp{N_{\HP,n}(k_n)} }
	= \smallO{\Exp{N_{\HP,n}(k_n)}}
\end{align*}

\TM{ Not so fast! You seem assume that dropping into 
the ball around a {\bf degree} $k$ vertex has order 
$k/n$. That has to be argued. It is of course only true if the height of the particular vertex is appropriate. Please correct / provide the relevant details. I guess we may have to appeal to ``concentration of heights'' here. }
A vertex $z \in V_{\H,n}(k_n) \setminus V_{\HP,n}(k_n)$ could be one of the additional vertices in the binomial graph or it is a degree $k_n-\ell$ vertex of the Poisson graph which receives exactly $\ell$ new vertices in its neighborhood in the transition from the Poisson graph to the binomial graph. The probability that one of the additional vertices of the binomial graph (compared to the smaller Poisson graph) has degree $k_n$ is asymptotically of the order $k_n^{-(2\alpha+1)}$ (as can be seen by considering the alternative coupling between the binomial and the Poisson process, where instead of taking $z_1, \dots, z_N$ for the Poisson process, we take the points $z_n, z_{n-1}, \dots, z_{n-N+1}$ (resp. points with index larger than $n$ after we hit $z_1$):
\TM{ I don't quite follow the construction here. } for this graph, we have that the expected number of degree $k_n$ vertices is $\bigT{n k^{-2\alpha-1}}$, so the probability that a vertex chosen uniformly from the Poisson graph has degree $k$ is $\Theta(k^{-2\alpha-1})$). Therefore, the expected number of additional points with degree $k_n$ is $\bigO{\sqrt{n\log n} k_n^{-2\alpha-1}} = \smallO{n k_n^{-2\alpha-1}} = \smallO{\Exp{N_{\HP,n}(k_n)}}$. The expected number of degree $k_n-\ell$ vertices of the Poisson graph which receive exactly $\ell$ new vertices can be bounded in a sum resp. series similarly as done for $z \in V_{\H,n}(k_n) \setminus V_{\HP,n}(k_n)$ in the case $N \geq n$. We therefore conclude that
\[
	\CExp{\left|N_{\H,n}(k_n)-N_{\HP,n}(k_n)\right|}{A_n^{-}} = \smallO{\Exp{N_{\HP,n}(k_n)}},
\]
which finishes the proof.
\end{proof}

We note that this result together with Lemma~\ref{lem:N_k_HP_P} implies Lemma~\ref{lem:N_k_H_P}. Next we prove Proposition~\ref{prop:clustering_ast_H_Pois}, which states
\[
	\lim_{n \to \infty} s_\alpha(k_n)\Exp{\left|c_{\H,n}^\ast(k_n) - c_{\HP,n}^\ast(k_n)\right|} = 0.
\]

\begin{proof}[Proof of Proposition~\ref{prop:clustering_ast_H_Pois}]
First we note that Proposition~\ref{prop:couling_c_H_P}, Proposition~\ref{prop:concentration_local_clustering_P_n}, Proposition~\ref{prop:convergence_average_clustering_P_n} and Theorem~\ref{thm:asymptotics_average_clustering_P} together imply that
\[
	\Exp{c_{\HP,n}^\ast(k_n)} = (1+\smallO{1})s_\alpha(k_n)
\]
Therefore it suffices to show that
\[
	\Exp{\left|c_{\H,n}^\ast(k_n) - c_{\HP,n}^\ast(k_n)\right|} = \smallO{\Exp{c_{\HP,n}^\ast(k_n)}}.
\]
For this we observe that we are looking at the modified clustering coefficient, where we divide by the expected number of degree $k_n$ vertices. As the expected numbers of degree $k_n$ vertices in $G_{\HP,n}$ and $G_{\H,n}$ are asymptotically equivalent (see Lemma~\ref{lem:diff_Nk_hyperbolic_binomial_poisson}), it is therefore sufficient to consider the sum of the clustering coefficients of all vertices of degree $k_n$.
Given again the standard coupling between the binomial and Poisson process (as used in the proof of Lemma~\ref{lem:diff_Nk_hyperbolic_binomial_poisson}), we denote by $V_{\H,n}(k_n)$ the set of degree $k_n$ vertices in $G_{\H,n}$ and by $V_{\HP,n}(k_n)$ the set of degree $k_n$ vertices in the graph $G_{\HP,n}$. If a vertex is contained in both sets, it must have the same degree in both the Poisson and binomial graph, and given the nature of the coupling, the neighbourhoods are therefore the same and hence also their clustering coefficients agree.

The difference of the sum of the clustering coefficients therefore comes from all the clustering coefficients of the symmetric difference $V_{\H,n}(k_n) \Delta V_{\HP,n}(k_n)$. This symmetric difference is again a Poisson process, whose expected number of points is $\Exp{\left|N_{\H,n}(k_n) - N_{\HP,n}(k_n)\right|} = \smallO{\Exp{N_{\H,n}(k_n)}}$ by Lemma~\ref{lem:diff_Nk_hyperbolic_binomial_poisson}. Therefore we have that
\begin{align*}
	\Exp{\left|c_{\H,n}^\ast(k_n) - c_{\HP,n}^\ast(k_n)\right|}
	&\le \frac{\Exp{\left|N_{\H,n}(k_n)-N_{\HP,n}(k_n)\right|}}{\Exp{N_{\H,n}(k_n)}} \Exp{c_{\HP,n}^\ast(k_n)}
	= \smallO{1}\Exp{c_{\HP,n}^\ast(k_n)},
\end{align*}
which finishes the proof.
\end{proof}

We end this section with the proof of Lemma~\ref{lem:clustering_ast_H}, whose statement is
\[
	\Exp{\left|c_{\H, n}^\ast(k_n) - c_{\H, n}(k_n)\right|} = \smallO{s_\alpha(k_n)}.
\]

\begin{proof}[Proof of Lemma~\ref{lem:clustering_ast_H}]
Let $0 < \delta < 1$ and define the event
\begin{align*}
	A_n &= \left\{\left|N_{\H,n}(k_n) - \Exp{N_{\H,n}(k_n)}\right| \le \Exp{N_{\H,n}(k_n)}^{\frac{1 + \delta}{2}}\right\}.
	%B_n &= \left\{\left|N_{\H,n}(k_n) - \Exp{N_{\H,n}(k_n)}\right| \le n\right\},
\end{align*}

Since $N_{\H,n}(k_n) = \sum_{i = 1}^n \ind{D_\H(i) = k_n}$ it follows from Lemma~\ref{lem:general_concentration_sum_indicators}, with $c = \Exp{N_{\H,n}(k_n)}^{-\frac{1-\delta}{2}}$, that
\begin{equation}\label{eq:clustering_ast_H_prob_A}
	\Prob{A_n} \ge 1 - \bigO{e^{-\frac{\Exp{N_{\H,n}(k_n)}^\delta}{2}}} = 1 - \bigO{e^{-\frac{n^\delta k_n^{-\delta(2\alpha + 1)}}{2}}},
\end{equation}
where the last part is due to Lemma~\ref{lem:diff_Nk_hyperbolic_binomial_poisson}. 

On the event $A_n$
\[
	\left|\frac{\Exp{N_{\H,n}(k_n)}}{N_{\H,n}(k_n)} - 1\right| 
	\le \frac{\Exp{N_{\H,n}(k_n)}^{\frac{1 + \delta}{2}}}{\Exp{N_{\H,n}(k_n)}+\Exp{N_{\H,n}(k_n)}^{\frac{1 + \delta}{2}}}
	\le \Exp{N_{\H,n}(k_n)}^{-\frac{1 - \delta}{2}}.
\]

Therefore we have
\begin{align*}
	\Exp{\left|c_{\H, n}^\ast(k_n) - c_{\H, n}(k_n)\right|}
	&\le \Exp{\left|c_{\H, n}^\ast(k_n) - c_{\H, n}(k_n)\right|\ind{A_n}} + \bigO{1 - \Prob{A_n}}\\
	&= \Exp{c_{\H, n}^\ast(k_n)\left|\frac{\Exp{N_{\H,n}(k_n)}}{N_{\H,n}(k_n)} - 1\right|\ind{A_n}}
		+ \bigO{e^{-\frac{n^\delta k_n^{-\delta(2\alpha + 1)}}{2}}}\\
	&\le \Exp{c_{\H, n}^\ast(k_n)}\Exp{N_{\H,n}(k_n)}^{-\frac{1 - \delta}{2}} 
		+ \bigO{e^{-\frac{n^\delta k_n^{-\delta(2\alpha + 1)}}{2}}}.
\end{align*}
The second term is clearly $\smallO{s_\alpha(k_n)}$. The first term is clearly $\smallO{\Exp{c_{\H, n}^\ast(k_n)}}$ which is $\smallO{s_\alpha(k_n)}$ by Proposition~\ref{prop:clustering_ast_H_Pois}. 
\end{proof}









