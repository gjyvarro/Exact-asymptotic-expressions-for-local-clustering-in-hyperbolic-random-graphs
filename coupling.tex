\section{Equivalence for local clustering in hyperbolic and Poissonized random graph}

In this section we establish the equivalence between $c_{\H,n}^\ast(k)$ and $c_{\Pcal,n}^\ast(k)$ as expressed in Proposition~\ref{prop:couling_c_H_P}, using the coupling procedure explained in Section~\ref{ssec:coupling_H_P}. 

Recall that $\mathcal{P}_{\alpha,\nu}$ denotes a Poisson process on $\mathbb{R} \times \mathbb{R}_+$, with intensity $f_{\alpha,\nu}(x,y)$, $I_n = \left(-\frac{\pi}{2}e^{R_n/2}, \frac{\pi}{2}e^{R_n/2}\right)$, $\mathcal{R}_n = I_n \times (0,R_n]$ and $\mathcal{V}_n = \mathcal{P}_{\alpha, \nu}\cap \mathcal{R}_n$. In addition we define for any interval $I \subseteq \mathbb{R}_+$, $\Rcal_n(I) := I_n \times I$ and denote by $\BallPo{p}$ the \emph{ball}
\[
	\BallPo{p} = \left\{p^\prime \in \mathcal{V}_n : |x - x^\prime |_{\pi e^{R_n/2}} < e^{\frac{y+y^\prime}{2}}\right\}.
\]
Note that when $p \in \mathcal{V}_n$ then $\BallPo{p}$ denotes its neighborhood in the graph $G_{\mathcal{P},n}$. 
Note that the above definition implies that for all $y\in [0,R_n]$ we have 
\begin{equation} \label{eq:upper_ballPo_inclusion}
\Rcal_n([R_n-y - 2\ln (\pi/2), R_n]) \subseteq \BallPo{(0,y)}
\end{equation}
- this is a fact which we are going to use several times in our analysis. 

For any Borel-measurable subset $S \subseteq \mathbb{R} \times \mathbb{R}_+$, we let 
\[
	\mu_{\alpha, \nu} (S) = \int_S f_{\alpha, \nu}(x,y) \, dx \, dy = \frac{\nu \alpha}{\pi}\int_S e^{-\alpha y}dy.
\]
Thus, the number of points of $\Pcal_{\alpha, \nu}$ inside $S$ is distributed as $\Po ({\mu_{\alpha, \nu,} (S)})$.

Finally, we remind the reader that $\BallHyp{p}$ denotes the image under $\Psi$ of the ball of hyperbolic radius $R_n$ around the point $\Psi^{-1}(p)$ and that under the coupling between the hyperbolic random graph and the finite box model, described in Section~\ref{ssec:coupling_H_P}, two point $p$ and $p^\prime$ are connected if and only if
\[
	|x-x^\prime|_{\pi e^{r_n/2}} \le \Omega(R_n - y, R_n - y^\prime),
\]
where the function $\Omega$ can be approximated, for $y + y^\prime < R_n$, using Lemma~\ref{lem:asymptotics_Omega_hyperbolic} by  
\[
	e^{\frac{1}{2}(y+y^\prime)} - K e^{\frac{3}{2}(y+y^\prime) - R_n} \leq \Omega(r, r^\prime) 
		\leq  e^{\frac{1}{2}(y+y^\prime)} + K e^{\frac{3}{2}(y+y^\prime) - R_n}.
\]

To prove Proposition~\ref{prop:couling_c_H_P} we calculate the error in two steps. First we show in Section~\ref{ssec:coupling_HP_ast_P} that
\[
	\lim_{n \to \infty} s_\alpha(k_n) \Exp{\left|c_{\HP,n}^\ast(k_n) - c_{\Pcal,n}^\ast(k_n)\right|} = 0,
\]
Then, in Section~\ref{ssec:coupling_H_HP}, we prove Lemma~\ref{lem:clustering_ast_H} and prove that
\[
	\lim_{n \to \infty} s_\alpha(k_n) \Exp{\left| c_{\H,n}^\ast(k_n) - c_{\HP,n}^\ast(k_n)\right|} = 0.
\]
Together these results yield Proposition~\ref{prop:couling_c_H_P}.

\subsection{Some results on the hyperbolic geometric graph}

We start with some basic results for the hyperbolic random geometric graph. Observe that Lemma~\ref{lem:asymptotics_Omega_hyperbolic} implies the following. 
\begin{corollary}\label{cor:balls_inclusion}
\begin{equation*}
 \BallPo{p} \cap \Rcal_n ([K,R_n]) \subseteq \BallHyp{p} \cap \Rcal_n (K,R_n). 
\end{equation*}
\end{corollary}


Furthermore, Lemma~\ref{lem:asymptotics_Omega_hyperbolic} enables us to determine the measure of a ball around a given point $p=(0,y)$ - this is will be fairly useful in our subsequent analysis. 

\begin{lemma}
Let $\alpha > 1/2$, $\nu > 0$ and $\{k_n\}_{n\ge 1}$ be a sequence such that $k_n = \smallO{n^{1/(2\alpha + 1)}}$. Then
\begin{equation} \label{eq:n_k_Hyp}
	\Exp{N_{\HP,n}(k_n)} = \bigT{1} n k_n^{-(2\alpha + 1)},
\end{equation}
and
\begin{equation} \label{eq:n_k_Po}
	\Exp{N_{\Pcal,n}(k_n)} = \bigT{1} n k_n^{-(2\alpha + 1)}.
\end{equation}
Moreover,
\begin{equation}\label{eq:equivalence_N_HP_P}
	\lim_{n \to \infty} \left|\frac{\Exp{N_{\HP,n}(k_n)}}{\Exp{N_{\Pcal,n}(k_n)}} - 1\right| = 0.
\end{equation}
\end{lemma}

\begin{proof}
Recall that
\[
	\Exp{N_{\H,n}(k_n)} = \int_{\Rcal_n} \rho_{\HP,n}(y,k_n) f_{\alpha,\nu}(x,y) \dd x \dd y. 
\]
Then by Lemma~\ref{lem:concentration_argument_rho_approximation} and a concentration argument
\begin{align*}
	\Exp{N_{\H,n}(k_n)} &= (1 + \smallO{1})\int_{\Rcal_n} \rho(y,k_n) f_{\alpha,\nu}(x,y) \dd x \dd y\\
	&= (1 + \smallO{1}) n \int_{0}^{R_n} \rho(y,k_n) f_{\alpha,\nu}(x,y) \dd x \dd y = \bigT{1}n k_n^{-(2\alpha + 1)}.
\end{align*}
Similarly,
\[
	\Exp{N_{\Pcal,n}(k_n)} = (1 + \smallO{1})\int_{\Rcal_n} \rho(y,k_n) f_{\alpha,\nu}(x,y) \dd x \dd y
\]
From which both~\eqref{eq:n_k_Po} and~\eqref{eq:equivalence_N_HP_P} follow.
\end{proof}

Let $p \in \Rcal_n$. Then we can see that the curve $x^\prime = e^{\frac{1}{2} (y + y^\prime)}$ with $x^\prime \geq 0$ meets the right boundary of $\Rcal$, that is, the line $x^\prime = \frac{\pi}{2} e^{R_n/2}$ at $y^\prime = R_n - y + 2\ln \frac{\pi}{2}$. Hence, any point $p^{\prime} \in \Rcal ([R_n - y + 2\ln \frac{\pi}{2}, R_n])$ is included in $\BallPo{p}$. In other words,
\begin{equation*} \label{eq:P_ball_inclusion_lower}
\BallPo{p} \cap \Rcal ([R_n - y +2\ln \frac{\pi}{2},R_n]) = \Rcal ([R_n - y + 2\ln \frac{\pi}{2},R_n]).
\end{equation*}
This together with \eqref{eq:tail_inclusion_hyperbolic_ball} implies that 
\begin{equation}\label{eq:symm_diff_upper_P} 
(\BallHyp{p} \bigtriangleup \BallPo{p})  \cap \Rcal ([R_n - y + 2 \ln \frac{\pi}{2},R_n]) = \emptyset. 
\end{equation}

\begin{lemma}\label{lem:sym_diff_measure_H_P}
For any $y' \in (0,R_n - y)$ we have
\[
	\mu_{\alpha,\nu} (\BallHyp{(0,y')} \bigtriangleup \BallPo{(0,y')} ) 
	= \Theta (1) \cdot \begin{cases} 
		e^{(1/2-\alpha)R_n + \alpha y'}, & \mbox{if } \alpha < 3/2 \\
		(R_n-y) e^{3y'/2 - R_n}, & \mbox{if }\alpha = 3/2\\
		e^{3y'/2 - R_n}, &  \mbox{if } \alpha > 3/2 
	\end{cases}.
\]
\end{lemma}

\begin{proof}
Let $r_n := R_n - y_n$. Lemma~\ref{lem:asymptotics_Omega_hyperbolic} implies that for such a $p_n$, if a point $p^\prime$ belongs to $\BallHyp{p_n} \bigtriangleup \BallPo{p_n} \cap \Rcal ([0,r_n])$ then 
\[
	|x_n - x^\prime| = \Theta(1) \cdot e^{\frac{3}{2} (y_n + y^\prime) - R_n}.
\]
Now, if $p^\prime \in [r_n, r_n + 2 \ln \frac{\pi}{2})]$ and also $p^\prime \in \BallHyp{p_n} \bigtriangleup \BallPo{p_n}$, then 
\[
	|x_n-x^\prime| =\frac{\pi}{2} e^{R_n/2} - e^{\frac{1}{2} (y_n + y^\prime)}.
\]
Finally,~\eqref{eq:symm_diff_upper_P} implies that no point in $\Rcal ([r_n+2\ln \frac{\pi}{2},R_n])$ belongs to $\BallHyp{p_n} \bigtriangleup \BallPo{p_n}$. We first compute the expected number of points in $p^\prime \in \BallHyp{p_n} \bigtriangleup \BallPo{p_n}$ that have $R_n - y^\prime \le r_n$. The result depends on the value of $\alpha$, yielding the following three cases
\begin{align*}
	\mu_{\alpha,\nu} (\BallHyp{p_n} \bigtriangleup \BallPo{p_n} \cap \Rcal ([0,r_n])) 
	&= \Theta(1) \cdot e^{3y_n/2 - R_n}\int_0^{r_n} e^{(3/2-\alpha) y} \, dy \\
	&= \Theta(1)\cdot \begin{cases} e^{(1/2-\alpha)R_n + \alpha y_n}, & \mbox{if } \alpha < 3/2 \\
		r_n e^{3y_n/2 - R_n}, & \mbox{if }\alpha = 3/2\\
		e^{3y_n/2 - R_n}, &  \mbox{if } \alpha > 3/2
	\end{cases}.
\end{align*}
Next we compute the number of remaining points in $\BallHyp{p_n} \bigtriangleup \BallPo{p_n}$, 
\begin{align*}
	\mu_{\alpha,\nu} (\BallHyp{p_n} \bigtriangleup \BallPo{p_n} \cap \Rcal ([r_n, R_n])) 
	&= \frac{\nu\alpha}{\pi} \int_{r_n}^{r_n + 2 \ln \frac{\pi}{2}} 
		\left(\frac{\pi}{2} e^{R_n/2} - e^{\frac{1}{2} (y_n + y)}\right)e^{-\alpha y} \, dy \\
	&= O(1) \cdot e^{R_n/2} \int_{r_n}^{r_n + 2 \ln \frac{\pi}{2}} e^{-\alpha y} \, dy 
		= O(1) \cdot e^{R_n/2} e^{-\alpha r_n} \\
	&=O(1) \cdot e^{(1/2 - \alpha) R_n + \alpha y_n}.
\end{align*}
Now note that for any $\alpha > 3/2$, we have 
\[
	\left( (1/2 - \alpha) R_n + \alpha y_n\right) - \left(3y_n/2 - R_n \right)\to -\infty,
\]
since
\begin{align*}
	\left( (1/2 - \alpha) R_n + \alpha y_n\right) - \left(3y_n/2 - R_n \right) 
	&= (3/2-\alpha )R_n - (3/2 - \alpha) y_n 
	&= (3/2 -\alpha) (R_n- y_n) \to -\infty.
\end{align*}
For $\alpha = 3/2$, these two quantities are equal. From these observations, we deduce that 
\[
	\mu_{\alpha,\nu} (\BallHyp{p_n} \bigtriangleup \BallPo{p_n} ) 
	= \Theta (1) \cdot \begin{cases} 
		e^{(1/2-\alpha)R_n + \alpha y_n}, & \mbox{if } \alpha < 3/2 \\
		r_n e^{3y_n/2 - R_n}, & \mbox{if }\alpha = 3/2\\
		e^{3y_n/2 - R_n}, &  \mbox{if } \alpha > 3/2 
	\end{cases}.
\]
\end{proof}

Using this result on the symmetric difference we can establish a bound on the difference between the number of nodes of degree $k_n$ in $G_{\HP,n}$ and $G_{\Pcal,n}$.

\begin{lemma} \label{lem:difference_N_HP_P}
We have 
\[
	\lim_{n \to \infty} \left|\frac{\Exp{N_{\HP,n}(k_n)}}{\Exp{N_{\Pcal,n}(k_n)}} - 1\right| = 0.
\]
\end{lemma}

\begin{proof} 
Recall that
\[
	\Exp{N_{\Pcal,n}(k_n)} = \int_{\Rcal_n} \rho_{\HP,n}(y,k_n) f_{\alpha,\nu}(x,y) \dd x \dd y.
\]
Then, by Lemma~\ref{lem:concentration_argument_rho_approximation} we have that
\[
	\Exp{N_{\Pcal,n}(k_n)} = (1+\smallO{1}) \int_{\Rcal_n} \rho(y,k_n) f_{\alpha,\nu}(x,y) \dd x \dd y.
\]
The same result holds for $\Exp{N_{\Pcal,n}(k_n)}$ from which the results follows.
\end{proof}

%\subsection{Figures for proof strategy}
%
%\PvdH{I will create some figures illustrating the proof strategy for the coupling. These will be merged with the computations as soon as Nikolaos finishes the computations.}
%
%\begin{figure}
%
%%\begin{tikzpicture}
%%
%%	\pgfmathsetmacro{\u}{0}
%%	\pgfmathsetmacro{\v}{0.5}
%%	\pgfmathsetmacro{\uu}{1}
%%	\pgfmathsetmacro{\vv}{1.2}
%%	\pgfmathsetmacro{\uuu}{-3}
%%	\pgfmathsetmacro{\vvv}{2}
%%	\pgfmathsetmacro{\e}{0.05}
%%	\pgfmathsetmacro{\K}{1}
%%	\pgfmathsetmacro{\R}{3}
%%	\pgfmathsetmacro{\r}{(pi/2)*exp(\R/2)}
%%
%%	\draw[line width=1pt,dashed] (-\r,0) -- (\r,0) -- (\r,\R) -- (-\r,\R) -- (-\r,0);
%%
%%	\draw[domain=0:{\R-\v},variable=\x, samples=200, black, line width=1pt] plot ({(exp(\R/2)/2)*rad(acos((cosh(\R-\v)*cosh(\R-\x)-cosh(\R))/(sinh(\R-\v)*sinh(\R-\x))))},{\x});
%%
%%\end{tikzpicture}
%
%\begin{tikzpicture}[scale=0.85]
%	%Define the coordinates 
%	%p = (\u,\v), p_1 = (\uu, \vv)
%	%Box \Rcal_n has width 2\r and height \t (\r = \pi/2 e^{R_n/2})
%	\pgfmathsetmacro{\u}{0}
%	\pgfmathsetmacro{\v}{0.5}
%	\pgfmathsetmacro{\uu}{1}
%	\pgfmathsetmacro{\vv}{1.2}
%	\pgfmathsetmacro{\uuu}{5.5}
%	\pgfmathsetmacro{\vvv}{1.7}
%	\pgfmathsetmacro{\epsilon}{0.05}
%	\pgfmathsetmacro{\K}{1}
%	\pgfmathsetmacro{\R}{3}
%	\pgfmathsetmacro{\r}{(pi/2)*exp(\R/2)}
%	\pgfmathsetmacro{\t}{\R}
%	
%	\pgfmathsetmacro{\leftintvandvvv}{-exp((\v + \vvv)/2)}
%	\pgfmathsetmacro{\rightintvandvvv}{exp((\v + \vvv)/2)}
%	\pgfmathsetmacro{\leftintvvandvvv}{\uu-exp((\vv + \vvv)/2)}
%	\pgfmathsetmacro{\rightintvvandvvv}{\uu+exp((\vv + \vvv)/2)}
%		
%	%The box \Rcal_n
%	\draw[line width=1pt,dashed] (0,0) -- (\r,0) -- (\r,\t) -- (0,\t);
%
%	%Dram all three nodes
%    \draw node[fill, circle, inner sep=0pt, minimum size=5pt] (p0) at (\u,\v) {};
%    \path (p0)+(-0.3,0.3) node {$p_0$};
%    
%    \draw node[fill,blue, circle, inner sep=0pt, minimum size=5pt] (p1) at (\uu,\vv) {};
%    \path (p1)+(-0.3,0.3) node {\color{blue}$p_1$};	
%
%	
%	%Boundaries p_0 = (\u,\v)
%	
%	%Limit model
%	%Right boundary
%	\pgfmathsetmacro{\rightbounduv}{\u+exp((\v)/2)}
%	\draw[domain=0:\R,smooth,variable=\y,black, dotted, line width=1pt] plot ({exp((\v+\y)/2)},{\y});
%    
%    %Hyperbolic model
%    %Right boundary
%	\draw[domain=0:{\R-\v},variable=\y, samples=200, black, line width=1pt] plot 		
%		({(exp(\R/2)/2)*rad(acos((cosh(\R-\v)*cosh(\R-\y)-cosh(\R))/(sinh(\R-\v)*sinh(\R-\y))))},{\y});
%
%    
%	%Boundaries p_1 = (\uu,\vv)
%	
%	%Limit model
%	\pgfmathsetmacro{\rboundvv}{2*ln(\r-\uu)-\vv}
%	\pgfmathsetmacro{\lboundvv}{2*ln(\r+\uu)-\vv}
%
%    
%    %Hyperbolic model
%	\pgfmathsetmacro{\rhboundvv}{1.76}
%	\pgfmathsetmacro{\lhboundvv}{\R-\vv}
%
%	%Intersection
%	\pgfmathsetmacro{\regionright}{\uu+exp((\vv+\rboundvv)/2)}
%
%	\draw[red,line width=1pt,pattern=north west lines, pattern color=red] 
%		plot[domain=0.6:\rboundvv,smooth,variable=\y,red] ({\uu+exp((\vv+\y)/2)},{\y}) 
%		--
%		(\regionright,\rboundvv) 
%		--
%		(\regionright,\rhboundvv)
%		--
%		plot[domain={\rhboundvv-0.01}:0.6,smooth,variable=\y,red] ({\uu+(exp(\R/2)/2)*rad(acos((cosh(\R-\vv)*cosh(\R-\y)-cosh(\R))/(sinh(\R-\vv)*sinh(\R-\y))))},{\y});
%		
%	\draw[red,line width=1pt,pattern=north west lines, pattern color=red] 
%		plot[domain=0:0.6,smooth,variable=\y,red] ({\uu+exp((\vv+\y)/2)},{\y}) 
%		--
%		plot[domain=0.6:0,smooth,variable=\y,red] ({\uu+(exp(\R/2)/2)*rad(acos((cosh(\R-\vv)*cosh(\R-\y)-cosh(\R))/(sinh(\R-\vv)*sinh(\R-\y))))},{\y});
%
%	%Limit model
%	%Right boundary
%	\draw[domain=0:\rboundvv,smooth,variable=\y,blue, dotted, line width=1pt] plot ({\uu+exp((\vv+\y)/2)},{\y});
%
%    %Hyperbolic model
%    %Right boundary
%	\draw[domain=0:\rhboundvv,variable=\y, samples=200, blue, line width=1pt] plot 		
%		({\uu+(exp(\R/2)/2)*rad(acos((cosh(\R-\vv)*cosh(\R-\y)-cosh(\R))/(sinh(\R-\vv)*sinh(\R-\y))))},{\y});
%
%
%
%\end{tikzpicture}~\hspace{5pt}~
%\begin{tikzpicture}[scale=0.85]
%	%Define the coordinates 
%	%p = (\u,\v), p_1 = (\uu, \vv)
%	%Box \Rcal_n has width 2\r and height \t (\r = \pi/2 e^{R_n/2})
%	\pgfmathsetmacro{\u}{0}
%	\pgfmathsetmacro{\v}{0.5}
%	\pgfmathsetmacro{\uu}{1}
%	\pgfmathsetmacro{\vv}{2.7}
%	\pgfmathsetmacro{\uuu}{5.5}
%	\pgfmathsetmacro{\vvv}{1.7}
%	\pgfmathsetmacro{\epsilon}{0.05}
%	\pgfmathsetmacro{\K}{1}
%	\pgfmathsetmacro{\R}{3}
%	\pgfmathsetmacro{\r}{(pi/2)*exp(\R/2)}
%	\pgfmathsetmacro{\t}{\R}
%	
%	\pgfmathsetmacro{\leftintvandvvv}{-exp((\v + \vvv)/2)}
%	\pgfmathsetmacro{\rightintvandvvv}{exp((\v + \vvv)/2)}
%	\pgfmathsetmacro{\leftintvvandvvv}{\uu-exp((\vv + \vvv)/2)}
%	\pgfmathsetmacro{\rightintvvandvvv}{\uu+exp((\vv + \vvv)/2)}
%		
%	%The box \Rcal_n
%	\draw[line width=1pt,dashed] (0,0) -- (\r,0) -- (\r,\t) -- (0,\t);
%
%	%Dram all three nodes
%    \draw node[fill, circle, inner sep=0pt, minimum size=5pt] (p0) at (\u,\v) {};
%    \path (p0)+(-0.3,0.3) node {$p_0$};
%    
%    \draw node[fill,blue, circle, inner sep=0pt, minimum size=5pt] (p1) at (\uu,\vv) {};
%    \path (p1)+(-0.3,-0.3) node {\color{blue}$p_1$};	
%
%	
%	%Boundaries p_0 = (\u,\v)
%	
%	%Limit model
%	%Right boundary
%	\pgfmathsetmacro{\rightbounduv}{\u+exp((\v)/2)}
%	\draw[domain=0:\R,smooth,variable=\y,black, dotted, line width=1pt] plot ({exp((\v+\y)/2)},{\y});
%
%    
%    %Hyperbolic model
%    %Right boundary
%	\draw[domain=0:{\R-\v},variable=\y, samples=200, black, line width=1pt] plot 		
%		({(exp(\R/2)/2)*rad(acos((cosh(\R-\v)*cosh(\R-\y)-cosh(\R))/(sinh(\R-\v)*sinh(\R-\y))))},{\y});
%
%    
%	%Boundaries p_1 = (\uu,\vv)
%	
%	%Limit model
%	\pgfmathsetmacro{\rboundvv}{2*ln(\r-\uu)-\vv}
%	\pgfmathsetmacro{\lboundvv}{2*ln(\r+\uu)-\vv}
%
%    
%    %Hyperbolic model
%	\pgfmathsetmacro{\rhboundvv}{0.277}
%	\pgfmathsetmacro{\lhboundvv}{\R-\vv}
%
%	%Intersection
%	\pgfmathsetmacro{\regionright}{\uu+exp((\vv+\rboundvv)/2)}
%	\pgfmathsetmacro{\intersection}{0.15}
%	
%	\draw[red,line width=1pt,pattern=north west lines, pattern color=red] 
%		plot[domain=\intersection:\rboundvv,smooth,variable=\y,red] ({\uu+exp((\vv+\y)/2)},{\y}) 
%		--
%		(\regionright,\rboundvv) 
%		--
%		(\regionright,\rhboundvv)
%		--
%		plot[domain={\rhboundvv-0.01}:\intersection,smooth,variable=\y,red] ({\uu+(exp(\R/2)/2)*rad(acos((cosh(\R-\vv)*cosh(\R-\y)-cosh(\R))/(sinh(\R-\vv)*sinh(\R-\y))))},{\y});
%		
%	\draw[red,line width=1pt,pattern=north west lines, pattern color=red] 
%		plot[domain=0:\intersection,smooth,variable=\y,red] ({\uu+exp((\vv+\y)/2)},{\y}) 
%		--
%		plot[domain=\intersection:0,smooth,variable=\y,red] ({\uu+(exp(\R/2)/2)*rad(acos((cosh(\R-\vv)*cosh(\R-\y)-cosh(\R))/(sinh(\R-\vv)*sinh(\R-\y))))},{\y});
%
%	%Limit model
%	%Right boundary
%	\draw[domain=0:\rboundvv,smooth,variable=\y,blue, dotted, line width=1pt] plot ({\uu+exp((\vv+\y)/2)},{\y});
%
%    %Hyperbolic model
%    %Right boundary
%	\draw[domain=0:\rhboundvv,variable=\y, samples=200, blue, line width=1pt] plot 		
%		({\uu+(exp(\R/2)/2)*rad(acos((cosh(\R-\vv)*cosh(\R-\y)-cosh(\R))/(sinh(\R-\vv)*sinh(\R-\y))))},{\y});
%
%
%\end{tikzpicture}
%
%\begin{tikzpicture}[scale=0.85]
%	%Define the coordinates 
%	%p = (\u,\v), p_1 = (\uu, \vv) and p_2 = (\uuu, \vvv)
%	%Box \Rcal_n has width 2\r and height \t (\r = \pi/2 e^{R_n/2})
%	\pgfmathsetmacro{\u}{0}
%	\pgfmathsetmacro{\v}{0.5}
%	\pgfmathsetmacro{\uu}{1}
%	\pgfmathsetmacro{\vv}{1.2}
%	\pgfmathsetmacro{\uuu}{3}
%	\pgfmathsetmacro{\vvv}{2.5}
%	\pgfmathsetmacro{\epsilon}{0.05}
%	\pgfmathsetmacro{\K}{1}
%	\pgfmathsetmacro{\R}{3}
%	\pgfmathsetmacro{\r}{(pi/2)*exp(\R/2)}
%	\pgfmathsetmacro{\t}{\R}
%	
%	\pgfmathsetmacro{\leftintvandvvv}{-exp((\v + \vvv)/2)}
%	\pgfmathsetmacro{\rightintvandvvv}{exp((\v + \vvv)/2)}
%	\pgfmathsetmacro{\leftintvvandvvv}{\uu-exp((\vv + \vvv)/2)}
%	\pgfmathsetmacro{\rightintvvandvvv}{\uu+exp((\vv + \vvv)/2)}
%		
%	%The box \Rcal_n
%	\draw[line width=1pt,dashed] (0,0) -- (\r,0) -- (\r,\t) -- (0,\t);
%
%	%Dram all three nodes
%    \draw node[fill, circle, inner sep=0pt, minimum size=5pt] (p0) at (\u,\v) {};
%    \path (p0)+(-0.3,0.3) node {$p_0$};
%    
%    \draw node[fill,blue, circle, inner sep=0pt, minimum size=5pt] (p1) at (\uu,\vv) {};
%    \path (p1)+(-0.3,-0.3) node {\color{blue}$p_1$};	
%
%	
%	%Boundaries p_0 = (\u,\v)
%	
%	%Limit model
%	%Right boundary
%	\pgfmathsetmacro{\rightbounduv}{\u+exp((\v)/2)}
%	\draw[domain=0:\R,smooth,variable=\y,black, dotted, line width=1pt] plot ({exp((\v+\y)/2)},{\y});
%
%    
%    %Hyperbolic model
%    %Right boundary
%	\draw[domain=0:{\R-\v},variable=\y, samples=200, black, line width=1pt] plot 		
%		({(exp(\R/2)/2)*rad(acos((cosh(\R-\v)*cosh(\R-\y)-cosh(\R))/(sinh(\R-\v)*sinh(\R-\y))))},{\y});
%
%    
%	%Boundaries p_1 = (\uu,\vv)
%	
%	%Limit model
%	\pgfmathsetmacro{\rboundvv}{2*ln(\r-\uu)-\vv}
%	\pgfmathsetmacro{\lboundvv}{2*ln(\r+\uu)-\vv}
%
%    
%    %Hyperbolic model
%	\pgfmathsetmacro{\rhboundvv}{1.76}
%	\pgfmathsetmacro{\lhboundvv}{\R-\vv}
%
%	%Intersection
%	\pgfmathsetmacro{\regionright}{\uu+exp((\vv+\rboundvv)/2)}
%	\pgfmathsetmacro{\intersection}{0.6}
%	
%	\draw[red,line width=1pt,pattern=north west lines, pattern color=red] 
%		plot[domain=\intersection:\rboundvv,smooth,variable=\y,red] ({\uu+exp((\vv+\y)/2)},{\y}) 
%		--
%		(\regionright,\rboundvv) 
%		--
%		(\regionright,\rhboundvv)
%		--
%		plot[domain={\rhboundvv-0.01}:\intersection,smooth,variable=\y,red] ({\uu+(exp(\R/2)/2)*rad(acos((cosh(\R-\vv)*cosh(\R-\y)-cosh(\R))/(sinh(\R-\vv)*sinh(\R-\y))))},{\y});
%		
%	\draw[red,line width=1pt,pattern=north west lines, pattern color=red] 
%		plot[domain=0:\intersection,smooth,variable=\y,red] ({\uu+exp((\vv+\y)/2)},{\y}) 
%		--
%		plot[domain=\intersection:0,smooth,variable=\y,red] ({\uu+(exp(\R/2)/2)*rad(acos((cosh(\R-\vv)*cosh(\R-\y)-cosh(\R))/(sinh(\R-\vv)*sinh(\R-\y))))},{\y});
%
%	%Limit model
%	%Right boundary
%	\draw[domain=0:\rboundvv,smooth,variable=\y,blue, dotted, line width=1pt] plot ({\uu+exp((\vv+\y)/2)},{\y});
%
%    %Hyperbolic model
%    %Right boundary
%	\draw[domain=0:\rhboundvv,variable=\y, samples=200, blue, line width=1pt] plot 		
%		({\uu+(exp(\R/2)/2)*rad(acos((cosh(\R-\vv)*cosh(\R-\y)-cosh(\R))/(sinh(\R-\vv)*sinh(\R-\y))))},{\y});
%
%
%\end{tikzpicture}
%\end{figure}



\subsection{Equivalence clustering $G_{\HP,n}(\alpha,\nu)$ and $G_{\Pcal,n}(\alpha,\nu)$}\label{ssec:coupling_HP_ast_P}

We are going to show the following lemma: 
\begin{lemma} \label{lem:equivalence_c_HP_P}
Let $k_n$ be an increasing sequence of positive integers such that 
$k_n= O(n^{\frac{1}{2\alpha +1}})$. Then
\[
	\lim_{n \to \infty} s_\alpha(k_n)^{-1} \, \Exp{\left|c_{\HP,n}^\ast(k_n) - c_{\Pcal,n}^\ast(k_n)\right|} = 0.
\]
%\begin{enumerate}
%\item If $1/2 < \alpha \leq 3/4$, then
%$$ \lim_{n\to \infty} k_n^{4\alpha -2}\cdot \Exp{\left|  c_{\H}^\ast(k_n) - c_\Pcal^\ast(k_n)\right|}=0. $$
%\item If $\alpha = 3/4$, then 
%$$ \lim_{n\to \infty} \frac{k_n}{\log k_n} \cdot \Exp{\left|  c_{\H}^\ast(k_n) - c_\Pcal^\ast(k_n)\right|}=0.$$
%\item If $3/4 < \alpha$, then
%$$ \lim_{n\to \infty} k_n \cdot \Exp{\left|  c_{\H}^\ast(k_n) - c_\Pcal^\ast(k_n)\right|}=0. $$
%\end{enumerate}
\end{lemma}

Note that for $\alpha > 3/4$, $s_{3/4}(k_n) = \log(k_n)^{-1} s_{\alpha}(k_n) = \smallO{s_{\alpha}(k_n)}$ and hence it suffices to prove the following two cases:
\begin{enumerate}
\item if $1/2 < \alpha \leq 3/4$, then
\[
	\lim_{n\to \infty} k_n^{4\alpha -2}\cdot \Exp{\left|  c_{\H}^\ast(k_n) - c_\Pcal^\ast(k_n)\right|}=0,
\]
\item if $3/4 < \alpha$, then
\[ 
	\lim_{n\to \infty} k_n \cdot \Exp{\left|  c_{\H}^\ast(k_n) - c_\Pcal^\ast(k_n)\right|}=0.
\]
\end{enumerate}

Recall the definition of $\Kcal_{C}(k_n)$
\[
	\Kcal_C(k_n) = \left\{p \in \R : \frac{k_n - C \kappa_n}{\xi_{\alpha,\nu}} \vee 0 \le e^{\frac{y}{2}}
	\le \frac{k_n + C \kappa_n}{\xi_{\alpha,\nu}} \wedge e^{R_n/2} \right\},
\]
with $C > 0$ and 
\[
	\kappa_n := \begin{cases}
		\log(n) &\mbox{if } k_n = \bigT{1},\\
		\sqrt{k_n \log(k_n)} &\mbox{else.}
	\end{cases}
\]

The following lemma will be frequently used in the proof of Lemma~\ref{lem:equivalence_c_HP_P}
\begin{lemma} \label{eq:gamma_approx}
Let $s, t, r \in \mathbb{R}$ be fixed and let $\lambda_y = \Mu{\BallPo{0,y}}$. Then for any sequence $k_n$ of positive integers with $k_n = \bigO{n^{\frac{1}{2\alpha + 1}}}$ and $C > 0$ large enough,
\[
	\int_{\Kcal_{C}(k_n)} e^{ys} e^{-\lambda_y} \frac{\lambda_y^{k_n-r}}{(k_n-t)!}e^{-\alpha y} \dd y =
	\bigO{1} \, \frac{\Gamma (k_n- (2\alpha +r -2s))}{\Gamma(k_n-t+1)} = \bigO{1} \, k_n^{-2\alpha+t-r-1+ 2s},
\]
as $n \to \infty$.
\end{lemma}
\begin{proof}
%Note that for $y \in \Kcal_{C}$ 
%\[
%	e^{ys} e^{-\lambda_y} \frac{\lambda_y^{k_n-r}}{(k_n-t)!}e^{-\alpha y} = \bigO{k_n^{2} \frac{\Gamma(k_n + 1)}{\Gamma(k_n +1 - t)}} \hat{\rho}_n(y,k_n)
%\]
To evaluate this integral we perform a change of variable setting $z=\lambda_y$. 
We have $dz =\frac{1}{2} \lambda_y dy$ and therefore 
\begin{equation} \label{eq:gamma_approx_I}
 \int_{I_\eps (k_n)} e^{ys} \cdot e^{-\lambda_y} \cdot \lambda_y^{k_n-r} \cdot \lambda_y^{-2\alpha}  dy = O(1) \cdot \int_{k_n^{1-\eps}}^{k_n^{1+\eps}} e^{-z} \cdot z^{k_n-1-r-2\alpha+2s}  dz. 
 \end{equation}
Since $k_n\to \infty$, then as $n\to \infty$ we have 
\begin{equation} \label{eq:gamma_approx_II}
\int_{k_n^{1-\eps}}^{k_n^{1+\eps}} e^{-z} \cdot z^{k_n-(2\alpha+1+r-2s)}  dz / \Gamma (k_n-(2\alpha+r -2s)) \to 1.
\end{equation}
The claim follows from Stirling's formula.
\end{proof}



\begin{proof}[Proof of Lemma~\ref{lem:equivalence_c_HP_P}] 
To keep notations short we abbreviate $\Exp{N_{\H} (k_n)}$ and $\Exp{N_{\Pcal} (k_n)}$ by $\expH$ and $\expP$, respectively.
Then we have
\begin{align*} 
	&\Exp{\left|  c_{\H}^\ast(k_n) - c_\Pcal^\ast(k_n)\right|}= 
	\binom{k_n}{2}^{-1}\Exp{\left|\sum_{p \in \Pcal} 
    	\frac{\ind{D_\H(p) = k_n}}{\expH} \Delta_\H(p )
        - \frac{\ind{D_\Pcal(p) = k_n}}{\expP}  \Delta_\Pcal(p)\right|} \\
    &\le\binom{k_n}{2}^{-1} \expH^{-1} \Exp{\left|\sum_{p \in \Pcal} \ind{D_\H((0,y)) = k_n} \Delta_\H(p) 
    	- \ind{D_\Pcal(p) = k_n} \Delta_\Pcal(p)\right|} \\
    &\hspace{10pt}+ \binom{k_n}{2}^{-1} \left|\frac{1}{\expH} - \frac{1}{\expP}\right|\Exp{
        	\sum_{p \in \Pcal} \ind{D_\Pcal(p) = k_n} \Delta_\Pcal(p) }
\end{align*}
The last term can be rewritten as
\begin{align*}
	\left|1 - \frac{\expH}{\expP}\right| \Exp{c_\Pcal^\ast(k_n)} = \left|1 - \frac{\expH}{\expP}\right|c_\infty(k_n)(1+o(1)),
\end{align*}
where we used Proposition~\ref{prop:convergence_average_clustering_P_n}. The first term in this product converges to zero by Lemma~\ref{lem:difference_N_HP_P} while the second term scales as $s_\alpha(k_n)$ by Theorem~\ref{thm:asymptotics_average_clustering_P}. Hence
\[
	\left|1 - \frac{\expH}{\expP}\right| \Exp{c_\Pcal^\ast(k_n)} = \smallO{s_\alpha(k_n)},
\]
and therefore we are left to analyze the other term. By the Campbell-Mecke formula~\eqref{eq:def_Campbell-Mecke} we have that
\begin{align*}
	    &\Exp{\left|\sum_{p \in \Pcal} \ind{D_\H((0,y)) = k_n} \Delta_\H(p) 
	        	- \ind{D_\Pcal(p) = k_n} \Delta_\Pcal(p)\right|} \\
	    &= \int_{-I_n}^{I_n} \int_0^{R_n} 
	        \Exp{\left|\ind{D_\H((0,y)) = k_n} \Delta_\H ((0,y) ) - \ind{D_\Pcal((0,y)) = k_n} \Delta_\Pcal ((0,y))\right|} 
	        	e^{-\alpha y} \dd y \dd x.
\end{align*}
Since 
\begin{align*}
	\Exp{\frac{\ind{D_\H((0,y)) = k_n}}{\Exp{N_\H(k_n)}} \Delta_\H ((0,y) )}
	&\le \binom{k_n}{2} \rho_{\HP,n}(y,k_n)\expH^{-1} \\
	&= \binom{k_n}{2} \rho_{\HP,n}(y,k_n)\bigT{\expP^{-1}}\\
	&= \bigT{n^{-1} k_n^{2\alpha + 3}}\rho_{\HP,n}(y,k_n)
\end{align*}
and similar for the other term, it follows that
\begin{align*}
	&\hspace{-60pt}\Exp{ \left| \frac{\ind{D_\H((0,y)) = k_n}}{\Exp{N_\H(k_n)}} \Delta_\H ((0,y) )
		- \frac{\ind{D_\Pcal((0,y)) = k_n}}{\Exp{N_\Pcal(k_n)}}  \Delta_\Pcal ((0,y))\right|} \\
	&\le \bigT{n^{-1} k_n^{2\alpha + 3}}\left(\rho_{\HP,n}(k,n) + \rho_n(y,k_n)\right).
\end{align*}
Therefore, by a concentration argument, it is enough to consider the integral
\begin{equation} \label{eq:expectation_total}
	\int_{\Kcal_{C}(k_n)} \Exp{\left|\ind{D_\H((0,y)) = k_n} \Delta_\H ((0,y) ) 
		- \ind{D_\Pcal((0,y)) = k_n} \Delta_\Pcal ((0,y))\right|} e^{-\alpha y} \dd y \dd x.
\end{equation}

We will first expand the integrand. We write $D_{\H}(y,k_n;\Pcal)$ for the indicator 
which is equal to 1 if and only if $\BallHyp{(0,y)}$ contains $k_n$ points from 
$\Pcal \setminus \{(0,y)\}$. We define $D_{\Pcal}(y,k_n;\Pcal)$ analogously for the 
ball $\BallPo{(0,y)}$. 
Again the Campbel-Mecke formula~\eqref{eq:Campbell-Mecke} yields
\begin{align*} 
 &\Exp{ \left| \frac{\ind{D_\H((0,y)) = k_n}}{\expH} \Delta_\H ((0,y) )
        - \frac{\ind{D_\Pcal((0,y)) = k_n}}{\expP}  \Delta_\Pcal ((0,y))
        \right|}\leq \\
 & {\mathbb E} \left[ \sum_{p_1,p_2 \in \Pcal \setminus \{(0,y)\}}^{\not =} 
  \left| D_{\H}(y,k_n-2; \Pcal \setminus \{ (0,y),p_1,p_2 \}) \expH^{-1} \Delta_\H ((0,y),p_1,p_2) \right. \right.\\
  & \hspace{5cm} 
\left. \left. -   D_{\Pcal} (y,k_n-2;\Pcal \setminus \{(0,y),p_1,p_2\}) \expP^{-1} \Delta_{\Pcal} ((0,y),p_1,p_2)
   \right| \right],
\end{align*}
where the sum ranges over all distinct pairs of points in $\Pcal \setminus \{ (0,y)\}$.
In what follows, we will set $\BallSym{p'} = \BallHyp{p'} \bigtriangleup \BallPo{p'}$ 
and $\BallInter{p'} =\BallHyp{p'} \cap \BallPo{p'}$. 
%and let $\hsym{p',(0,y);\Pcal }:= |\BallSym{p'} \cap \BallPo{(0,y)} \cap \Pcal|.$ 
We will now bound the sum that is inside the expectation. 
Note that each summand is the absolute value of the difference between two quantities  
that are either equal to 0 or of order $\expH^{-1}$ ($\expP^{-1}$).
We will split these summands into 6 classes. All but the last one are combinations of 
$p_1, p_2\in \Pcal \setminus \{(0,y)\}$ for which only one of the two terms of this difference
is non-zero. 
\begin{enumerate} 
\item both $p_1$ and $p_2$ have $y(p_1),y(p_2) < (1-\eps ) R_n \wedge (R_n-y)$ and 
\begin{enumerate}
\item $p_1$ is in $\BallInter{(0,y)}$ but $p_2 \in \BallHyp{p_1} \setminus \BallPo{p_1}$ 
and $\BallHyp{(0,y)}$ contains exactly $k_n-2$ or $k_n-1$ other points (depending on whether 
$p_2 \in \BallHyp{(0,y)}$ or not).
\item $p_1$ is in $\BallInter{(0,y)}$ but $p_2 \in \BallPo{p_1} \setminus \BallHyp{p_1}$ 
and $\BallPo{(0,y)}$ contains exactly $k_n-2$ or $k_n-1$ other points (depending on whether 
$p_2 \in \BallPo{(0,y)}$ or not).
\end{enumerate}
\item the above cases but with $y(p_1) \geq (1-\eps) R_n \wedge (R_n -y)$. 
\item $y(p_1) \geq K$ and $p_1 \in \BallHyp{(0,y)} \setminus 
\BallPo{(0,y)}$ and $p_2 \in \BallInter{(0,y)}$ - here we use 
Corollary~\ref{cor:balls_inclusion} which implies that if $p_1 \in \BallSym{(0,y)}$ and $y(p_1) \geq K$, then in fact $p_1 \in \BallHyp{(0,y)} \setminus 
\BallPo{(0,y)}$. 
\item $y(p_1) < K$ and $p_1 \in \BallSym{(0,y)}$ and $p_2 \in \BallInter{(0,y)}$. 
\item $p_1$ and $p_2$ are such that $\Delta_\H ((0,y),p_1,p_2)=\Delta_\Pcal ((0,y),p_1,p_2)=1$. 
\end{enumerate}
We bound this sum by the following expression:
\begin{align} 
& \sum_{p_1,p_2 \in \Pcal \setminus \{(0,y\}}^{\not =} 
  \left| D_{\H}(y,k_n; \Pcal) \expH^{-1} \Delta_\H ((0,y),p_1,p_2) \right. \nonumber \\
& \hspace{5cm} 
 \left. -   D_{\Pcal} (y,k_n;\Pcal) \expP^{-1} \Delta_{\Pcal} ((0,y),p_1,p_2) \right| \leq \nonumber 
 \\
& \expH^{-1} \times \nonumber \\
& \sum_{p_1,p_2\in \Pcal \setminus \{(0,y)\},\  y(p_1),y(p_2)< (1-\eps)R_n \wedge (R_n-y)} \ind{p_1 \in \BallInter{(0,y)}} \cdot \ind{p_2 \in \BallSym{p_1}} \times \nonumber \\
& \hspace{2cm} 
D_\H (y,k_n-2;\Pcal \setminus \{ p_1,p_2\}) 
%+ D_\H (y,k_n-1;\Pcal \setminus \{ p_1,p_2\}) \right)  
\label{eq:sum_1}\\
&+ \expP^{-1} \times \nonumber \\
& \sum_{p_1,p_2\in \Pcal \setminus \{(0,y)\},\  y(p_1),y(p_2)< (1-\eps)R_n \wedge (R_n-y)} \ind{p_1 \in \BallInter{(0,y)}} \cdot \ind{p_2 \in \BallSym{p_1}} \times \nonumber \\
& \hspace{2cm} D_\Pcal (y,k_n-2;\Pcal \setminus \{ p_1,p_2\}) 
%+ D_\Pcal (y,k_n-1;\Pcal \setminus \{ p_1,p_2\}) \right) 
\label{eq:sum_2}  \\
& +\expH^{-1} \sum_{p_1,p_2 \ \in \Pcal \setminus \{(0,y) \}, \ y(p_1) \geq (1-\eps) R_n \wedge (R_n-y)}  \ind{p_1 \in \BallInter{(0,y)}} \cdot \ind{p_2 \in \BallSym{p_1}\cap \BallHyp{(0,y)}}
\times \nonumber \\
&\hspace{2cm} D_\H (y,k_n-2;\Pcal \setminus \{ p_1,p_2\}) \label{eq:sum_3}
\\
&+\expP^{-1} \sum_{p_1,p_2 \ \in \Pcal \setminus \{(0,y) \}, \ y(p_1) \geq (1-\eps) R_n \wedge (R_n-y)} 
 \ind{p_1 \in \BallInter{(0,y)}} \cdot \ind{p_2 \in \BallSym{p_1}\cap \BallPo{(0,y)}}\times \nonumber \\
 &\hspace{2cm} D_\Pcal (y,k_n-2;\Pcal \setminus \{ p_1,p_2\}) \label{eq:sum_4}
\\
& + 2 \expH^{-1}\times \nonumber \\
&\sum_{p_1,p_2 \in\Pcal \setminus \{(0,y)\}, \ y(p_1) \geq K}\ind{p_1\in \BallHyp{(0,y)}\setminus \BallPo{(0,y)}} \ind{p_2\in \BallHyp{(0,y)}\cap \BallPo{(0,y)}}  \times \nonumber \\
& \hspace{2cm} D_\H (y,k_n-2;\Pcal \setminus \{ p_1,p_2\}) \label{eq:sum_5}\\
&+ (\expH^{-1}+\expP^{-1}) \cdot \sum_{p_1,p_2 \in\Pcal \setminus \{(0,y)\}, \ y(p_1) < K}
\ind{p_1\in \BallSym{(0,y)}} \ind{p_2\in \BallHyp{(0,y)}\cap \BallPo{(0,y)}} \label{eq:sum_6}\\
& +  \sum_{p_1,p_2 \in \Pcal \setminus \{(0,y)\}}^{\not =} \ind{p_1,p_2 \in \BallHyp{(0,y)}\cup \BallPo{(0,y)}} \cdot 
D_{\H}(y,k_n-2; \Pcal \setminus \{ p_1,p_2\})  \cdot 
\left| \expH^{-1} - \expP^{-1} \right|.  \label{eq:sum_7}
\end{align}
In the following sections we will give upper bounds on the expected values of each one of these partial sums. 

\subsubsection{The sums of~\eqref{eq:sum_1} and~\eqref{eq:sum_2}}

We shall bound only~\eqref{eq:sum_1};~\eqref{eq:sum_2} can be bounded by the same bound, up to a multiplicative constant. 
Note first that for any two points $p_1,p_2$ the following holds: $p_1 \in \BallHyp{(0,y)}$ and $p_2 \in \BallSym{p_1}\cap \BallHyp{(0,y)}$, then $p_2 \in \BallHyp{(0,y)}$ and $p_1 \in \BallSym{p_2}\cap \BallHyp{(0,y)}$.
Using this symmetry, it suffices to consider pairs $(p_1,p_2) \in [\Pcal \setminus \{(0,y)\} ]^2$ with $0\leq y(p_2) \leq y(p_1) \leq R- y$.  
Let $\dom{}$ denote the set of these pairs. 

We are going to consider several sub-cases and, thereby, split the domain $\dom{}$ into the corresponding sub-domains. 
Let $\omega =\omega (n) \to \infty$ as $n\to \infty$ be a slowly growing function and set $y_\omega := y +\omega$. 
We let $$\dom{1} =\{(p_1,p_2) \in \dom{} \ : \ y \leq y(p_1) \leq R_n/2, y_\omega \leq y(p_2) \leq y(p_1) \},$$ 
 $$\dom{2} = \{(p_1,p_2) \in \dom{} \ : \ y(p_1) \leq R_n/2, y(p_2) \leq y_\omega \}$$
and $$\dom{3} =  \{(p_1,p_2) \in \dom{} \ : R_n/2<y(p_1) \leq R_n -y, y(p_2) \leq y(p_1) \}.$$ 
Note that $\dom{} \subseteq \dom{1} \cup \dom{2}\cup \dom{3}$.
Hence, we can write 
\begin{equation} \label{eq:1sum-rewriting}
\begin{split} 
{\mathbb E}& \left[  \sum_{p_1, p_2\in \Pcal \setminus \{(0,y)\}, \ y(p_1), y(p_2) \leq (1-\eps) R_n\wedge (R_n-y)} \ind{p_1 \in \BallHyp{(0,y)}} \cdot \ind{p_2 \in \BallSym{p_1}\cap \BallHyp{(0,y)}} \times \right.\\ 
&\left. \hspace{2.5cm}  D_\H (y,k_n-2;\Pcal \setminus \{ (0,y),p_1,p_2\})  \right] \\ 
&=  \sum_{i=1}^3 {\mathbb E} \left[  \sum_{(p_1, p_2)\in \dom{i}} \ind{p_1 \in \BallHyp{(0,y)}} \cdot \ind{p_2 \in \BallSym{p_1}\cap \BallHyp{(0,y)}} \cdot D_\H (y,k_n-2;\Pcal \setminus \{ (0,y),p_1,p_2\})  \right].
\end{split}
\end{equation}
We bound each one of the above three summands as follows:  
\begin{equation} \label{eq:term1}
\begin{split}
&{\mathbb E} \left[  \sum_{(p_1, p_2)\in \dom{1}} \ind{p_1 \in \BallHyp{(0,y)}} \cdot \ind{p_2 \in \BallSym{p_1}\cap \BallHyp{(0,y)}} \cdot D_\H (y,k_n-2;\Pcal \setminus \{ (0,y),p_1,p_2\})  \right] \leq \\
& {\mathbb E} \left[  \sum_{(p_1, p_2)\in \dom{1}} \ind{p_1 \in \BallHyp{(0,y)}} \cdot \ind{p_2 \in  \BallHyp{(0,y)}} \cdot D_\H (y,k_n-2;\Pcal \setminus \{ (0,y),p_1,p_2\})  \right],
\end{split}
\end{equation}

\begin{equation} \label{eq:term2}
\begin{split}
&{\mathbb E} \left[  \sum_{(p_1, p_2)\in \dom{2}} \ind{p_1 \in \BallHyp{(0,y)}} \cdot \ind{p_2 \in \BallSym{p_1}\cap \BallHyp{(0,y)}} \cdot D_\H (y,k_n-2;\Pcal \setminus \{ (0,y),p_1,p_2\})  \right] \leq \\
& {\mathbb E} \left[  \sum_{(p_1, p_2)\in \dom{2}} \ind{p_1 \in \BallHyp{(0,y)}} \cdot \ind{p_2 \in \BallSym{p_1}} \cdot D_\H (y,k_n-2;\Pcal \setminus \{ (0,y),p_1,p_2\})  \right]
\end{split}
\end{equation}
and 
\begin{equation}\label{eq:term3}
\begin{split}
&{\mathbb E} \left[  \sum_{(p_1, p_2)\in \dom{3}} \ind{p_1 \in \BallHyp{(0,y)}} \cdot \ind{p_2 \in \BallSym{p_1}\cap \BallHyp{(0,y)}} \cdot D_\H (y,k_n-2;\Pcal \setminus \{ (0,y),p_1,p_2\})  \right] \leq \\
&{\mathbb E} \left[  \sum_{(p_1, p_2)\in \dom{3}} \ind{p_1 \in \BallHyp{(0,y)}} \cdot \ind{p_2 \in \BallHyp{(0,y)}} \cdot D_\H (y,k_n-2;\Pcal \setminus \{ (0,y),p_1,p_2\})  \right].
\end{split}
\end{equation}
We will bound each term using the Campbell-Mecke formula~\eqref{eq:Campbell-Mecke}. 
We will use $(x_i,y_i)$ for the co-ordinates of $p_i$, for $i=1,2$.


For the term in~\eqref{eq:term1}, we get
\begin{equation} \label{eq:1sum-expansion}
\begin{split} 
{\mathbb E}& \left[  \sum_{p_1, p_2\in \dom{1}} \ind{p_1 \in \BallInter{(0,y)}} \cdot \ind{p_2 \in \BallHyp{(0,y)}} \cdot 
D_\H (y,k_n-2;\Pcal \setminus \{ (0,y),p_1,p_2\})  \right] \\
&=\int_{-I_n}^{I_n} \int_y^{R_n/2}\int_{-I_n}^{I_n} \int_{y_\omega}^{y_1}
 \ind{(x_1,y_1) \in \BallHyp{(0,y)}} \cdot 
 \ind{(x_2,y_2) \in \BallHyp{(0,y)}}\times \\
& \hspace{1.8cm} \Prob{D_\H ((0,y))=k_n-2;\Pcal \setminus \{ (0,y),(x_1,y_1), (x_2,y_2)\}}
  e^{-\alpha y_2} \cdot e^{-\alpha y_1} dy_2 dx_2dy_1 dx_1.
\end{split}
\end{equation}

%Firstly, observe that $\Prob{D_\H ((0,y))=k_n-2;\Pcal \setminus \{ (0,y),(x',y'), (x'',y'')\}}$ does not depend on $(x',y')$ and $(x'',y'')$ with probability 1. 
In particular, we have 
\begin{equation*}
\begin{split} 
\Prob{D_\H ((0,y))=k_n-2;\Pcal \setminus \{ (0,y),(x',y'), (x'',y'')\}} =\Pop{y}{k_n-2}.
\end{split}
\end{equation*}
%and 
%\begin{equation*}
%\begin{split} 
%\Prob{D_\H ((0,y))=k_n-1;\Pcal \setminus \{ (0,y),(x',y'), (x'',y'')\}} =\Pop{y}{k_n-1}.
%\end{split}
%\end{equation*}
Thus, we can extract this term out of the quadruple integral. 

In particular, recall that $\mu_{\alpha,\nu} (\BallHyp{(0,y)}) = \lambda_y$ and moreover 
$\lambda_y = O(1) \cdot e^{y/2}$. 
Hence, 
\begin{equation}\label{eq:prob_k_n-2}
\Pop{y}{k_n-2}= e^{-\lambda_y} \frac{\lambda_y^{k_n-2}}{(k_n-2)!}  
= e^{-\lambda_y} \lambda_y^{k_n-2} \frac{1}{\Gamma (k_n-1)}.  
\end{equation}
Also, Lemma~\ref{lem:asymptotics_Omega_hyperbolic} implies that for 
$y'\leq R_n -y$, we have that if 
$(x',y') \in \BallHyp{(0,y)}$, then 
$|x'| < (1+ K) e^{y/2 + y'/2}$, where $K >0$ is as in Lemma~\ref{lem:asymptotics_Omega_hyperbolic}. 
%For $y> R_n - y$, we will simply take $|x'| < I_n$. 
Using these observations, we obtain: 
\begin{equation}
\begin{split}
{\mathbb E}& \left[  \sum_{p_1, p_2\in \dom{1}} \ind{p_1 \in \BallInter{(0,y)}} \cdot \ind{p_2 \in \BallHyp{(0,y)}} \cdot 
D_\H (y,k_n-2;\Pcal \setminus \{ (0,y),p_1,p_2\})  \right] = \\
& O(1) \cdot e^{-\lambda_y} \lambda_y^{k_n-2} \frac{1}{\Gamma (k_n-1)} \cdot 
e^{y}\int_y^{R_n/2} e^{y_1/2}\int_{y_\omega}^{y_1} e^{y_2/2} e^{-\alpha y_2} \cdot e^{-\alpha y_1} dy_2 dy_1.
\end{split}
\end{equation} 
Now, the double integral becomes
\begin{equation}
\begin{split}
& \int_y^{R_n/2} e^{y_1/2}\int_{y_\omega}^{y_1} e^{y_2/2} e^{-\alpha y_2} \cdot e^{-\alpha y_1} dy_2 dy_1 = \\
&  O(1) \cdot  \int_y^{R_n/2} e^{y_1/2 - \alpha y_1} \cdot 
e^{(1/2 - \alpha) y_\omega} dy_1 \\
& =O(1) \cdot e^{(1/2 - \alpha) y_\omega} \cdot \int_y^{R_n/2} e^{y_1/2 - \alpha y_1} d y_1 \\
& =O(1) \cdot e^{(1/2 - \alpha) y_\omega + (1/2 - \alpha) y} \\ 
& \ll e^{(1 - 2\alpha) y},
\end{split}
\end{equation}
since $y_\omega = y + \omega$ and $\omega \to \infty$. 
We then deduce that 
\begin{equation}
\begin{split}
{\mathbb E}& \left[  \sum_{p_1, p_2\in \dom{1}} \ind{p_1 \in \BallInter{(0,y)}} \cdot \ind{p_2 \in \BallHyp{(0,y)}} \cdot 
D_\H (y,k_n-2;\Pcal \setminus \{ (0,y),p_1,p_2\})  \right] \\
& \ll  e^{-\lambda_y} \lambda_y^{k_n-2} \frac{1}{\Gamma (k_n-1)} \cdot 
e^{(2- 2\alpha )y} \\
&=  e^{-\lambda_y} \lambda_y^{k_n-2+4-4\alpha} \frac{1}{\Gamma (k_n-1)} 
= e^{-\lambda_y} \lambda_y^{k_n+2-4\alpha} \frac{1}{\Gamma (k_n-1)}.
\end{split}
\end{equation}
We now integrate this with respect to $y$ and determine its contribution to~\eqref{eq:expectation_total} is 
\begin{equation*} 
\begin{split}
&{k_n \choose 2}^{-1} \cdot \expH^{-1} \cdot 
\frac{1}{\Gamma (k_n-1)} \cdot \int_{-I_n}^{I_n} \int_{I_{\eps} (k_n)} e^{-\lambda_y} \lambda_y^{k_n+2-4\alpha} e^{-\alpha y} dy dx.  
\end{split}
\end{equation*}
We will show that the latter integral can be approximated by the ratio of Gamma functions. 
As this approximation will be applied several times in the sequel, we will state it is a more 
general form.  
\begin{proposition} \label{eq:gamma_approx}
Let $s, t, r \in \mathbb{R}$ be fixed. Then 
$$\int_{I_\eps (k_n)}e^{ys}
e^{-\lambda_y} \frac{\lambda_y^{k_n-r}}{(k_n-t)!}e^{-\alpha y} dy =O(1) \cdot 
\frac{\Gamma (k_n- (2\alpha +r -2s))}{\Gamma(k_n-t+1)} =O(1) \cdot 
k_n^{-2\alpha+t-r-1+ 2s}.
 $$
\end{proposition}
\begin{proof}[Proof of Proposition~\ref{eq:gamma_approx}]
To evaluate this integral we perform a change of variable setting $z=\lambda_y$. 
We have $dz =\frac{1}{2} \lambda_y dy$ and therefore 
\begin{equation} \label{eq:gamma_approx_I}
 \int_{I_\eps (k_n)} e^{ys} \cdot e^{-\lambda_y} \cdot \lambda_y^{k_n-r} \cdot \lambda_y^{-2\alpha}  dy = O(1) \cdot \int_{k_n^{1-\eps}}^{k_n^{1+\eps}} e^{-z} \cdot z^{k_n-1-r-2\alpha+2s}  dz. 
 \end{equation}
Since $k_n\to \infty$, then as $n\to \infty$ we have 
\begin{equation} \label{eq:gamma_approx_II}
\int_{k_n^{1-\eps}}^{k_n^{1+\eps}} e^{-z} \cdot z^{k_n-(2\alpha+1+r-2s)}  dz / \Gamma (k_n-(2\alpha+r -2s)) \to 1.
\end{equation}
The claim follows from Stirling's formula.
\end{proof}
Therefore, 
\begin{equation*} 
\begin{split}
&{k_n \choose 2}^{-1} \cdot \expH^{-1} \cdot 
\frac{1}{\Gamma (k_n-1)} \cdot \int_{-I_n}^{I_n} \int_{I_{\eps} (k_n)} e^{-\lambda_y} \lambda_y^{k_n+2-4\alpha} e^{-\alpha y} dy dx  \\ 
&=O(1) \cdot \expH^{-1} \cdot n \cdot  
 \frac{ \Gamma (k_n-(6\alpha-2))}{\Gamma (k_n+1)}.
\end{split}
\end{equation*}
But $\expH = \Theta (1) \cdot n \cdot k_n^{-(2\alpha +1)}$ and by Stirling's formula, we have 
$\frac{ \Gamma (k_n-(6\alpha-2))}{\Gamma (k_n+1)} \sim \frac{1}{k_n^{6\alpha-1}}$ 
Thus, the above becomes: 
\begin{equation*} 
\begin{split}
&{k_n \choose 2}^{-1} \cdot \expH^{-1} \cdot 
\frac{1}{\Gamma (k_n-1)} \cdot \int_{-I_n}^{I_n} \int_{I_{\eps} (k_n)} e^{-\lambda_y} \lambda_y^{k_n+2-6\alpha} e^{-\alpha y} dy dx  \\ 
&=O(1) \cdot n^{-1} k_n^{2\alpha +1} \cdot n \cdot \frac{1}{k_n^{6\alpha-1}} = O(1) \cdot  k_n^{-4\alpha +2}.
\end{split}
\end{equation*}
We deduce that 
\begin{equation*}
\begin{split}
{k_n \choose 2}^{-1} \cdot \expH^{-1} \cdot 
\frac{1}{\Gamma (k_n-1)} \cdot \int_{-I_n}^{I_n} \int_{I_{\eps} (k_n)}  I_1 (y) e^{-\alpha y} dy dx \ll k_n^{-4\alpha+2}.
\end{split}
\end{equation*}
Now, for $1/2 < \alpha \leq 3/4$, we have
$$k_n^{4\alpha -2} \cdot \left[{k_n \choose 2}^{-1} \cdot \expH^{-1} \cdot 
\frac{1}{\Gamma (k_n-1)} \cdot \int_{-I_n}^{I_n} \int_{I_{\eps} (k_n)}  I_1 (y) e^{-\alpha y} dy dx \right] =o(1),$$
and for $\alpha >3/4$ we have 
$$k_n \cdot \left[{k_n \choose 2}^{-1} \cdot \expH^{-1} \cdot 
\frac{1}{\Gamma (k_n-1)} \cdot \int_{-I_n}^{I_n} \int_{I_{\eps} (k_n)}  I_1 (y) e^{-\alpha y} dy dx \right] \ll 
k_n^{-4\alpha + 3} \stackrel{\alpha >3/4}{=}o(1).$$

We will now bound the term in~\eqref{eq:term2}. 
We get
\begin{equation} \label{eq:1sum-expansion}
\begin{split} 
{\mathbb E}& \left[  \sum_{p_1, p_2\in \dom{2}} \ind{p_1 \in \BallInter{(0,y)}} \cdot \ind{p_2 \in \BallSym{(0,y)}} \cdot 
D_\H (y,k_n-2;\Pcal \setminus \{ (0,y),p_1,p_2\})  \right] \\
&=\int_{-I_n}^{I_n} \int_0^{R_n/2}\int_{-I_n}^{I_n} \int_0^{y_\omega}
 \ind{(x_1,y_1) \in \BallHyp{(0,y)}} \cdot 
 \ind{(x_2,y_2) \in \BallSym{(0,y)}}\times \\
& \hspace{1.8cm} \Prob{D_\H ((0,y))=k_n-2;\Pcal \setminus \{ (0,y),(x_1,y_1), (x_2,y_2)\}}
  e^{-\alpha y_2} \cdot e^{-\alpha y_1} dy_2 dx_2dy_1 dx_1.
\end{split}
\end{equation}

Now, Lemma~\ref{lem:asymptotics_Omega_hyperbolic} implies that for 
$y_2 \leq R_n -y_1$, we have that if 
$(x_2,y_2) \in \BallSym{(x_1,y_1)}$, then $x_2$ lies in an interval of length 
$Ke^{3 y/2 + 3y'/2 - R_n}$, where $K >0$ is again the constant in Lemma~\ref{lem:asymptotics_Omega_hyperbolic}. 
%For $y> R_n - y$, we will simply take $|x'| < I_n$. 
Using these observations together with~\eqref{eq:prob_k_n-2} , we obtain: 
\begin{equation} \label{eq:term2_intermediate}
\begin{split}
{\mathbb E}& \left[  \sum_{p_1, p_2\in \dom{2}} \ind{p_1 \in \BallInter{(0,y)}} \cdot \ind{p_2 \in \BallHyp{(0,y)}} \cdot 
D_\H (y,k_n-2;\Pcal \setminus \{ (0,y),p_1,p_2\})  \right] = \\
& O(1) \cdot e^{-\lambda_y} \lambda_y^{k_n-2} \frac{1}{\Gamma (k_n-1)} \cdot 
e^{y/2}\int_0^{R_n/2} e^{y_1/2 + 3y_1/2}\int_0^{y_\omega} e^{3y_2/2 - R_n} e^{-\alpha y_2} \cdot e^{-\alpha y_1} dy_2 dy_1. 
\end{split}
\end{equation} 
Now, if $\alpha < 3/2$ the latter integral becomes
\begin{equation*}
\begin{split}
& e^{-R_n} \int_0^{R_n/2} e^{2 y_1}\int_0^{y_\omega} e^{3y_2/2} e^{-\alpha y_2} \cdot e^{-\alpha y_1} dy_2 dy_1 = \\
&  O(1) \cdot e^{-R_n} \cdot  \int_0^{R_n/2} e^{(2-\alpha )y_1} \cdot 
e^{(3/2 - \alpha) y_\omega} dy_1 \\
& =O(1) \cdot e^{(3/2 - \alpha) y_\omega - R_n} \cdot \int_0^{R_n/2} e^{(2-\alpha) y_1} d y_1 \\
& =O(1) \cdot e^{(3/2 - \alpha) y_\omega - R_n+ (1 - \alpha/2) R_n } 
=O(1) \cdot e^{(3/2 - \alpha) y_\omega - \alpha R_n/2}. 
\end{split}
\end{equation*}
Substituting this into~\eqref{eq:term2_intermediate}, and using that 
$y_\omega = y + \omega$ and $\omega \to \infty$
we obtain: 
\begin{equation*}
\begin{split}
{\mathbb E}& \left[  \sum_{p_1, p_2\in \dom{2}} \ind{p_1 \in \BallInter{(0,y)}} \cdot \ind{p_2 \in \BallHyp{(0,y)}} \cdot 
D_\H (y,k_n-2;\Pcal \setminus \{ (0,y),p_1,p_2\})  \right] = \\
& \omega(1) \cdot e^{-\lambda_y} \lambda_y^{k_n-2} \frac{1}{\Gamma (k_n-1)} \cdot 
e^{y/2+(3/2 - \alpha) y - \alpha R_n/2} \\
&= \omega(1) \cdot n^{-\alpha} \cdot e^{-\lambda_y} \lambda_y^{k_n-2} \frac{1}{\Gamma (k_n-1)} \cdot e^{(2 - \alpha) y} 
\end{split}
\end{equation*}
We then proceed with the integration of this with respect to $y$ and determine its contribution to~\eqref{eq:expectation_total} is 
\begin{equation*} 
\begin{split}
\omega(1) \cdot n^{-\alpha} \cdot {k_n \choose 2}^{-1} \cdot \expH^{-1} \cdot 
\frac{1}{\Gamma (k_n-1)} \cdot \int_{-I_n}^{I_n} \int_{I_{\eps} (k_n)} e^{(2-\alpha )y} e^{-\lambda_y} \lambda_y^{k_n-2} e^{-\alpha y} dy dx.  
\end{split}
\end{equation*}
By Proposition~\ref{eq:gamma_approx}, we deduce that 
\begin{equation*}
\begin{split}
{k_n \choose 2}^{-1} & \cdot 
\frac{1}{\Gamma (k_n-1)} \cdot \int_{-I_n}^{I_n} \int_{I_{\eps} (k_n)} e^{(2-\alpha )y} e^{-\lambda_y} \lambda_y^{k_n-2} e^{-\alpha y} dy dx \\
&= O(1)\cdot  \frac{\Gamma (k_n - (2\alpha + 2 -2(2-\alpha)))}{\Gamma (k_n+1)} 
= O(1)\cdot  \frac{\Gamma(k_n - (4\alpha - 2))}{\Gamma (k_n+1)} = O(1) \cdot k_n^{-4\alpha +1}
\end{split}
\end{equation*}
using Stirling's formula at the last step. 
Recall also $\expH = \Theta (1) \cdot n \cdot k_n^{-(2\alpha +1)}$. 
Thus, the above becomes: 
\begin{equation*} 
\begin{split}
&{k_n \choose 2}^{-1} \cdot \expH^{-1} \cdot 
\frac{1}{\Gamma (k_n-1)} \cdot \int_{-I_n}^{I_n} \int_{I_{\eps} (k_n)} e^{(2-\alpha )y} e^{-\lambda_y} \lambda_y^{k_n-2} e^{-\alpha y} dy dx \\ 
&=O(1) \cdot k_n^{2\alpha +1 - 4\alpha +1} = O(1) \cdot k_n^{-2\alpha +2}. 
\end{split}
\end{equation*}
We deduce that 
\begin{equation*}
\begin{split}
{k_n \choose 2}^{-1} \cdot \expH^{-1} \cdot 
\frac{1}{\Gamma (k_n-1)} \cdot \int_{-I_n}^{I_n} \int_{I_{\eps} (k_n)}  I_2 (y) e^{-\alpha y} dy dx = \omega(1) \cdot n^{-\alpha} k_n^{-2\alpha+2}.
\end{split}
\end{equation*}
Now, for $1/2 < \alpha \leq 3/4$, we have
$$k_n^{4\alpha -2} \cdot \left[{k_n \choose 2}^{-1} \cdot \expH^{-1} \cdot 
\frac{1}{\Gamma (k_n-1)} \cdot \int_{-I_n}^{I_n} \int_{I_{\eps} (k_n)}  I_2 (y) e^{-\alpha y} dy dx \right] =\omega(1) \cdot n^{-\alpha} \cdot k_n^{2\alpha}.$$
But $k_n^2 \ll n^{\frac{2}{2\alpha +1}}  \ll n$, whereby the above is $o(1)$, if $\omega (1)$ is 
sufficiently slowly growing (depending only on $\alpha$).  
For $\alpha >3/4$ we have 
$$k_n \cdot \left[{k_n \choose 2}^{-1} \cdot \expH^{-1} \cdot 
\frac{1}{\Gamma (k_n-1)} \cdot \int_{-I_n}^{I_n} \int_{I_{\eps} (k_n)}  I_2 (y) e^{-\alpha y} dy dx \right] 
=\omega(1) \cdot n^{-\alpha} k_n^{3-2\alpha}.$$
But $3 -2\alpha < 3/2$, whereby $k_n^{3-2\alpha} \ll k_n^{3/2} = (k_n^2 )^{3/4}$. 
So 
$$ n^{-\alpha} k_n^{3-2\alpha} \\ \frac{(k_n^2)^{3/4}}{n^{\alpha}} \stackrel{\alpha >3/4}{\ll} \left( \frac{k_n^2}{n}\right)^{3/4} =o(1).$$

We will now consider the term in~\eqref{eq:term3}. 
Recall that $\dom{3}$ consists of all pairs $(p_1, p_2 ) \in \dom{}$ such that $R_n/2 < y(p_1) \leq (1-\eps)R_n \wedge (R_n-y)$ and $y(p_2) \leq y_\omega$ with the property that 
$p_1 \in \BallHyp{(0,y)}$ and $p_2 \in \BallSym{p_1} \cap \BallHyp{(0,y)}$.    
So, in particular, $p_2 \in (\BallHyp{p_1} \cup \BallPo{p_1}) \cap \BallHyp{(0,y)}$.

We will consider this intersection more closely. We use Lemma~\ref{lem:asymptotics_Omega_hyperbolic} to define a ball around $p_1$ that contains both 
$\BallHyp{p_1}$ and $\BallPo{p_1}$.
For $K > 0$, we define, for any point $p_1=(x_1,y_1) \in \R \times \R_+$,
\begin{equation}\label{eq:def_fatball}
	\FatBallHyp{p_1} : = \{ (x',y') \ : \  y' < R_n - y_1, \ | x_1 - x'| < (1+K) e^{\frac{1}{2} (y_1 + y')}  \}.
\end{equation}
%Note that $\Delta (r(p),r') = \frac12 e^{R/2} \theta_R (p)$.
It is an implication of Lemma~\ref{lem:asymptotics_Omega_hyperbolic}  that 
\begin{equation*} %\label{eq:ball_inclusion} 
(\BallHyp{p_1} \cup \BallPo{p_1}) \cap \Rcal([0, R_n - y_1]) \subseteq \FatBallHyp{p_1}
\end{equation*}
Therefore, any point $p_2 = (x_2,y_2) \in \BallSym{p_1} \cap \BallHyp{(0,y)}$ with 
$y_2 \leq R-y_1$ must belong to $\FatBallHyp{p_1} \cap \FatBallHyp{(0,y)}$.
%and 
%\begin{equation*} %\label{eq:ball_inclusion_lower}
%\BallHyp{p_1} \cap \Rcal ([R_n- y(p_1),R_n]) = \Rcal ([R_n-y(p_1),R_n]).
%\end{equation*}
%We thus conclude that 
%\begin{equation} \label{eq:hyperbolic_ball_inclusion}
%\BallSym{p_1} \subseteq \FatBallHyp{p_1}.
%\end{equation}
%In turn, these imply that 
%\begin{equation}
%\BallSym{p_1} \cap \BallHyp{(0,y)} \subseteq \FatBallHyp{p_1} \cap \FatBallHyp{(0,y)}.
%\end{equation} 
We will use this in order to derive a lower bound on $y_2$ as a function of $x_1, y_1$. 
Let us suppose without loss of generality that $x_1 < 0$. 
The left boundary of $\FatBallHyp{(0,y)}$ is given by the equation 
$x^\prime = (1+K)e^{\frac{1}{2} (y + y^\prime)}$ whereas the right boundary of $\FatBallHyp{p_1}$ is given by the curve having equation $x^\prime = x_1 + (1+ K)e^{\frac{1}{2} (y_1 + y^\prime)}.$
The equation that determines the intersection point $(\hat{x},\hat{y})$ of these curves  is
\[
	x_1 + (1+K)e^{(y_1 + \hat{y})/2}= (1+K) e^{(y + \hat{y})/2}.
\]
We can solve the above for $\hat{y}$  
\begin{equation*} 
\begin{split}
|x_1| &=(1+K) e^{\hat{y}/2} \left( e^{y_1/2} + e^{y/2} \right).
\end{split}
\end{equation*}
But $y_1 > R_n/2$ and $y< (1+\eps) R_n /(2\alpha +1)$. So if $\eps$ is small enough depending on $\alpha$, we have 
$$ |x_1| =(1+K) e^{\hat{y}/2} \left( e^{y_1/2} + e^{y/2} \right) = (1+K+o(1))e^{\hat{y}/2 + y_1/2}. $$
Let $c_K$ denote the multiplicative term $1+ K+o(1)$, which appears in the above.
The above yields
\begin{equation} \label{eq:to_use_I}
\hat{y}= \left(2 \log(|x_1|e^{-y_1/2}) - \log c_K \right) \vee 0 := c(x_1,y_1). 
\end{equation}
In particular, note that $\hat{y} = 0$ if and only if $|x_1| \leq c_K e^{y_1/2}$.  
Moreover, since $p_1 \in \BallHyp{(0,y)}$ and $x_1 \leq R_n - y$, we also have that 
$|x_1| \leq e^{(y+y_1)/2} (1+o(1))$. This upper bound on $|x_1|$ together with~\eqref{eq:to_use_I}, imply that for $n$ sufficiently large, we have $\hat{y} \leq y$. This observation will be used below, where 
we integrate over $y_2$, thus ensuring that the integrals are non-zero. 

We conclude that 
\begin{equation*}\label{eq:intersex_approx}
	p^\prime \in \FatBallHyp{(0,y)}\cap \FatBallHyp{(x_1,y_1)} \Rightarrow y^\prime \ge c(x_1,y_1),
\end{equation*}

Therefore, with $p_2 = (x_2,y_2)$ we have 
\begin{equation} \label{eq:symdiff_loc}
\begin{split} 
 \ind{(x_2,y_2) \in \BallSym{p_1}\cap \BallHyp{(0,y)}} \leq \ind{y_2 \geq c(x_1,y_1), (x_2,y_2) \in \FatBallHyp{(0,y)}}.
\end{split}
\end{equation}
Therefore, if we integrate this over $x_2, y_2$ we get 
\begin{equation*}
\begin{split}
&\int_{-I_n}^{I_n} \int_{0}^{y_\omega}  \ind{(x_2,y_2) \in \BallSym{p_1}\cap \BallHyp{(0,y)}}  
e^{-\alpha y_2} dy_2 dx_2\\
&  \leq 
\int_{-I_n}^{I_n} \int_{0}^{y_\omega}  \ind{y_2 \geq c(x_1,y_1), (x_2,y_2) \in \FatBallHyp{(0,y)}}  
e^{-\alpha y_2} dy_2 dx_2 
\leq 
(1+K) \cdot e^{y/2} \int_{c(x_1,y_1)}^{y_\omega} e^{y_2/2 - \alpha y_2} dy_2 \\
&=O(1) \cdot e^{y/2 + (1/2 -\alpha) c(x_1,y_1)}.
\end{split}
\end{equation*}
Note also that 
$$ {\mathbb E} \left[ D_\H (y,k_n-2;\Pcal \setminus \{ (0,y),p_1,p_2\}) \right] =e^{-\lambda_y} \lambda_y^{k_n-2} \frac{1}{\Gamma (k_n-1)},$$
uniformly over all $(p_1, p_2) \in \dom{3}$. 

So the Campbell-Mecke formula yields: 
\begin{equation}
\begin{split}
&{\mathbb E} \left[  \sum_{(p_1, p_2)\in \dom{3}} \ind{p_1 \in \BallHyp{(0,y)}} \cdot \ind{p_2 \in \BallSym{p_1}\cap \BallHyp{(0,y)}} \cdot D_\H (y,k_n-2;\Pcal \setminus \{ (0,y),p_1,p_2\})  \right] = \\
&O(1)\cdot e^{-\lambda_y}  \frac{\lambda_y^{k_n-2}}{\Gamma (k_n-1)}  \cdot e^{y/2} \int_{-I_n}^{I_n} \int_{R_n/2}^{(R_n - y) \wedge (1-\eps)R_n}  
\ind{p_1 \in \BallHyp{(0,y)}}
e^{(1/2 -\alpha) c(x_1,y_1) - \alpha y_1} dy_1dx_1 \\
&= O(1) \cdot e^{-\lambda_y}  \frac{\lambda_y^{k_n-2}}{\Gamma (k_n-1)}\cdot e^{y/2} \int_{-I_n}^{I_n} \int_{R_n/2}^{(R_n - y) \wedge (1-\eps)R_n}  
\ind{p_1 \in \FatBallHyp{(0,y)}}
e^{(1/2 -\alpha) c(x_1,y_1) - \alpha y_1} dy_1dx_1.
\end{split}
\end{equation}

%We will consider the points of $\Pcal_{\alpha, \nu}$ that are contained in $\BallHyp{p_n}$ but not in $\BallPo{p_n}$ and in particular the number of ordered pairs of points $(p^\prime,p^{\prime \prime})$ with $p^\prime \in \BallHyp{p} \setminus \BallPo{p}$ and $p^{\prime \prime} \in \BallHyp{p^\prime}$. 


%To obtain an upper bound on the expectation of $|\EdgeDiff|$ we shall make use of~\eqref{eq:hyperbolic_ball_inclusion}. More specifically, we will apply~\eqref{eq:Campbell-Mecke} to the Poisson point process $\Pcal_{\alpha,\nu}$ with 
%\[
%	h(p^\prime, \Pcal_{\alpha,\nu}) = \ind{p^\prime \in \Psi (\BallHyp{p_n})\setminus \BallPo{p_n}} \cdot
%    |\Pcal_{\alpha,\mu} \cap \left( \FatBallHyp{p^\prime} \cap \FatBallHyp{p_n} \right)|.
%\]

%To calculate the expectation of the above function we need to approximate the 
%intersection of the two balls $\FatBallHyp{(0,y)}$ and $\FatBallHyp{p_1}$.
%
% 
%
%???
%whereby 
%\begin{equation*}
%\begin{split}
% \mu (\FatBallHyp{(0,y)}\cap \FatBallHyp{(x_1,y_1)}) &= O(1) \cdot e^{y/2} 
%\int_{c(x_1,y_1)}^{R_n - y_1}  e^{y'/2 - \alpha y'} dy' \\
%& = O(1) \cdot e^{y/2 + c(x_1,y_1) (1/2 -\alpha)}.
%\end{split}
%\end{equation*}
%Also, 
%\begin{equation*}
%\mu () = O(n) \cdot \int_{R_n -y_1}^{R_n} e^{(1/2-\alpha)y'} dy' =   
%\end{equation*}
%
Now, we integrate this quantity over $x_1$: 
$$O(1) \cdot e^{y/2} \cdot \int_0^{(1+K)e^{y/2 + y_1/2}} e^{c(x_1,y_1) (1/2 -\alpha)} dx_1. $$
We will split this integral into two parts according to the value of $c(x_1,y_1)$:
$$
\int_0^{(1+K) e^{y/2 + y_1/2}} e^{c(x_1,y_1) (1/2 -\alpha)} dx_1 = 
\int_{c_K e^{y_1/2}}^{(1+K)e^{y/2 + y_1/2}} e^{c(x_1,y_1) (1/2 -\alpha)} dx_1 + \int_0^{c_K e^{y_1/2}} dx_1.
$$
The first integral becomes: 
\begin{equation*}
\begin{split} 
&\int_{c_K e^{y_1/2}}^{(1+K)e^{y/2 + y_1/2}} e^{c(x_1,y_1) (1/2 -\alpha)} dx_1  = 
\int_{c_K e^{y_1/2}}^{(1+K)e^{y/2 + y_1/2}} e^{c(x_1,y_1)/2 (1 -2\alpha)} dx_1  \\
&= O(1)\cdot \int_{c_K e^{y_1/2}}^{(1+K)e^{y/2 + y_1/2}} x_1^{1 -2\alpha} 
e^{-\frac{y_1}{2} (1-2\alpha)} dx_1 \\
&= O(1) \cdot e^{-y_1/2 + \alpha y_1} \cdot e^{\frac{(y+y_1)}{2} 2(1-\alpha)} \\
&=O(1) \cdot e^{y_1/2 +y(1-\alpha)}.  
\end{split}
\end{equation*}
The second integral trivially gives: 
$$ \int_0^{c_K e^{y_1/2}} dx_1 = O(1) \cdot e^{y_1/2} = O(1) \cdot e^{y_1/2 +y(1-\alpha)}.$$
We conclude that 
$$ e^{y/2} \cdot \int_0^{(1+K)e^{y/2 + y_1/2}} e^{c(x_1,y_1) (1/2 -\alpha)} dx_1 = 
O(1) \cdot e^{y_1/2 +y(3/2-\alpha)}. $$
Now, we integrate this with respect to $y_1$ and get 
$$e^{y(3/2 -\alpha)} \int_{R_n/2}^{R_n-y} e^{(1/2-\alpha)y_1} dy_1 =  O(1) \cdot e^{y(3/2 -\alpha)} 
 e^{(1/2 -\alpha) R_n/2} = O(1) \cdot n^{1/2 -\alpha} \cdot e^{y(3/2 - \alpha)}.$$
\begin{equation} \label{eq:term3_intermediate}
\begin{split}
{\mathbb E}& \left[  \sum_{p_1, p_2\in \dom{3}} \ind{p_1 \in \BallInter{(0,y)}} \cdot \ind{p_2 \in 
\BallSym{p_1} \cap \BallHyp{(0,y)}} \cdot 
D_\H (y,k_n-2;\Pcal \setminus \{ (0,y),p_1,p_2\})  \right] = \\
& O(1) \cdot n^{1/2 -\alpha} \cdot e^{y(3/2 - \alpha)} \cdot e^{-\lambda_y} \lambda_y^{k_n-2} \frac{1}{\Gamma (k_n-1)}. 
\end{split}
\end{equation}
We now apply Proposition~\ref{eq:gamma_approx} and get 
\begin{equation*}
\begin{split}
&{k_n \choose 2}^{-1}\cdot  \frac{1}{\Gamma (k_n-1)} \cdot \int_{-I_n}^{I_n} \int_{I_\eps(k_n)} 
e^{y(3/2 - \alpha)} \cdot e^{-\lambda_y} \lambda_y^{k_n-2} dy  = \\
& O(n) \cdot \frac{\Gamma (k_n - (2 + 2\alpha - 3 + 2\alpha))}{\Gamma(k_n+1)} 
=O(n) \cdot \frac{1}{k_n^{4\alpha}}.
\end{split}
\end{equation*}
We multiply the above by $\expH^{-1} = \Theta (1) \cdot n^{-1} \cdot k_n^{2\alpha +1}$.
Now, if $1/2 < \alpha \leq 3/4$, then we get 
$$k_n^{4\alpha -2}  \cdot \frac{k_n^{2\alpha +1}}{k_n^{4\alpha}} \cdot \frac{1}{n^{\alpha -1/2}}= k_n^{2\alpha -1} \cdot  \frac{1}{n^{\alpha -1/2}}=\left(\frac{k_n^2}{n} \right)^{\alpha -1/2} =o(1).$$
For $\alpha > 3/4$, we have  
$$k_n \cdot \frac{k_n^{2\alpha +1}}{k_n^{4\alpha}} \cdot \frac{1}{n^{\alpha -1/2}}= k_n^{-2\alpha +2} \cdot  \frac{1}{n^{\alpha -1/2}}.$$
If $\alpha\geq 1$, then clearly this is $o(1)$. Suppose now that $3/4 < \alpha <1$ and recall that
$k_n = O(1) \cdot n^{1/(2\alpha +1)}$. So 
$$k_n^{-2\alpha +2} \cdot  \frac{1}{n^{\alpha -1/2}} =O(1) \cdot n^{\frac{-2\alpha +2}{2\alpha +1} - \alpha + 1/2}. $$
This exponent is negative as:
\begin{equation*}
\begin{split}
\frac{-2\alpha +2}{2\alpha +1} - \alpha + 1/2 &= \frac{-2\alpha -1+3}{2\alpha +1} - \alpha + 1/2 
= -1 + \frac{3}{2\alpha +1} - \alpha +1/2 \\
&= -1/2 - \alpha +\frac{3}{2\alpha +1} 
\stackrel{\alpha > 3/4}{\leq} -1/2 - \frac34 +\frac{6}{5} = - \frac{5}{4} + \frac{6}{5} <0.
\end{split}
\end{equation*}

%
%Therefore we conclude that 
%\[ 
%	\BallHyp{(x,y)} \cap \BallHyp{p_n} \subseteq \FatBallHyp{p_n} \cap \Rcal([c(y_n,y), R_n]) 
%	\bigcup \Rcal ([R_n - y_n,R_n]).
%\]
%
%
%
%
%
%
%
%Lemma~\ref{lem:asymptotics_Omega_hyperbolic} also implies 
%that if $(x'',y'') \in  \BallSym{(x',y')}$, then $x''$ ranges 
%in an interval of length at most $K e^{3y'/2 + 3y''/2 - R_n}$. 
%Furthermore, such a point satisfies $y'' < R_n - y' + 2\ln (\pi/2)$, for otherwise 
%it would belong to $\BallHyp{(x',y')} \cap \BallPo{(x',y')}$. 
%
%These two observations yield
%\begin{equation*}
%\begin{split}
%&\int_{-I_n}^{I_n} \int_0^{(1-\eps) R_n\wedge (R_n-y)}\int_{-I_n}^{I_n} \int_0^{(1-\eps) R_n \wedge (R_n-y)}
% \ind{(x',y') \in \BallInter{(0,y)}} \cdot 
% \ind{(x'',y'') \in \BallSym{(x',y')} \cap \BallHyp{(0,y)}}\times \\
%& \hspace{1.8cm} e^{-\alpha y'} \cdot e^{-\alpha y''} dy'' dx''dy' dx' \\
%&\leq \int_{-I_n}^{I_n} \int_0^{(1-\eps) R_n \wedge (R_n - y)}\int_{-I_n}^{I_n} \int_0^{ R_n-y'+2\ln (\pi/2)} \\
%&\hspace{1cm} \ind{(x',y') \in \BallInter{(0,y)}} \cdot 
% \ind{(x'',y'') \in \BallSym{(x',y')}\cap \BallHyp{(0,y)}}\cdot
%  e^{-\alpha y'} \cdot e^{-\alpha y''} dy'' dx''dy' dx'
%%\\
%%&+ \int_{-I_n}^{I_n} \int_{(1-\eps) R_n \wedge (R_n - y)}^{(1-\eps)R_n} 
%%\int_{-I_n}^{I_n} \int_0^{R_n-y'+2\ln (\pi/2)}
%%\ind{(x',y') \in \BallHyp{(0,y)}\cap \BallPo{(0,y)}} \cdot 
%% \ind{(x'',y'') \in \BallHyp{(x',y')}\bigtriangleup \BallPo{(x',y')}}\times \\
%%& \hspace{1.8cm} e^{-\alpha y'} \cdot e^{-\alpha y''} dy'' dx''dy' dx.
%\end{split}
%\end{equation*}
%We will bound the above integral as follows. We split the inner integral with respect to 
%$y''$ into two parts, namely, $y'' \leq 2y$ and $y''>2y$. 
%For these two case we use the following bound:
%$$ \ind{(x'',y'') \in \BallSym{(x',y')}\cap \BallHyp{(0,y)}}
%\leq \begin{cases} 
% \ind{(x'',y'') \in \BallSym{(x',y')}} & \mbox{if $y''\leq 2y$,} \\
%  \ind{(x'',y'') \in \BallHyp{(0,y)}}  & \mbox{if $y''> 2y$} 
%\end{cases}.
%$$
%Also, we bound: 
%$$ \ind{(x',y') \in \BallInter{(0,y)}} \leq \ind{(x',y') \in \BallPo{(0,y)}}. $$
%Thus, we write 
%\begin{equation*}
%\begin{split}
%&\int_{-I_n}^{I_n} \int_0^{(1-\eps) R_n \wedge (R_n - y)}\int_{-I_n}^{I_n} \int_0^{ R_n-y'+2\ln (\pi/2)}
% \ind{(x',y') \in \BallInter{(0,y)}} \cdot 
% \ind{(x'',y'') \in \BallSym{(x',y')}\cap \BallHyp{(0,y)}}\times \\
%& \hspace{1.8cm} e^{-\alpha y'} \cdot e^{-\alpha y''} dy'' dx''dy' dx' \\
%&\leq 
%\int_{-I_n}^{I_n} \int_0^{(1-\eps) R_n \wedge (R_n - y)}\int_{-I_n}^{I_n} \int_0^{2y}
% \ind{(x',y') \in  \BallPo{(0,y)}} \cdot 
% \ind{(x'',y'') \in \BallSym{(x',y')}}\times \\
%& \hspace{1.8cm} e^{-\alpha y'} \cdot e^{-\alpha y''} dy'' dx''dy' dx'  \\
%&+
%\int_{-I_n}^{I_n} \int_0^{(1-\eps) R_n \wedge (R_n - y)}\int_{-I_n}^{I_n} \int_{2y}^{ R_n-y'+2\ln (\pi/2)}
% \ind{(x',y') \in \BallPo{(0,y)}} \cdot 
% \ind{(x'',y'') \in \BallHyp{(0,y)}}\times \\
%& \hspace{1.8cm} e^{-\alpha y'} \cdot e^{-\alpha y''} dy'' dx''dy' dx' \\
%&=O(1) \cdot e^{y/2}  \int_0^{(1-\eps) R_n \wedge (R_n - y)} e^{y'/2} 
%\int_0^{ R_n-y'+2\ln (\pi/2)} e^{3y'/2 + 3y''/2 - R_n}\cdot e^{-\alpha y''} dy''  \cdot e^{-\alpha y'} dy' \\
%&= O(1) \cdot e^{y/2-R_n} \int_0^{R_n - y} e^{2y'} 
%\int_0^{ R_n-y'+2\ln (\pi/2)} e^{(3/2 -\alpha) y''}dy''  \cdot e^{-\alpha y'} dy. 
%\end{split}
%\end{equation*}
%We will use Lemma~\ref{lem:asymptotics_Omega_hyperbolic} and write 
%\begin{equation*}
%\begin{split}
%&\int_{-I_n}^{I_n} \int_0^{(1-\eps) R_n \wedge (R_n - y)}\int_{-I_n}^{I_n} \int_0^{2y}
% \ind{(x',y') \in  \BallPo{(0,y)}} \cdot  \ind{(x'',y'') \in \BallSym{(x',y')}}\times \\
%& \hspace{1.8cm} e^{-\alpha y'} \cdot e^{-\alpha y''} dy'' dx''dy' dx' \\
%& \leq e^{y/2} \cdot  \int_0^{R_n - y} e^{y'/2}  \int_0^{2y}  e^{3y'/2 + 3y''/2 - R_n}
%\cdot e^{-\alpha y'} \cdot e^{-\alpha y''} dy'' dx''dy' dx' \\
%&= e^{y/2-R_n} \cdot  \int_0^{R_n - y} e^{2y'}  \int_0^{2y}  e^{3y''/2 - \alpha y''}
%\cdot e^{-\alpha y'} dy'' dy' \\
%\end{split}
%\end{equation*}
%
%Now, to evaluate the inner integral we make a case distinction depending on the value 
%of $\alpha$:
%$$ 
%\int_0^{ 2y} e^{(3/2 -\alpha) y''}dy'' = 
%O(1) \begin{cases}
%R \cdot e^{(3 - 2\alpha)y} & \mbox{if $\alpha \leq 3/2$} \\
%1 & \mbox{if $\alpha > 3/2$}
%\end{cases}. 
%$$
%Hence, for $\alpha \leq 3/2$, we get 
%\begin{equation*}
%\begin{split}
%&  \int_0^{R_n - y} e^{2y'}  \int_0^{2y}  e^{3y''/2 - \alpha y''}
%\cdot e^{-\alpha y'} dy'' dy' \leq \\
%& O(R_n)\cdot  e^{(3- 2\alpha)y} \cdot \int_0^{R_n - y} e^{(2-\alpha)y'}dy' \\
%&= O(R_n) \cdot e^{(3-2\alpha)y} 
%\begin{cases} 
%R_n \cdot e^{(2-\alpha) (R_n - y)} & \mbox{if $\alpha \leq 2$} \\
%1 & \mbox{if $\alpha >2$}
%\end{cases}.
%\end{split}
%\end{equation*}
%
%\begin{equation*}
%\begin{split} 
%&O(1) \cdot e^{y/2-R_n} \int_0^{R_n - y} e^{2y'} 
%\int_0^{ R_n-y'+2\ln (\pi/2)} e^{(3/2 -\alpha) y''}dy''  \cdot e^{-\alpha y'} dy \\
%&=O(1) \cdot e^{y/2 +(1/2 -\alpha)R_n} \int_0^{R_n - y} e^{y'/2}dy' \\
%&=O(1) \cdot e^{y/2 + (1/2 -\alpha)R_n+ (R_n - y)/2} \\
%&=O(1) \cdot e^{(1 -\alpha)R_n}.
%\end{split}
%\end{equation*}
%The contribution of this term to~\eqref{eq:expectation_total} is 
%\begin{equation*} 
%\begin{split}
%&{k_n \choose 2}^{-1} \cdot \expH^{-1} \cdot 
%e^{(1/2 - \alpha)R_n} \cdot \int_{-I_n}^{I_n} \int_{I_{\eps} (k_n)} e^{y/2}
%\left( \Pop{y}{k_n-2} + \Pop{y}{k_n-1} \right) e^{-\alpha y} dy dx \\  
%&\stackrel{\eqref{eq:prob_k_n-2}}{=} O(1) \cdot \expH^{-1} \cdot 
%e^{(1/2 - \alpha)R_n}\cdot I_n \cdot  \int_{I_\eps (k_n)} 
%e^{y/2} \cdot e^{-\lambda_y} \left(
%\frac{\lambda_y^{k_n-2}}{k_n!}+\frac{\lambda_y^{k_n-1}}{(k_n+1)!} \right) e^{-\alpha y} dy dx \\ 
%&\stackrel{I_n = O(n)}{=}  O(1) \cdot \expH^{-1} \cdot 
%e^{(1/2 - \alpha)R_n} \cdot n \cdot
%\int_{I_\eps(k_n)} e^{-\lambda_y} 
%\left(\frac{\lambda_y^{k_n-2}}{k_n!}
%+\frac{\lambda_y^{k_n-1}}{(k_n+1)!} \right)  e^{-\alpha y}dy. 
%\end{split}
%\end{equation*}
%We will show that the latter integral can be approximated by the ratio of Gamma functions. 
%As this approximation will be applied several times in the sequel, we will state it is a more 
%general form.  
%\begin{proposition} \label{eq:gamma_approx}
%Let $s, t, r \in \mathbb{R}$ be fixed. Then 
%$$\int_{I_\eps (k_n)}e^{ys}
%e^{-\lambda_y} \frac{\lambda_y^{k_n-r}}{(k_n-t)!}e^{-\alpha y} dy =O(1) \cdot 
%\frac{\Gamma (k_n- (2\alpha +r -2s))}{\Gamma(k_n-t+1)} =O(1) \cdot 
%k_n^{-2\alpha+t-r-1+ 2s}.
% $$
%\end{proposition}
%\begin{proof}[Proof of Claim~\ref{eq:gamma_approx}]
%To evaluate this integral we perform a change of variable setting $z=\lambda_y$. 
%We have $dz =\frac{1}{2} \lambda_y dy$ and therefore 
%\begin{equation} \label{eq:gamma_approx_I}
% \int_{I_\eps (k_n)} e^{ys} \cdot e^{-\lambda_y} \cdot \lambda_y^{k_n-r} \cdot \lambda_y^{-2\alpha}  dy = O(1) \cdot \int_{k_n^{1-\eps}}^{k_n^{1+\eps}} e^{-z} \cdot z^{k_n-1-r-2\alpha+2s}  dz. 
% \end{equation}
%Since $k_n\to \infty$, then as $n\to \infty$ we have 
%\begin{equation} \label{eq:gamma_approx_II}
%\int_{k_n^{1-\eps}}^{k_n^{1+\eps}} e^{-z} \cdot z^{k_n-(2\alpha+1+r-2s)}  dz / \Gamma (k_n-(2\alpha+r -2s)) \to 1.
%\end{equation}
%The claim follows from Stirling's formula.
%\end{proof}
%Therefore, 
%\begin{equation*} 
%\begin{split}
%&{k_n \choose 2}^{-1} \cdot \expH^{-1} \cdot 
%e^{(1/2 - \alpha)R_n}
%\int_{-I_n}^{I_n} \int_{I_\eps (k_n)} e^{y/2}
%\left( \Pop{y}{k_n-2} + \Pop{y}{k_n-1} \right) e^{-\alpha y} dy dx \\ 
%&=O(1) \cdot \expH^{-1} \cdot 
%e^{(1/2 - \alpha)R_n} \cdot n \cdot \left( 
% \frac{ \Gamma (k_n-(2\alpha+1))}{\Gamma (k_n+1)} + \frac{\Gamma (k_n-(2\alpha+3))}{\Gamma (k_n+2)}\right) \\
%& =O(1) \cdot \expH^{-1} \cdot 
%e^{(1/2 - \alpha)R_n} \cdot n \cdot k_n^{-(2\alpha +2)}.
%\end{split}
%\end{equation*}
%But $\expH = \Theta (1) \cdot n \cdot k_n^{-(2\alpha +1)}$. 
%Therefore, 
%\begin{equation*} 
%\begin{split}
%&{k_n \choose 2}^{-1} \cdot \expH^{-1} \cdot 
%e^{(1/2 - \alpha)R_n}
%\int_{-I_n}^{I_n} \int_{I_\eps (k_n)} e^{y/2}
%\Prob{D_{\H} ((0,y)) = k_n-2} e^{-\alpha y} dy dx \\ 
%& O(1) \cdot e^{(1/2 - \alpha)R_n} \cdot k_n^{-1}. 
%\end{split}
%\end{equation*}
%Therefore, for $1/2 < \alpha < 3/4$ we have 
%\begin{equation*}
%\begin{split}
%&k_n^{4\alpha -2} \cdot {k_n \choose 2}^{-1} \cdot \expH^{-1} \cdot 
%e^{(1/2 - \alpha)R_n}
%\int_{-I_n}^{I_n} \int_{I_\eps (k_n)} e^{y/2}
%\left( \Pop{y}{k_n-2} + \Pop{y}{k_n-1} \right) e^{-\alpha y} dy dx = \\
%&=O(1) \cdot k_n^{4\alpha -3} \cdot e^{(1/2 - \alpha)R_n} \to 0, \ \mbox{as } n\to \infty.
%\end{split}
%\end{equation*}
%Similarly, for $\alpha > 3/4$, we have 
%\begin{equation*}
%\begin{split}
%&k_n \cdot {k_n \choose 2}^{-1} \cdot \expH^{-1} \cdot 
%e^{(1/2 - \alpha)R_n}
%\int_{-I_n}^{I_n} \int_{I_\eps (k_n)} e^{y/2}
%\left( \Pop{y}{k_n-2} + \Pop{y}{k_n-1} \right) e^{-\alpha y} dy dx = \\
%&=O(1) \cdot e^{(1/2 - \alpha)R_n} \to 0, \ \mbox{as } n\to \infty.
%\end{split}
%\end{equation*}
%The calculations for the second term are almost identical and we omit them.
%%As this convergence is polynomial in $n$, we also have for $\alpha = 3/4$, 
%%\begin{equation*}
%%\begin{split}
%%&\frac{k_n}{\log k_n} \cdot {k_n \choose 2}^{-1} \cdot \expH^{-1} \cdot 
%%e^{(1/2 - \alpha)R_n}
%%\int_{-I_n}^{I_n} \int_{I_\eps (k_n)} e^{y/2}
%%\left( \Pop{y}{k_n-2} + \Pop{y}{k_n-1} \right) e^{-\alpha y} dy dx =o(1).
%%\end{split}
%%\end{equation*}
%


\subsubsection{The sums of~\eqref{eq:sum_3} and~\eqref{eq:sum_4}}

As in the previous section, we will only consider~\eqref{eq:sum_3} as~\eqref{eq:sum_4} relies on very similar estimates 
and yields the same upper bound, up to a multiplicative constant. 
Recall that in this case, we consider pairs $(p_1,p_2)$, with $p_1 = (x_1,y_1)$ satisfying 
$y_1 \geq (R_n - y) \wedge (1-\eps)R_n$, and $p_1 \in \BallHyp{(0,y)}$, $p_2 \in \BallSym{p_1} \cap \BallHyp{(0,y)}$. 
We split this into three sub-cases:  i. $y_2 \geq R_n - y$; ii. $R_n -y_1 \leq y_2 \leq R_n -y$ and iii. $y_2 < R_n - y_1$.  

In the first case, note that $y_1 + y_2 \geq 2(R_n - y) > R_n$, since $2y < 2(1+\eps)
\frac{R_n}{2\alpha +1} < R_n$. Thus, $p_2 \in \BallHyp{p_1}$. Furthermore, 
$y_2 > R_n - y_1 + 2\ln (\pi/2)$, which implies that $p_2 \in \BallPo{p_1}$ too. 
Hence, the contribution from these pairs is zero.   

Let 
$$\mathcal{S}_1 = \{(p_1, p_2) \ : \  p_1,p_2 \ \in \Pcal \setminus \{(0,y) \}, \ y_1 \geq (1-\eps) R_n \wedge (R_n-y), \ 
R_n -y \leq y_2 \leq R_n \}. $$
The Campbell-Mecke formula yields: 
\begin{equation*}
\begin{split} 
&\Exp { \sum_{(p_1,p_2)  \in \mathcal{S}_1} 
\ind{p_1 \in \BallHyp{(0,y)}} \cdot \ind{p_2 \in \BallSym{p_1} \cap \BallHyp{(0,y)}}
\cdot D_\H (y,k_n-2;\Pcal \setminus \{ p_1\})} \\
& 
=O(1) \cdot \int_{-I_n}^{I_n} \int_{(1-\eps) R_n \wedge (R_n-y)}^{R_n} \ind{(x_1,y_1) \in \BallHyp{(0,y)}}
\int_{-I_n}^{I_n} \int_{R_n - y}^{R_n} 
\ind{(x_2,y_2) \in \BallSym{(x_1,y_1)} \cap \BallHyp{(0,y)}} \times \\ 
& \hspace{2cm}  D_\H (y,k_n-2;\Pcal \setminus \{ (x_1,y_1), (x_2,y_2)\}) \cdot
e^{-\alpha y_2 - \alpha y_1} dy_2 dx_2 dy_1 dx_1.
\end{split}
\end{equation*}
Note that uniformly over all $(x_1,y_1)$ and $(x_2,y_2)$ in $\mathcal{S}_1$, we have 
$$ D_\H (y,k_n-2;\Pcal \setminus \{ (x_1,y_1), (x_2,y_2)\})   = e^{\lambda_y} \cdot \frac{\lambda_y^{k_n-2}}{\Gamma (k_n-1)!}, $$
so this term can be extracted from the integral. 
It thus suffices to bound the integral: 
\begin{equation*}
\begin{split}
&\int_{-I_n}^{I_n} \int_{(1-\eps) R_n \wedge (R_n-y)}^{R_n} \ind{(x_1,y_1) \in \BallHyp{(0,y)}}
\int_{-I_n}^{I_n} \int_{R_n - y}^{R_n } 
\ind{(x_2,y_2) \in \BallSym{(x_1,y_1)} \cap \BallHyp{(0,y)}} \times \\ 
& \hspace{2cm}  e^{-\alpha y_2 - \alpha y_1} dy_2 dx_2 dy_1 dx_1 \\
&\leq \int_{-I_n}^{I_n} \int_{(1-\eps) R_n \wedge (R_n-y)}^{R_n} 
\int_{-I_n}^{I_n} \int_{R_n - y}^{R_n}  e^{-\alpha y_2 - \alpha y_1} dy_2 dx_2 dy_1 dx_1\\
&= \left[ \int_{-I_n}^{I_n} \int_{(1-\eps) R_n \wedge (R_n-y)}^{R_n} 
e^{- \alpha y_1} dy_1 dx_1\
\right] \cdot 
\left[\int_{-I_n}^{I_n} \int_{R_n - y}^{R_n}  e^{-\alpha y_2} dy_2 dx_2 \right].
\end{split}
\end{equation*}
We evaluate
$$  \int_{-I_n}^{I_n} \int_{(1-\eps) R_n \wedge (R_n-y)}^{R_n} 
e^{- \alpha y_1} dy_1 dx_1= O(1) \cdot n \cdot e^{-\alpha R_n + ((\eps R_n) \vee y))\alpha}
=O(1) \cdot n \cdot e^{-\alpha R_n + \alpha y + \alpha \eps R_n }
$$
and 
$$\int_{-I_n}^{I_n} \int_{R_n - y}^{R_n}  e^{-\alpha y_2} dy_2 dx_2 
=O(1) \cdot n \cdot e^{-\alpha R_n +\alpha y}.
$$
Also, $n \cdot e^{-\alpha R_n} = O(1) \cdot e^{(1/2 -\alpha) R_n}$, whereby we deduce that 
\begin{equation*}
\begin{split}
&\int_{\mathcal{S}_1 \times \mathcal{S}_1} \ind{(x_1,y_1) \in \BallHyp{(0,y)}} \cdot
\ind{(x_2,y_2) \in \BallSym{(x_1,y_1)} \cap \BallHyp{(0,y)}} 
 e^{-\alpha y_2 - \alpha y_1} dy_2 dx_2 dy_1 dx_1 \\
&=O(1) \cdot e^{(1-2\alpha)R_n + 2\alpha y + \alpha \eps R_n} =O(1) \cdot n^{2(1-2\alpha) + 2\alpha \eps} \cdot e^{2\alpha y}.
\end{split}.
\end{equation*}
We incorporate this term into~\eqref{eq:expectation_total} and obtain
\begin{equation*} 
\begin{split}
& O(1) \cdot n^{(2(1-2\alpha) + 2\alpha \eps} \cdot {k_n \choose 2}^{-1} \cdot \expH^{-1} \cdot 
\int_{-I_n}^{I_n} \int_{I_\eps (k_n)} 
e^{2\alpha y} \cdot e^{-\lambda_y} 
\frac{\lambda_y^{k_n-2}}{(k_n-2)!} e^{-\alpha y} dy dx \\ 
&=O(1) \cdot n^{(2(1-2\alpha) + 2\alpha \eps} \cdot \expH^{-1}
\int_{-I_n}^{I_n} \int_{I_\eps (k_n)} 
e^{-\lambda_y} 
\frac{\lambda_y^{k_n + 2\alpha - 2}}{k_n!}  dy dx \\ 
&\stackrel{I_n = O(n)}{=}  O(1) \cdot n^{(2(1-2\alpha) + 2\alpha \eps} \cdot \expH^{-1} \cdot  n \cdot 
\int_{I_\eps (k_n)} e^{-\lambda_y} 
\frac{\lambda_y^{k_n +2\alpha -2}}{k_n!} dy. 
\end{split}
\end{equation*}
By Proposition~\ref{eq:gamma_approx} we have
\begin{equation*} 
\begin{split} 
&\int_{I_\eps (k_n)} e^{-\lambda_y} \frac{\lambda_y^{k_n+2\alpha}}{k_n!}  dy= 
O(1) \cdot \frac{\Gamma(k_n+2\alpha-2)}{\Gamma (k_n+1)} = O(1) \cdot 
\frac{1}{k_n^{-2\alpha +3}}.
\end{split}
\end{equation*}
We now substitute this into the previous expression and obtain: 
\begin{equation*}
\begin{split}
& n^{(2(1-2\alpha) + 2\alpha \eps} \cdot \expH^{-1} \cdot  n \cdot 
\int_{I_\eps (k_n)} e^{-\lambda_y} 
\frac{\lambda_y^{k_n +2\alpha}}{k_n!} dy \\
&= O(1) \cdot n^{(2(1-2\alpha) + 2\alpha \eps} \cdot \expH^{-1} \cdot  n \cdot 
\frac{1}{k_n^{-2\alpha +3}} \\
&\stackrel{\expH = O(1) \cdot n \cdot k_n^{-(2\alpha +1)}}{=}
O(1) \cdot n^{(2(1-2\alpha) + 2\alpha \eps} \cdot k_n^{2\alpha +1} \cdot 
\frac{1}{k_n^{-2\alpha +3}} = O(1) \cdot n^{(2(1-2\alpha) + 2\alpha \eps} \cdot k_n^{4\alpha -2}.
\end{split}
\end{equation*}
Thus, for $1/2 < \alpha \leq3/4$, we have 
\begin{equation*}
\begin{split}
k_n^{4 \alpha -2} \cdot  n^{2(1-2\alpha) + 2\alpha \eps} \cdot k_n^{4\alpha -2} = 
n^{2\alpha \eps} \cdot \left( \frac{k_n^2}{n} \right)^{2(2\alpha -1)} = o(1). 
\end{split}
\end{equation*}
%provided that $\eps = \eps (\alpha)>0$ is small enough. 
Similarly, for $\alpha > 3/4$ we have $2\alpha -1 > 1/2$ and we get
\begin{equation*} 
\begin{split}
&k_n \cdot n^{2(1-2\alpha) + 2\alpha \eps} \cdot k_n^{4\alpha -2}
\ll k_n \cdot n^{-1/2 + 2\alpha \eps}  \cdot k_n^{4\alpha -2} \cdot n^{1-2\alpha}  = o(1),
\end{split}
\end{equation*}
provided that $\eps$ is small enough, depending on $\alpha$. 


We now consider the second sub-domain:
$$\mathcal{S}_2 = \{(p_1, p_2) \ : \  p_1,p_2 \ \in \Pcal \setminus \{(0,y) \}, \ y_1 \geq (1-\eps) R_n \wedge (R_n-y), \ 
R_n -y_1 \leq y_2 \leq R_n -y \}. $$
The Campbell-Mecke formula yields: 
\begin{equation*}
\begin{split} 
&\Exp { \sum_{(p_1,p_2)  \in \mathcal{S}_2} 
\ind{p_1 \in \BallHyp{(0,y)}} \cdot \ind{p_2 \in \BallSym{p_1} \cap \BallHyp{(0,y)}}
\cdot D_\H (y,k_n-2;\Pcal \setminus \{ p_1\})} \\
& 
=O(1) \cdot \int_{-I_n}^{I_n} \int_{(1-\eps) R_n \wedge (R_n-y)}^{R_n} \ind{(x_1,y_1) \in \BallHyp{(0,y)}}
\int_{-I_n}^{I_n} \int_{R_n - y_1}^{R_n-y} 
\ind{(x_2,y_2) \in \BallSym{(x_1,y_1)} \cap \BallHyp{(0,y)}} \times \\ 
& \hspace{2cm}  D_\H (y,k_n-2;\Pcal \setminus \{ (x_1,y_1), (x_2,y_2)\}) \cdot
e^{-\alpha y_2 - \alpha y_1} dy_2 dx_2 dy_1 dx_1.
\end{split}
\end{equation*}
Note that uniformly over all $(x_1,y_1)$ and $(x_2,y_2)$ we have 
$$ D_\H (y,k_n-2;\Pcal \setminus \{ (x_1,y_1), (x_2,y_2)\})   = e^{\lambda_y} \cdot \frac{\lambda_y^{k_n-2}}{\Gamma (k_n-1)!}, $$
so this term can be extracted from the integral. 

So it suffices to bound the integral: 
\begin{equation}
\begin{split}
&\int_{-I_n}^{I_n} \int_{(1-\eps) R_n \wedge (R_n-y)}^{R_n} \ind{(x_1,y_1) \in \BallHyp{(0,y)}}
\int_{-I_n}^{I_n} \int_{R_n - y_1}^{R_n - y} 
\ind{(x_2,y_2) \in \BallSym{(x_1,y_1)} \cap \BallHyp{(0,y)}} \times \\ 
& \hspace{2cm}  e^{-\alpha y_2 - \alpha y_1} dy_2 dx_2 dy_1 dx_1 \\
&\leq \int_{-I_n}^{I_n} \int_{(1-\eps) R_n \wedge (R_n-y)}^{R_n} \ind{(x_1,y_1) \in \BallHyp{(0,y)}}
\int_{-I_n}^{I_n} \int_{R_n - y_1}^{R_n - y} 
\ind{(x_2,y_2) \in  \BallHyp{(0,y)}} \times \\ 
& \hspace{2cm}  e^{-\alpha y_2 - \alpha y_1} dy_2 dx_2 dy_1 dx_1.
%&= \left[ \int_{-I_n}^{I_n} \int_{(1-\eps) R_n \wedge (R_n-y)}^{R_n} \ind{(x_1,y_1) \in \BallHyp{(0,y)}}e^{- \alpha y_1} dy_1 dx_1\right]\times  \\
%& \left[ \int_{-I_n}^{I_n} \int_{R_n-y}^{R_n} \ind{(x_2,y_2) \in \BallHyp{(0,y)}}e^{- \alpha y_2} dy_2 dx_2\right]\\
%&\leq \left[ \int_{-I_n}^{I_n} \int_{(1-\eps) R_n \wedge (R_n-y)}^{R_n}
%e^{- \alpha y_1} dy_1 dx_1\right] \cdot
 %\left[ \int_{-I_n}^{I_n} \int_{R_n-y}^{R_n}e^{- \alpha y_2} dy_2 dx_2 \right]
\end{split}
\end{equation}
Now, by Lemma~\ref{}
\begin{equation*}
\begin{split}
&\int_{-I_n}^{I_n} \int_{R_n - y_1}^{R_n - y} \ind{(x_2,y_2) \in  \BallHyp{(0,y)}} \cdot  
e^{-\alpha y_2} dy_2 dx_2 = O(1) \cdot e^{y/2} \int_{R_n - y_1}^{R_n - y} e^{(1/2 - \alpha) y_2} dy_2 \\
&= O(1) \cdot e^{y/2 + (1/2 - \alpha) (R_n - y_1)}.
\end{split}
\end{equation*}
We then integrate with respect to $y_1$:
\begin{equation*}
\begin{split} O(1) \cdot e^{y/2} \cdot 
&\int_{-I_n}^{I_n} \int_{(1-\eps) R_n \wedge (R_n-y)}^{R_n} \ind{(x_1,y_1) \in \BallHyp{(0,y)}} 
e^{(1/2 - \alpha) (R_n - y_1)} e^{-\alpha y_1} dy_1dx_1 \leq \\
& O(1) \cdot e^{y/2 + (1/2 -\alpha) R_n} \cdot \int_{-I_n}^{I_n} \int_{(1-\eps) R_n \wedge (R_n-y)}^{R_n} 
e^{(\alpha -1/2) y_1} e^{-\alpha y_1} dy_1dx_1 \\
&=O(1) \cdot e^{y/2 + (1 -\alpha) R_n -((1-\eps) R_n \wedge (R_n-y))/2} \\
&=O(1) \cdot e^{y/2 + (1/2 -\alpha) R_n + ((\eps R_n) \vee y)/2}\\
&\stackrel{(\eps R_n) \vee y < \eps R_n + y}{=}O(1) \cdot e^{y + (1/2 -\alpha) R_n + \eps R_n}\\
&= O(1) \cdot n^{1- 2\alpha+ \eps} \cdot e^{y}. 
\end{split}
\end{equation*}


%The expected value of the third term can be bounded as 
%follows: 
%\begin{equation*}
%\begin{split} 
%&\Exp { \sum_{p_1 \ \in \Pcal \setminus \{(0,y) \}, \ y(p_1) \geq (1-\eps) R_n \wedge (R_n-y)} k_n 
%\cdot D_\H (y,k_n-1;\Pcal \setminus \{ p_1\})} \\
%&= k_n \cdot \int_{-I_n}^{I_n} \int_{(1-\eps) R_n \wedge (R_n-y)}^{R_n} 
% D_\H (y,k_n-1; \Pcal \setminus \{ p_1\}) 
% e^{-\alpha y} dy_1 dx_1 \\
%&= k_n \cdot \Pop{y}{k_n-1} \cdot \int_{-I_n}^{I_n} \int_{(1-\eps) R_n \wedge (R_n-y)}^{R_n}  e^{-\alpha y} dy_1 dx_1 \\
%&= O(1)\cdot k_n \cdot \Pop{y}{k_n-1} \cdot I_n \cdot 
%e^{-\alpha \left( (1-\eps) R_n \wedge (R_n-y)\right)} \\
%&= O(1) \cdot k_n \cdot \Pop{y}{k_n-1} \cdot e^{(1/2 - \alpha)R_n +\alpha \cdot ( \eps R_n \vee y)}.
%\end{split}
%\end{equation*}
%Now, we bound $\eps R_n \vee y < \eps R_n + y$. 
%This implies that 
%$$e^{(1/2 - \alpha)R_n +\alpha \cdot ( \eps R_n \vee y)} < e^{(1/2 -\alpha) R_n + \alpha(\eps R_n +y)}= O(1) \cdot n^{1-2\alpha + 2\alpha \eps} \cdot e^{\alpha y}.$$
%
%%Now, if $y > \eps R_n$, then 
%%$e^{(1/2 - \alpha)R_n +\alpha \cdot ( \eps R_n \vee y)} = e^{(1/2 - \alpha)R_n +\alpha y}< e^{y/2}$, as $y< R_n$. 
%%If $y \leq \eps R_n$, then 
%%$e^{(1/2 - \alpha)R_n +\alpha \cdot ( \eps R_n \vee y)} = e^{(1/2 - \alpha)R_n +\alpha \eps R_n} < e^{y/2}$, provided that $\eps = \eps (\alpha)$ is small enough and $n$ is sufficiently large. 
%Therefore, 
%\begin{equation*}
%\begin{split} 
%&\Exp { \sum_{p_1 \ \in \Pcal \setminus \{(0,y) \}, \ y(p_1) \geq (1-\eps) R_n \wedge (R_n-y)} k_n 
%\cdot D_\H (y,k_n-1;\Pcal \setminus \{ p_1\})}= \\
%&\hspace{1.8cm} O(1) \cdot n^{1-2\alpha + 2\alpha\eps}\cdot k_n \cdot \Pop{y}{k_n-1} \cdot e^{\alpha y}.
%\end{split}
%\end{equation*}
%Recall also that 
%$$ \Pop{y}{k_n-1} = e^{-\lambda_y} 
%\frac{\lambda_y^{k_n-1}}{(k_n-1)!}. $$
The contribution 
of this term to~\eqref{eq:expectation_total} is:
\begin{equation*} 
\begin{split}
& O\left( n^{1-2\alpha + \eps} \right) \cdot {k_n \choose 2}^{-1} \cdot \expH^{-1} \cdot 
\int_{-I_n}^{I_n} \int_{I_\eps (k_n)} 
e^{y} \cdot e^{-\lambda_y} 
\frac{\lambda_y^{k_n-2}}{(k_n-2)!} e^{-\alpha y} dy dx \\ 
&= O\left( n^{1-2\alpha + \eps} \right) \cdot \expH^{-1}
\int_{-I_n}^{I_n} \int_{I_\eps (k_n)} 
e^{-\lambda_y} 
\frac{\lambda_y^{k_n - 2\alpha}}{k_n!}  dy dx \\ 
&\stackrel{I_n = O(n)}{=}  O\left( n^{1-2\alpha + \eps} \right) \cdot \expH^{-1} \cdot  n \cdot 
\int_{I_\eps (k_n)} e^{-\lambda_y} 
\frac{\lambda_y^{k_n -2\alpha}}{k_n!} dy. 
\end{split}
\end{equation*}
By Proposition~\ref{eq:gamma_approx} we have
\begin{equation*} 
\begin{split} 
&\int_{I_\eps (k_n)} e^{-\lambda_y} \frac{\lambda_y^{k_n-2\alpha}}{k_n!}  dy= 
%O(1)\cdot \frac{1}{\Gamma (k_n+1)} \cdot \int_{\lambda_{2(1-\eps)\ln k_n}}^{\lambda_{2(1+\eps)\ln k_n}} 
%e^{-z} z^{k_n -1 - 2\alpha} dz \\
O(1) \cdot \frac{\Gamma(k_n-2\alpha)}{\Gamma (k_n+1)} = O(1) \cdot 
\frac{1}{k_n^{2\alpha +1}}.
\end{split}
\end{equation*}
Substituting this into the above expression we obtain: 
\begin{equation*}
\begin{split}
&O\left( n^{1-2\alpha + \eps} \right) \cdot {k_n \choose 2}^{-1} \cdot \expH^{-1} \cdot
\int_{-I_n}^{I_n} \int_{I_\eps (k_n)} e^{y }\cdot
e^{-\lambda_y} 
\frac{\lambda_y^{k_n-2}}{(k_n-1)!} e^{-\alpha y} dy dx \\ 
&=O\left( n^{1-2\alpha + \eps} \right) \cdot  \expH^{-1} \cdot n \cdot  
\frac{1}{k_n^{2\alpha +1}} \\
&\stackrel{\expH = O(1) \cdot n \cdot k_n^{-(2\alpha +1)}}{=}O\left( n^{1-2\alpha + \eps} \right).
\end{split}
\end{equation*}
%But $\expH = O(1) \cdot n \cdot k_n^{-(2\alpha +1)}$. 
%We thus conclude that 
%\begin{equation*}
%\begin{split}
%&{k_n \choose 2}^{-1} \cdot \expH^{-1} \cdot k_n \cdot 
%e^{(1/2 - \alpha)R_n + \alpha \cdot (\eps R_n \vee y)}
%\int_{-I_n}^{I_n} \int_{I_\eps (k_n)} 
%e^{-\lambda_y} 
%\frac{\lambda_y^{k_n-1}}{(k_n-1)!} e^{-\alpha y} dy dx \\ 
%&=O(1) \cdot  \frac{1}{k_n}.
%\end{split}
%\end{equation*}
Thus, for $1/2 < \alpha \leq3/4$, we have 
\begin{equation} \label{eq:2nd_term-1st_regime}
\begin{split}
k_n^{4 \alpha -2} \cdot  n^{1-2\alpha + \eps} 
= n^{\eps} \left( \frac{k_n^2}{n}\right)^{2\alpha -1} = o(1).
\end{split}
\end{equation}
%provided that $\eps = \eps (\alpha)>0$ is small enough. 
Similarly, for $\alpha > 3/4$ we have $2\alpha -1 > 1/2$ and we get
\begin{equation} \label{eq:2nd_term-2nd_regime}
\begin{split}
&k_n \cdot n^{1-2\alpha + \eps} 
\ll n^{-1/2 + \eps}  \cdot k_n  = o(1),
\end{split}
\end{equation}
provided that $\eps$ is small enough, depending on $\alpha$. 

For the third sub-domain:
$$\mathcal{S}_3 = \{(p_1, p_2) \ : \  p_1,p_2 \ \in \Pcal \setminus \{(0,y) \}, \ y_1 \geq (1-\eps) R_n \wedge (R_n-y), \ y_2 \leq R_n -y_1 \}, $$
we shall use~\eqref{eq:symdiff_loc} which states that if 
$p_2=(x_2,y_2) \in \BallSym{p_1}\cap \BallHyp{(0,y)}$ and $y_2\leq R_n - y_1$, then 
$y_2 \geq c(x_1,y_1)$, where $c(x_1,y_1) = \left(2 \log(|x_1|e^{-y_1/2}) - \log c_K \right) \vee 0$ (cf.~\eqref{eq:to_use_I}). Moreover, $p_2 \in \FatBallHyp{p_1}$.


Again, we will use the Campbell-Mecke formula: 
\begin{equation*}
\begin{split} 
&\Exp { \sum_{(p_1,p_2)  \in \mathcal{S}_3} 
\ind{p_1 \in \BallHyp{(0,y)}} \cdot \ind{p_2 \in \BallSym{p_1} \cap \BallHyp{(0,y)}}
\cdot D_\H (y,k_n-2;\Pcal \setminus \{ p_1\})} \\
& =O(1) \cdot \int_{-I_n}^{I_n} \int_{(1-\eps) R_n \wedge (R_n-y)}^{R_n} \ind{(x_1,y_1) \in \BallHyp{(0,y)}}
\int_{-I_n}^{I_n} \int_{0}^{R_n-y_1} 
\ind{(x_2,y_2) \in \BallSym{(x_1,y_1)} \cap \BallHyp{(0,y)}} \times \\ 
& \hspace{2cm}  D_\H (y,k_n-2;\Pcal \setminus \{ (x_1,y_1), (x_2,y_2)\}) \cdot
e^{-\alpha y_2 - \alpha y_1} dy_2 dx_2 dy_1 dx_1 \\
& =O(1) \cdot 
e^{\lambda_y} \cdot \frac{\lambda_y^{k_n-2}}{(k_n-2)!}\times \\
& \int_{-I_n}^{I_n} \int_{(1-\eps) R_n \wedge (R_n-y)}^{R_n} \ind{(x_1,y_1) \in \BallHyp{(0,y)}}
\int_{-I_n}^{I_n} \int_{0}^{R_n-y_1} 
\ind{(x_2,y_2) \in \BallSym{(x_1,y_1)} \cap \BallHyp{(0,y)}} \times \\ 
& \hspace{2cm}  e^{-\alpha y_2 - \alpha y_1} dy_2 dx_2 dy_1 dx_1.
\end{split}
\end{equation*}
The inner integral with respect to $(x_2,y_2)$ is 
\begin{equation*}
\begin{split}
&\int_{-I_n}^{I_n} \int_{0}^{R_n - y_1}  \ind{(x_2,y_2) \in \BallSym{p_1}\cap \BallHyp{(0,y)}}  
e^{-\alpha y_2} dy_2 dx_2\\
&  \leq 
\int_{-I_n}^{I_n} \int_{0}^{R_n-y_1}  \ind{y_2 \geq c(x_1,y_1), (x_2,y_2) \in \FatBallHyp{(0,y)}}  
e^{-\alpha y_2} dy_2 dx_2 
=
O(1) \cdot e^{y/2} \int_{c(x_1,y_1)}^{R_n - y_1} e^{y_2/2 - \alpha y_2} dy_2 \\
&=O(1) \cdot e^{y/2 + (1/2 -\alpha) c(x_1,y_1)},
\end{split}
\end{equation*}
uniformly over all $x_1,y_1$. 
Thus, we get
\begin{equation*}
\begin{split}
 &\int_{-I_n}^{I_n} \int_{(1-\eps) R_n \wedge (R_n-y)}^{R_n} \ind{(x_1,y_1) \in \BallHyp{(0,y)}}
\int_{-I_n}^{I_n} \int_{0}^{R_n-y_1} 
\ind{(x_2,y_2) \in \BallSym{(x_1,y_1)} \cap \BallHyp{(0,y)}} \times \\ 
& \hspace{2cm}  e^{-\alpha y_2 - \alpha y_1} dy_2 dx_2 dy_1 dx_1 \\
&\leq O(1) \cdot  
\int_{-I_n}^{I_n} \int_{(1-\eps) R_n \wedge (R_n-y)}^{R_n} 
e^{y/2 + (1/2 -\alpha) c(x_1,y_1)} e^{-\alpha y_1} dy_1 dx_1. 
\end{split}
\end{equation*}
We now integrate this with respect to $x_1$ from 0 to $I_n$.
We will split this integral into two parts according to the value of $c(x_1,y_1)$:
$$
\int_0^{I_n} e^{c(x_1,y_1) (1/2 -\alpha)} dx_1 = 
\int_{c_K e^{y_1/2}}^{I_n} e^{c(x_1,y_1) (1/2 -\alpha)} dx_1 + \int_0^{c_K e^{y_1/2}} dx_1.
$$
The first integral becomes: 
\begin{equation*}
\begin{split} 
&\int_{c_K e^{y_1/2}}^{I_n} e^{c(x_1,y_1) (1/2 -\alpha)} dx_1  = 
 O(1)\cdot \int_{c_K e^{y_1/2}}^{I_n} x_1^{1 -2\alpha} 
e^{-\frac{y_1}{2} (1-2\alpha)} dx_1 \\
&\stackrel{I_n = O(1)e^{R_n/2}}{=} 
\begin{cases}
O(R_n) \cdot e^{-y_1/2 + \alpha y_1} \cdot e^{\frac{R_n}{2} 2(1-\alpha)} & \ \mbox{if $\alpha \leq 1$}
\\
O(1) \cdot e^{-y_1/2 + \alpha y_1 + 2(1-\alpha)y_1/2} & \ \mbox{if $\alpha > 1$}
\end{cases}.
\\
&=\begin{cases}
O(R_n) \cdot e^{(\alpha -1/2) y_1} \cdot n^{2(1-\alpha)} & \ \mbox{if $\alpha \leq 1$}
\\
O(1) \cdot e^{y_1/2} &\ \mbox{if $\alpha > 1$}
\end{cases}.  
\end{split}
\end{equation*}
The second integral trivially gives: 
$$ \int_0^{c_K e^{y_1/2}} dx_1 = O(1) \cdot e^{y_1/2}.$$
Putting these two together we conclude that 
$$ e^{y/2} \cdot \int_0^{I_n} e^{c(x_1,y_1) (1/2 -\alpha)} dx_1 = 
O(1) \cdot e^{y_1/2 +y(3/2-\alpha)}. $$
Now, we integrate these with respect to $y_1$: for $\alpha \leq 1$
\begin{equation}
\begin{split} 
&n^{2(1-\alpha)} \cdot \int_{(1-\eps) R_n \wedge (R_n-y)}^{R_n} e^{(\alpha -1/2)y_1 - \alpha y_1} dy_1 
=O(1) \cdot n^{2(1-\alpha)} \cdot e^{-R_n/2 + \eps R_n/2  + y/2}  \\
&= O(1) \cdot n^{1-2\alpha + \eps} \cdot e^{y/2}.
\end{split}
\end{equation}
Also,
\begin{equation}
\begin{split}
&\int_{(1-\eps) R_n \wedge (R_n-y)}^{R_n} e^{(1/2 - \alpha) y_1} dy_1 
= O(1) \cdot e^{(1/2-\alpha) R_n + y(\alpha -1/2) + \eps R_n (\alpha -1/2)} \\
&= O(1) \cdot n^{1-2\alpha +\eps(2\alpha -1)} \cdot e^{y(\alpha -1/2)}. 
\end{split}
\end{equation}
Therefore, when $1/2 < \alpha \leq 1$, we have 
\begin{equation} \label{eq:case_a<1}
\begin{split} 
&\Exp { \sum_{(p_1,p_2)  \in \mathcal{S}_3} 
\ind{p_1 \in \BallHyp{(0,y)}} \cdot \ind{p_2 \in \BallSym{p_1} \cap \BallHyp{(0,y)}}
\cdot D_\H (y,k_n-2;\Pcal \setminus \{ p_1\})} \\
&=O(R_n) \cdot e^{\lambda_y} \cdot \frac{\lambda_y^{k_n-2}}{(k_n-2)!} \cdot 
n^{1-2\alpha + \eps} \cdot e^{y/2}
\end{split}
\end{equation}
For $\alpha >1$, we get 
\begin{equation} \label{eq:case_a>1}
\begin{split} 
&\Exp { \sum_{(p_1,p_2)  \in \mathcal{S}_3} 
\ind{p_1 \in \BallHyp{(0,y)}} \cdot \ind{p_2 \in \BallSym{p_1} \cap \BallHyp{(0,y)}}
\cdot D_\H (y,k_n-2;\Pcal \setminus \{ p_1\})} \\
& O(1) \cdot e^{\lambda_y} \cdot \frac{\lambda_y^{k_n-2}}{(k_n-2)!} \cdot 
 n^{1-2\alpha +\eps(2\alpha -1)} \cdot e^{y(\alpha -1/2)}.
\end{split}
\end{equation}
For the former case, we can deduce that counterparts of~\eqref{eq:2nd_term-1st_regime} and~\eqref{eq:2nd_term-2nd_regime}. 

For $\alpha > 1$, the contribution of~\eqref{eq:case_a>1} to~\eqref{eq:expectation_total} is:
\begin{equation*} 
\begin{split}
& O\left( n^{1-2\alpha + \eps (2\alpha -1)} \right) \cdot {k_n \choose 2}^{-1} \cdot \expH^{-1} \cdot 
\int_{-I_n}^{I_n} \int_{I_\eps (k_n)} 
e^{(\alpha -1/2) y} \cdot e^{-\lambda_y} 
\frac{\lambda_y^{k_n-2}}{(k_n-2)!} e^{-\alpha y} dy dx \\ 
&= O\left( n^{1-2\alpha + \eps(2\alpha -1)} \right) \cdot \expH^{-1}
\int_{-I_n}^{I_n} \int_{I_\eps (k_n)} 
e^{-\lambda_y} 
\frac{\lambda_y^{k_n - 3}}{k_n!}  dy dx \\ 
&\stackrel{I_n = O(n)}{=}  O\left( n^{1-2\alpha + \eps(2\alpha -1)} \right) 
\cdot \expH^{-1} \cdot  n \cdot 
\int_{I_\eps (k_n)} e^{-\lambda_y} 
\frac{\lambda_y^{k_n -3}}{k_n!} dy. 
\end{split}
\end{equation*}
By Proposition~\ref{eq:gamma_approx} we have
\begin{equation*} 
\begin{split} 
&\int_{I_\eps (k_n)} e^{-\lambda_y} \frac{\lambda_y^{k_n-3}}{k_n!}  dy= 
%O(1)\cdot \frac{1}{\Gamma (k_n+1)} \cdot \int_{\lambda_{2(1-\eps)\ln k_n}}^{\lambda_{2(1+\eps)\ln k_n}} 
%e^{-z} z^{k_n -1 - 2\alpha} dz \\
O(1) \cdot \frac{\Gamma(k_n-3)}{\Gamma (k_n+1)} = O(1) \cdot 
\frac{1}{k_n^4}.
\end{split}
\end{equation*}
Substituting this into the above expression we obtain: 
\begin{equation*}
\begin{split}
&O\left( n^{1-2\alpha + \eps(2\alpha -1)} \right) \cdot {k_n \choose 2}^{-1} \cdot \expH^{-1} \cdot
\int_{-I_n}^{I_n} \int_{I_\eps (k_n)} e^{(\alpha -1/2)y }\cdot
e^{-\lambda_y} 
\frac{\lambda_y^{k_n-2}}{(k_n-1)!} e^{-\alpha y} dy dx \\ 
&=O\left( n^{1-2\alpha + \eps(2\alpha -1)} \right) \cdot  \expH^{-1} \cdot n \cdot  
\frac{1}{k_n^4} \\
&\stackrel{\expH = O(1) \cdot n \cdot k_n^{-(2\alpha +1)}}{=}O(1)\cdot  
n^{1-2\alpha + \eps(2\alpha -1)} \cdot k_n^{2\alpha -3} .
\end{split}
\end{equation*}
So
\begin{equation*}
\begin{split}
&k_n \cdot n^{1-2\alpha + \eps(2\alpha -1)} \cdot k_n^{2\alpha -3} 
\ll n^{1 - 2\alpha + \eps(2\alpha -1)}  \cdot k_n^{2\alpha - 1}  = o(1),
\end{split}
\end{equation*}
if we select $\eps$ small enough, depending on $\alpha$. 

%Suppose now that $k_n \leq n^{1/3 - 2\frac{2\alpha}{2\alpha -1}\eps}$. 
%In this case, the term obtained in~\eqref{eq:2nd_term-1st_regime}. 
%Regarding the term obtained in~\eqref{eq:2nd_term-2nd_regime}, note that 
%$k_n /n \leq n^{-2/3}$. So since in this case $2\alpha > 3/2$, it follows that 
%the term tends to 0, provided that $\eps$ is small enough, depending on $\alpha$. 
%
%For $\alpha =3/4$, the above implies that 
%\begin{equation*}
%\begin{split}
%&\frac{k_n}{\log k_n} \cdot {k_n \choose 2}^{-1} \cdot \expH^{-1} \cdot k_n \cdot 
%e^{(1/2 - \alpha)R_n + \eps R_n}
%\int_{-I_n}^{I_n} \int_{I_\eps ( k_n )} 
%e^{-\lambda_y} 
%\frac{\lambda_y^{k_n-1}}{(k_n-1)!} e^{-\alpha y} dy dx =o(1).
%\end{split}
%\end{equation*}

\subsubsection{The sum of~\eqref{eq:sum_5}}

We will give an upper bound on the expectation of 
$$\expH^{-1} \cdot \sum_{p_1,p_2\in\Pcal \setminus \{(0,y)\}}\ind{p_1\in \BallHyp{(0,y)} \setminus 
\BallPo{(0,y)}}\ind{p_2 \in \BallHyp{(0,y)}\cap \BallHyp{p_1}} \cdot 
D_\H (y,k_n-2;\Pcal \setminus \{ p_1, p_2\}).$$
Let us set $p=(0,y)$. Recall that $\BallSym{p}\cap \Rcal([R_n - y + 2 \log\left(\frac{\pi}{2}\right),R_n]) = \emptyset$. Thus, the summand in the above sum is equal to 0, when 
$y(p_1) > R_n - y + 2 \log (\pi/2)$. 

Recall that  we have  
\begin{equation} \label{eq:ball_multitude}
\begin{split}
\Exp{D_\H (y,k_n-2;\Pcal \setminus \{ p_1, p_2\})}  =O(1)\cdot   \Pop{y}{k_n-2}.
\end{split}
\end{equation}


%Let $X(p,p_1) : =|\Pcal \cap \BallHyp{p}\cap \BallHyp{p_1}|$, thus $X(p,p_1)$ is 
%Poisson-distributed with parameter equal to $\mu_{\Pcal} ( \BallHyp{p}\cap \BallHyp{p_1})=:\lambda(p,p_1) $.
%With $X' (p,p_1):= |\Pcal \cap \BallHyp{p} \setminus ( \BallHyp{p}\cap \BallHyp{p_1})|$, we also have that $X' (p,p_1)$ is Poisson-distributed with 
%parameter $\mu_{\Pcal} (\BallHyp{p}) - \mu_{\Pcal} ( \BallHyp{p}\cap \BallHyp{p_1})$ and it is 
%independent of $X(p,p_1)$. 
%Thus, the above sum is written as 
%\begin{equation*}
%\begin{split}
%&\sum_{p_1\in\Pcal \setminus \{(0,y)\}}\ind{p_1\in \BallSym{(0,y)}} \cdot 
%\Prob{X'(p,p_1) + X(p,p_1)= k_n-1}
%\cdot  X(p,p_1) \leq \\
%&\sum_{p_1\in\Pcal \setminus \{(0,y)\}}\ind{p_1\in \BallSym{(0,y)}} \cdot 
%\Prob{X'(p,p_1) + \ell = k_n-1} \cdot \ell \cdot \Prob{X(p,p_1)=\ell}  \\
%& \hspace{2cm} + \Prob{|X(p,p_1) - \lambda(p,p_1) | > \lambda(p,p_1) /2}
%\end{split}
%\end{equation*}

We are going to define an extended ball around $p$ that contains both $\BallHyp{p}$ and $\BallPo{p}$ and we are going to use as an upper approximation on $\BallHyp{p}$.
For $K > 0$ as in Lemma~\ref{lem:asymptotics_Omega_hyperbolic}, we define:
\begin{equation}\label{eq:def_fatball_K}
	\FatBallHyp{p} : = \{ p^\prime : y^\prime < R_n - y, \ |x^\prime| < (1+K) e^{\frac{1}{2} (y + y^\prime)}  \}.
\end{equation}
Observe that,
\begin{equation*} %\label{eq:ball_inclusion} 
\BallHyp{p} \cap \Rcal ([0,r(p))) \subseteq \FatBallHyp{p}
\end{equation*}
and 
\begin{equation*} %\label{eq:ball_inclusion_lower}
\BallHyp{p} \cap \Rcal ([r(p),R_n]) = \Rcal ([r(p),R_n]).
\end{equation*}
We thus conclude that 
\begin{equation} \label{eq:ball_inclusions}
\BallHyp{p} \subseteq \FatBallHyp{p} \cup \Rcal([r(p),R_n]).
\end{equation}
Hence, if we set 
\[
h_y(p_1, \Pcal) := \ind{p_1 \in \BallHyp{p}\setminus \BallPo{p}} \cdot    
\left( \mu_{\alpha,\nu} \left( \FatBallHyp{p_1} \cap \FatBallHyp{p} \right)
+ \mu_{\alpha,\nu} \left( \Rcal([r(p),R_n]) \right) \right),
\]
then 
\begin{equation*}
\begin{split}
&\ind{p_1\in \BallHyp{p}\setminus \BallPo{p}} \cdot \Exp{ \left(\sum_{p_2 \in \Pcal \setminus 
\{p,p_1\}} \ind{p_2 \in \BallHyp{p} \cap \BallPo{p_1}}\right) \cdot 
D_\H (y,k_n-2;\Pcal \setminus \{ p_1, p_2\})
} \\
&=O(1)\cdot
\ind{p_1\in \BallHyp{p}\setminus \BallPo{p}} \cdot \mu_{\alpha,\nu} (\BallHyp{p}\cap \BallHyp{p_1}) \cdot  \Pop{y}{k_n-2} \\
& \leq O(1) \cdot  h_y (p_1, \Pcal)\cdot  \Pop{y}{k_n-2}. 
\end{split}
\end{equation*}
To calculate the expectation of the above function we need to approximate the 
intersection of the two balls $\FatBallHyp{p}$ and $\FatBallHyp{p_1}$, 
where $p_1= (x_1,y_1)$. 
Let us assume without loss of generality that $x_1 > 0$. 
The right boundary of $\FatBallHyp{p}$ is given by the equation 
$x = x(y_1) = (1+K)e^{\frac{1}{2} (y + y_1)}$ whereas the left boundary of $\FatBallHyp{p_1}$ is given by the curve $x = x(y_1)= x_1 - (1+ K)e^{\frac{1}{2} (y + y_1)}.$ 

The equation that determines the intersecting point of the two curves is
\[
	x_1 - (1+K)e^{(\hat{y} + y_1)/2}= (1+K) e^{(\hat{y} + y)/2},
\]
where $\hat{y}$ is the $y$-coordinate of the intersecting point. 
We can solve the above for $\hat{y}$  
\begin{equation*} 
\begin{split}
x_1 &=(1+K) e^{\hat{y}/2} \left( e^{y/2} + e^{y_1/2} \right).
\end{split}
\end{equation*}
But since $p_1=(x_1,y_1)  \in \BallSym{p}$, we also have $x_1 > e^{\frac{y + y_1}{2}}$. Therefore, 
\begin{equation*}
\begin{split}
 e^{\hat{y}/2}& > \frac{1}{1+K}~\frac{e^{\frac{y + y_1}{2}}}{ e^{y/2}+ e^{y_1/2}} \geq 
\frac{1}{2(1+K)}~\frac{e^{\frac{y_1 + y}{2}}}{ e^{\max \{y, y_1\} /2}} 
> \frac{1}{2(1 + K)} ~ e^{\min\{y, y_1\}/2}. 
 \end{split}
\end{equation*}
The above yields
\begin{equation} \label{eq:to_use_I}
\hat{y} > \min\{y, y_1\} - 2\log(2(1+K)) := c(y_1, y). 
\end{equation}
which, in turn, implies the following 
\begin{equation}\label{eq:intersex_approx}
	p \in \FatBallHyp{(0,y)}\cap \FatBallHyp{p_1} \Rightarrow y(p) \ge c(y_1,y).
\end{equation}
We thus conclude that 
\[ 
	\BallHyp{p_1} \cap \BallHyp{p} \subseteq \FatBallHyp{p} \cap \Rcal([c(y_1,y), R_n]) 
	\bigcup \Rcal ([R_n - y,R_n]),
\]
which in turn implies that
\[
	\mu_{\alpha,\nu} \left( \FatBallHyp{p_1} \cap \BallHyp{p} \right) \leq 
	\mu_{\alpha,\nu}\left( \FatBallHyp{p} \cap  \Rcal([c(y_1,y), R_n]\right) + 
	\mu_{\alpha,\nu} (\Rcal ([R_n - y, R_n]) ).
\]
Therefore, 
\begin{align} 
	h_y(p_1, \Pcal) &\leq \ind{p_1 \in \BallHyp{p}\setminus \BallPo{p}} 
    	\mu_{\alpha,\nu}  \left( \FatBallHyp{p} \cap  \Rcal([c(y_1,y), R_n])\right)
        \label{eq:h_upper_bound_1}\\
	&\hspace{10pt}+ \ind{p_1 \in \BallHyp{p}\setminus \BallPo{p}}
    	\mu_{\alpha,\nu}  \left( \Rcal ([R_n - y, R_n]) \right). \label{eq:h_upper_bound_2}
\end{align}

Now,~\eqref{eq:Campbell-Mecke} gives
\begin{align*}
\Exp{\left( \sum_{p_1 \in \Pcal} 
		h_y (p_1, \Pcal \setminus \{ p_1 \})\right)} 
	=\frac{\nu \alpha}{\pi} \int_{\Rcal_n} \Exp{h_y((x_1,y_1), \Pcal \setminus \{(x_1,y_1)\})}
		e^{-\alpha y_1} \, dx_1 \, dy_1.
\end{align*}
Recall that $(\BallSym{(0,y)})\cap \Rcal([R_n - y + 2 \log\left(\frac{\pi}{2}\right),R_n]) = \emptyset$. 
We will calculate the measure of each one of the two summands. The first one is:
\begin{align*}
	\mu_{\alpha,\nu}\left( \FatBallHyp{(0,y)} \cap  \Rcal([c(y_1,y), R_n])\right) 
	&\leq (1+ K) \frac{\nu \alpha}{\pi} \cdot e^{y/2}  \int_{c(y_1,y)}^{R_n} e^{-(\alpha - \frac{1}{2}) y'} \, dy' \\
	&=  \bigO{e^{\frac{y}{2} - (\alpha-\frac{1}{2}) \min \{y,y_1\}}}.
\end{align*}

The second summand is: 
\begin{align*}
	\mu_{\alpha,\nu} \left( \Rcal([R_n - y,R_n]) \right) 
    &= \frac{\nu \alpha}{\pi} \int_{R_n - y}^{R_n} \pi e^{\frac{R_n}{2}} e^{-\alpha y'} \, dy' 
    	= \bigO{e^{\frac{R_n}{2}} e^{-\alpha (R_n-y)}} = \bigO{e^{\alpha y - (\alpha - \frac{1}{2})R_n}}. 
\end{align*}
%Thus, 
%\[ 
%\Exp{\left( \sum_{p_1 \in \Pcal} h_y(p_1, \Pcal \setminus \{ p_1 \})\right)}
%    = \bigO{e^{\frac{y_n}{2} -(\alpha - \frac{1}{2}) \min \{y,y_n\}} + e^{\alpha y_n - (\alpha - \frac{1}{2})R_n}}.
%\]
Using these, 
\begin{eqnarray} 
	\lefteqn{\int_{\Rcal_n ([0, R_n - y_n + 2 \ln \frac{\pi}{2}])} \Exp{h_y((x_1,y_1), \Pcal \setminus \{(x_1,y_1)\})} 
    e^{-\alpha y_1} \, dx_1 \, dy_1 =}  \nonumber \\
	& & O(1) \cdot \left(\int_{\Rcal_n ([0, R_n - y+ 2 \ln \frac{\pi}{2}])} \ind{(x_1,y_1) \in \BallSym{p}} 		e^{\frac{y}{2} - (\alpha - \frac{1}{2}) \min \{y,y_1\} - \alpha y_1} \, dx_1 \, dy_1 \right.  \nonumber \\ 
	& & \hspace{1cm}+\left. \int_{\Rcal_n ([0, R_n - y + 2 \ln \frac{\pi}{2}])} 
    	\ind{(x,y) \in \BallHyp{(0,y)}} 
    	e^{\alpha y - (\alpha - \frac{1}{2})R_n - \alpha y_1} \, dx_1 \, dy_1\right). \nonumber \\
	& &\label{eq:Mecke_sum}
\end{eqnarray}
Now, Lemma~\ref{lem:asymptotics_Omega_hyperbolic} implies that 
for any $y \in [0, R_n - y_n + 2 \ln \frac{\pi}{2}]$, we have 
\[ 
	\int \ind{(x_1,y_1) \in \BallSym{(0,y)}} \, dx_1 \leq K e^{\frac{3}{2} (y_1 + y) - R_n}.
\]
Therefore, the first integral in~\eqref{eq:Mecke_sum} is 
\begin{align*}
	&\hspace{-20pt}\int_{\Rcal ([0, R_n - y + 2 \ln \frac{\pi}{2}])} \Exp{h_y((x_1,y_1), \Pcal \setminus \{(x_1,y_1)\})} 
    	e^{-\alpha y_1} \, dx_1 \, dy_1 \\
	&= O(1) \cdot e^{2 y - R_n} \int_{0}^{R_n - y + 2 \ln \frac{\pi}{2}} 
    	e^{\frac{3y_1}{2} - (\alpha - \frac{1}{2})\min\{y_1,y\} - \alpha y_1} \, dy_1 \\
 	&=  O(1) \cdot e^{2 y - R_n} \left( \int_{0}^{y} e^{\frac{3y_1}{2} - (2\alpha - \frac{1}{2})y_1} \, dy_1 
 		+ e^{-(\alpha-\frac{1}{2}) y}\int_{y}^{R_n - y + 2 \ln \frac{\pi}{2}} e^{(\frac{3}{2} - \alpha) y_1} \, dy_1 \right)\\
  	&= O(1) \cdot 
	\begin{cases}
	e^{(4-2\alpha) y - R_n}, & \mbox{if $\alpha < 2$} \\
	R_n \cdot e^{2y - R_n}, & \mbox{if $\alpha \geq 2$}
	\end{cases}
	+
	\begin{cases}
	e^{-(\alpha - \frac{1}{2})R_n +y}, & \mbox{if $\alpha < 3/2$} \\
	R_n \cdot  e^{2(2-\alpha)y - R_n}, & \mbox{if $\alpha \geq 3/2$}
	\end{cases}.
\end{align*}

Similarly, the second integral in~\eqref{eq:Mecke_sum} is
\begin{align*}
	&\hspace{-30pt}\int_{\Rcal ([0, R_n - y + 2 \ln \frac{\pi}{2}])} \ind{(x_1,y_1) \in \BallSym{(0,y)}} e^{\alpha y - (\alpha - \frac{1}{2})R_n - \alpha y_1} \, dx_1 \, dy_1\\
	&= e^{\frac{3y}{2} - R_n + \alpha y - (\alpha - \frac{1}{2})R_n} 
    	\cdot \int_{0}^{R_n - y + 2 \ln \frac{\pi}{2}} e^{\frac{3y_1}{2}-\alpha y_1} \, dy_1\\
	&= O(1)\cdot 
	\begin{cases} 
	e^{\frac{3y}{2} - R_n + \alpha y - (\alpha - \frac{1}{2})R_n + (\frac{3}{2} - \alpha)(R_n-y)} 
	, & \mbox{if $\alpha < 3/2$} \\ 
	R_n \cdot e^{(\frac{3}{2} +\alpha)y -  (\alpha + \frac{1}{2})R_n}, & \mbox{if 
	$\alpha \geq 3/2$}
	\end{cases}	\\
	&= O(1) \cdot 
	\begin{cases}
	  e^{-(2\alpha-1) R_n + 2 \alpha y}, & \mbox{if $\alpha < 3/2$} \\
	  R_n \cdot e^{(\frac{3}{2} +\alpha)y -  (\alpha + \frac{1}{2})R_n}, & \mbox{if 
	$\alpha \geq 3/2$}
	\end{cases}.
\end{align*}

We thus conclude that 
\begin{equation} \label{eq:upper_bound_faulty_edges} 
\Exp{\left( \sum_{p_1 \in \Pcal \setminus\{p\}} 
		h_y (p_1, \Pcal \setminus \{ p_1 \})\right)}  = O(1) \cdot 
\left( \Lambda_1 + \Lambda_2 + \Lambda_3 \right),
\end{equation}
where 
\begin{align*}
 \Lambda_1 &= \Lambda_1 (y) =\begin{cases}
	e^{(4-2\alpha) y - R_n}, & \mbox{if $\alpha < 2$} \\
	R_n \cdot e^{2y - R_n}, & \mbox{if $\alpha \geq 2$}
	\end{cases},  \\
\Lambda_2 &= \Lambda_2 (y) = 
\begin{cases}
	e^{-(\alpha - \frac{1}{2})R_n +y}, & \mbox{if $\alpha < 3/2$} \\
	R_n \cdot  e^{2(2-\alpha)y - R_n}, & \mbox{if $\alpha \geq 3/2$}
	\end{cases} \\
	&\stackrel{2-\alpha \leq 1/2}{\leq}
	\begin{cases}
	e^{-(\alpha - \frac{1}{2})R_n +y}, & \mbox{if $\alpha < 3/2$} \\
	R_n \cdot  e^{y - R_n}, & \mbox{if $\alpha \geq 3/2$}
	\end{cases}\\
\mbox{and } \Lambda_3 &= \Lambda_3 (y) = 
\begin{cases}
	  e^{-(2\alpha-1) R_n + 2 \alpha y}, & \mbox{if $\alpha < 3/2$} \\
	  R_n \cdot e^{(\frac{3}{2} +\alpha)y -  (\alpha + \frac{1}{2})R_n}, & \mbox{if 
	$\alpha \geq 3/2$}
	\end{cases}.
\end{align*}

Substituting this into~\eqref{eq:expectation_total} we now need to calculate:
\begin{align*}
&{k_n\choose 2}^{-1}\cdot \expH^{-1} \cdot \int_{-I_n}^{I_n} 
\int_{I_\eps(k_n)} \Exp{\left( \sum_{p_1 \in \Pcal} h_y (p_1, \Pcal \setminus \{ p_1 \})\right)} \cdot 
 \Pop{y}{k_n-2} \cdot e^{-\alpha y} dy.
\end{align*}
Firstly, note that as $\expH = \Theta (1) \cdot n \cdot k_n^{-(2\alpha +1)}$, we have 
$$ {k_n\choose 2}^{-1}\cdot \expH^{-1} = O(1) \cdot \frac{k_n^{2\alpha-1}}{n}.$$
Also, $\Exp{\left( \sum_{p_1 \in \Pcal} h_y (p_1, \Pcal \setminus \{ p_1 \})\right)}$ is given 
as the sum of $\Lambda_1, \Lambda_2$ and $\Lambda_3$ (cf.~\eqref{eq:upper_bound_faulty_edges}). 
We need to integrate these expressions together with $e^{-\lambda_y} \frac{\lambda_y^{k_n-2}}{(k_n-2)!}$.
For this, we will use again Claim~\ref{eq:gamma_approx}.


Using $n=\nu e^{R_n/2}$ as well as Claim~\ref{eq:gamma_approx}, 
we deduce
\begin{align*} 
M_1:= \int_{-I_n}^{I_n} \int_{I_\eps (k_n)} \Lambda_1 (y)  \Pop{y}{k_n-2} e^{-\alpha y} dy
&= O(1) \cdot 
\begin{cases} 
\frac{k_n^{7-6\alpha}}{n}, & \mbox{if $\alpha <2$} \\
R_n^2 \cdot \frac{k_n^{3-2\alpha}}{n}, & \mbox{if $\alpha \geq 2$}
\end{cases}. 
%\\
%& = O(1) \cdot 
%\begin{cases} 
%\frac{k_n^{2(1+\eps) (4-\alpha)}}{n}, \mbox{if $\alpha <2$} \\
%R_n^2 \cdot \frac{k_n^{2(1-\eps)(2-\alpha)}}{n}, \mbox{if $\alpha \geq 2$}
%\end{cases}
\end{align*}
\begin{align*} 
M_2:= \int_{-I_n}^{I_n} \int_{I_\eps (k_n)} \Lambda_2 (y)  \Pop{y}{k_n-2} e^{-\alpha y} dy
&= O(1) \cdot 
\begin{cases}
e^{-(\alpha - 1)R_n} k_n^{-2\alpha+1}, & \mbox{if $\alpha < 3/2$} \\
R_n  \cdot \frac{k_n^{7-6\alpha }}{n}, & \mbox{if $\alpha \geq 3/2$}
\end{cases} 
\\
=\begin{cases}
\frac{k_n^{1-2\alpha}}{n^{2(\alpha-1)}}, & \mbox{if $\alpha < 3/2$} \\
R_n  \cdot \frac{k_n^{7-6\alpha }}{n}, & \mbox{if $\alpha \geq 3/2$}
\end{cases}
\end{align*}
and finally 
\begin{align*} 
M_3:=\int_{-I_n}^{I_n} \int_{I_\eps (k_n)} \Lambda_3 (y)  \Pop{y}{k_n-2} e^{-\alpha y} dy
&= O(1) \cdot 
\begin{cases} 
e^{-(2\alpha - 3/2) R_n} k_n^{2\alpha - 1}, & \mbox{if $\alpha <3/2$} \\ 
R_n \cdot n \cdot e^{-(\alpha +\frac12)R_n} k_n^2, &\mbox{if $\alpha \geq 3/2$}
\end{cases} \\
&=O(1) \cdot 
\begin{cases} 
\frac{k_n^{2\alpha - 1}}{n^{4\alpha -3}}, & \mbox{if $\alpha <3/2$} \\ 
R_n \cdot \frac{k_n^2}{n^{2\alpha}}, &\mbox{if $\alpha \geq 3/2$}
\end{cases}  .
\end{align*}
Setting 
$$ J_3 = \frac{k_n^{2\alpha-1}}{n}\cdot  \left(M_1+ M_2 + M_3 \right) $$
we deduce that 
\begin{align*}
&{k_n\choose 2}^{-1}\cdot \expH^{-1} \cdot \int_{-I_n}^{I_n} \int_{I_\eps (k_n)} \Exp{\left( \sum_{p_1 \in \Pcal \setminus \{(0,y)\}} h_y (p_1, \Pcal \setminus \{ p_1 \})\right)} \cdot  \Pop{y}{k_n-2} \cdot e^{-\alpha y} dy \\
&= O(1) \cdot J_3
\end{align*}


%\begin{align*}
%&\stackrel{I_n=O(1)\cdot e^{R_n/2}}{=} O(1)\cdot {k_n\choose 2}^{-1} \cdot \expH^{-1} 
%\cdot \left(  e^{2(1+\eps)(4-2\alpha) \ln k_n - R_n/2} +e^{-(\alpha - 1)R_n +2(1+\eps) \ln k_n}  \right.
%\\
%&\left. \hspace{3cm}+ e^{-(2\alpha - 3/2) R_n + 4(1+\eps) \alpha \ln k_n}  \right) \\
%&\stackrel{\expH = \Theta (1) \cdot n \cdot k_n^{2\alpha +1}}{=}
%O(1) \cdot \frac{1}{n \cdot k_n^{2\alpha + 3}} 
%\cdot \left(  e^{2(1+\eps)(4-2\alpha)\ln  k_n - R_n/2} +e^{-(\alpha - 1)R_n +2(1+\eps)\ln k_n}  \right.
%\\
%&\left. \hspace{3cm}+ e^{-(2\alpha - 3/2) R_n + 4(1+\eps) \alpha \ln k_n}  \right).
%\end{align*}
%Let 
%\begin{align*}
%J_3 &: = \frac{1}{n \cdot k_n^{2\alpha + 3}} 
%\cdot \left(  e^{2(1+\eps)(4-2\alpha) \ln k_n - R_n/2} +e^{-(\alpha - 1)R_n +2(1+\eps)\ln k_n} 
%+ e^{-(2\alpha - 3/2) R_n + 4(1+\eps) \alpha \ln k_n}  \right) \\
%& = O(1) \cdot \frac{1}{n \cdot k_n^{2\alpha + 3}} \cdot 
%\left( 
%\frac{k_n^{2(1+\eps) (4-2\alpha)}}{n} + \frac{k_n^{2(1+\eps)}}{n^{2(\alpha-1)}} 
%+\frac{k_n^{4\alpha (1+\eps)}}{n^{4\alpha - 3}}.
%\right) 
%\end{align*}

Now, we will consider the two cases according to the value of $\alpha$. 
Assume first that $1/2 < \alpha \leq 3/4$. 
In this case, we want to show that 
\begin{equation} \label{eq:int3_to_prove_I}
\lim_{n \to \infty} k_n^{4\alpha -2} \cdot J_3 = 0. 
\end{equation}
Using the above expression for $J_3$, we have 
\begin{align*} 
 k_n^{4\alpha -2} \cdot J_3 &= O(1) \cdot  
 \frac{k_n^{6\alpha -3}}{n} \cdot 
\left( 
\frac{k_n^{7-6\alpha}}{n} + \frac{k_n^{2(1-\alpha)}}{n^{2(\alpha-1)}} 
+\frac{k_n^{2\alpha-1}}{n^{4\alpha - 3}}.
\right) 
\end{align*}
We wish to show that each one of the above three terms is $o(1)$ for $k_n = O(n^{\frac{1}{2\alpha +1}})$. 
For the first one we have 
$$ \frac{k_n^{6\alpha -3}}{n} \cdot \frac{k_n^{7-6\alpha}}{n} = \frac{k_n^{4}}{n^2} = O(1) \cdot n^{\frac{4}{2\alpha +1} -2} \stackrel{\alpha >1/2}{=} o(1). 
$$
The second one yields: 
$$ \frac{k_n^{6\alpha -3}}{n} \cdot  \frac{k_n^{-2\alpha+1}}{n^{2(\alpha-1)}} =\frac{k_n^{4\alpha -2}}{n^{2\alpha -1}} =O(1) \frac{n^{\frac{4\alpha -2}{2\alpha+1}}}{n^{2\alpha -1}}.$$
We need to show that $\frac{4\alpha -2}{2\alpha+1}< 2\alpha -1$. Indeed, rearranging this 
 yields, $4\alpha -2 < 4\alpha^2 -1$, which is equivalent to $0< 4\alpha^2 - 4\alpha +1=(2\alpha- 1)^2$. This holds for all $\alpha >1/2$.  
 
 Finally, the third one yields: 
$$ \frac{k_n^{6\alpha -3}}{n} \cdot \frac{k_n^{2\alpha -1}}{n^{4\alpha - 3}}  
= \frac{k_n^{8\alpha -4}}{n^{2(2\alpha -1)}} = \frac{k_n^{4(2\alpha -1)}}{n^{2(2\alpha-1)}}.$$
But $k_n^4 \leq O(1)\cdot n^{\frac{4}{2\alpha+1}} = o(n^2)$, as $2\alpha +1 >2$.
 
For $\alpha >3/4$, we would like to show that 
\begin{equation} \label{eq:int3_to_prove_II}
\lim_{n \to \infty} k_n \cdot J_3 = 0. 
\end{equation}
Firstly, if $3/4 < \alpha < 3/2$ we have, 
\begin{align*} 
 k_n \cdot J_3 &= O(1) \cdot  
 \frac{k_n^{2\alpha}}{n} \cdot 
\left( 
\frac{k_n^{7-6\alpha}}{n} + \frac{k_n^{-2\alpha +1}}{n^{2(\alpha-1)}} 
+\frac{k_n^{2\alpha -1}}{n^{4\alpha - 3}}.
\right) 
\end{align*}
As above we will deal with the three term of this. 
For the first one we have 
$$  \frac{k_n^{2\alpha}}{n} \cdot 
\frac{k_n^{7-6\alpha}}{n}  = \frac{k_n^{7-4\alpha }}{n^2} \stackrel{\alpha >3/4}{<}  
\frac{k_n^{4}}{n^2} =o(1). 
$$
The second one yields: 
$$\frac{k_n^{2\alpha}}{n} \cdot 
\frac{k_n^{-2\alpha +1}}{n^{2(\alpha-1)}} =
\frac{k_n}{n^{2\alpha-1}} \stackrel{\alpha > 3/4}{<} \frac{k_n}{n^{1/2}} = O(1) 
\frac{n^{\frac{1}{2\alpha+1}}}{n^{1/2}} =o(1).$$
Finally, the third one yields: 
$$  \frac{k_n^{2\alpha}}{n} \cdot \frac{k_n^{2\alpha -1}}{n^{4\alpha - 3}}
= \frac{k_n^{4\alpha -1}}{n^{2(2\alpha -1)}}=
O(1) \frac{n^{\frac{4\alpha-1}{2\alpha+1}}}{n^{2(2\alpha -1)}}.$$
We need to show that $\frac{4\alpha-1}{2\alpha+1}< 2(2\alpha -1)$, which is 
equivalent to $8\alpha^2 - 4 \alpha -1>0$; this is indeed the case for any $\alpha \geq 3/4$. 

 
 For $3/2 \leq \alpha <2$, it is only $M_2$ and $M_3$ that change values. 
 In particular, for any $\alpha \geq 3/2$ we have 
 $$\frac{k_n}{n} \cdot M_2 =O(1)\cdot R_n \cdot \frac{k_n^{2\alpha}}{n} \cdot 
\frac{k_n^{7-6\alpha}}{n} =o(1),$$
as above. 
Also, 
$$ \frac{k_n}{n} \cdot M_3 = O(1)\cdot
R_n \cdot \frac{k_n}{n} \cdot \frac{k_n^{2}}{n^{2\alpha}}
= R_n\cdot  \frac{k_n^{3}}{n^{2\alpha +1}} = o(1),
$$
since $k_n = o(n^{1/2})$ (and, therefore, $k_n^3 = o(n^{3/2})$) but $2\alpha +1 >2$. 

If $\alpha \geq 2$ too, then $M_1$ changes value and we have 
$$\frac{k_n}{n} \cdot M_1 =O(1) \cdot R_n^2 \cdot 
\frac{k_n}{n}  \cdot \frac{k_n^{3-2\alpha}}{n} = \frac{k_n^{4 - 2\alpha}}{n^2}= o(1),$$ 
since $\alpha \geq 2$. 
 
%%%%%%%%%%%%%%%%%%%%%%%%%%% old third summand %%%%%%%%%
%\begin{equation*}
%\begin{split}
%&\Exp {\sum_{p_1 \in \Pcal \setminus \{(0,y) \}} D_{\H}(y,k_n-1; \Pcal \setminus \{ (0,y),p_1\})  \ind{p_1\in \BallSym{(0,y)}}}\cdot k_n \cdot 2 \expH^{-1} \leq \\
%&\mu_{\alpha, \nu} (\BallSym{(0,y)})\cdot \Prob{D_{\H}((0,y))=k_n-1)} \cdot k_n \cdot 2 \expH^{-1}. 
%\end{split}
%\end{equation*} 
%We calculate $\mu_{\alpha, \nu} (\BallSym{(0,y)})$ in the following claim. 
%\begin{lemma}\label{lem:sym_diff_measure}
%Let $\eps \in (0,1)$. For any $0< y < (1- \eps) R_n$ we have 
%$$\mu_{\alpha, \nu} (\BallSym{p}) =\Theta(1) \cdot \begin{cases} 
%e^{(1/2-\alpha)R_n + \alpha y}, & \mbox{if } \alpha < 3/2 \\
%		r_y e^{3y/2 - R_n}, & \mbox{if }\alpha = 3/2\\
%		e^{3y/2 - R_n}, &  \mbox{if } \alpha > 3/2
%	\end{cases} $$
%\end{lemma}
%\begin{proof}
%We will now give an upper bound on $\mu (\BallSym{(0,y)})$. 
%We set $r_y = R_n- y$.
%Lemma~\ref{lem:asymptotics_Omega_hyperbolic} implies that for such a $p'$, if a point $p' = (x',y')$ belongs to $\BallSym{(0,y))} \cap \Rcal ([0,r_y])$ then 
%\[
%	|x' | = \Theta(1) \cdot e^{\frac{3}{2} (y + y') - R_n}.
%\]
%Now, if $y' \in [r_y, r_y + 2 \ln \frac{\pi}{2})]$ and also $p' \in \BallHyp{(0,y)} \bigtriangleup \BallPo{(0,y)}$, then 
%\[
%	|x'| =\frac{\pi}{2} e^{R_n/2} - e^{\frac{1}{2} (y + y')}.
%\]
%Finally, note that no point in $\Rcal ([r_y+2\ln \frac{\pi}{2},R_n])$ belongs to $\BallHyp{(0,y)} \bigtriangleup \BallPo{(0,y)}$, as this region is in fact part of $\BallHyp{(0,y)} \cap \BallPo{(0,y)}$. We first compute the expected number of points $p' \in \BallHyp{(0,y)} \bigtriangleup \BallPo{(0,y)}$ that have $R_n - y' \le r_y$. The result depends on the value of $\alpha$, yielding the following three cases
%\begin{align*}
%	\mu_{\alpha,\nu} (\BallSym{(0,y)} \cap \Rcal ([0,r_y])) 
%	&= \Theta(1) \cdot e^{3y/2 - R_n}\int_0^{r_y} e^{(3/2-\alpha) y'} \, dy' \\
%	&= \Theta(1)\cdot \begin{cases} e^{(1/2-\alpha)R_n + \alpha y}, & \mbox{if } \alpha < 3/2 \\
%		r_y e^{3y/2 - R_n}, & \mbox{if }\alpha = 3/2\\
%		e^{3y/2 - R_n}, &  \mbox{if } \alpha > 3/2
%	\end{cases}.
%\end{align*}
%Furthermore, 
%\begin{align*}
%\mu_{\alpha,\nu}& (\BallSym{(0,y)} \cap \Rcal ([r_y,r_y +2\ln (\pi/2)])) \leq
%\pi e^{R_n/2} \int_{r_y}^{r_y + 2 \ln (\pi/2)} e^{-\alpha y'} dy'\\
%& = O(1)\cdot 
%e^{R_n/2 - \alpha (R_n-y)} = O(1) \cdot e^{(1/2 -\alpha) R_n + \alpha y}. 
%\end{align*}
%Therefore, 
%\begin{equation*}
%\begin{split} 
%\mu_{\alpha, \nu} (\BallSym{p}) &=
%\mu_{\alpha,\nu} (\BallSym{(0,y)} \cap \Rcal ([0,r_y]))  +
%\mu_{\alpha,\nu} (\BallSym{(0,y)} \cap \Rcal ([r_y,r_y +2\ln (\pi/2)])) \\
%&=\Theta(1) \cdot \begin{cases} e^{(1/2-\alpha)R_n + \alpha y}, & \mbox{if } \alpha < 3/2 \\
%		r_y e^{3y/2 - R_n}, & \mbox{if }\alpha = 3/2\\
%		e^{3y/2 - R_n}, &  \mbox{if } \alpha > 3/2
%	\end{cases}
%\end{split}
%\end{equation*}
%\end{proof}
%
%Using the above claim, we deduce that for any $\alpha \leq 3/2$ we have 
%\begin{equation*}
%\begin{split}
%&\Exp {\sum_{p_1 \in \Pcal \setminus \{(0,y) \}}  \ind{p_1\in \BallSym{(0,y)}} 
%D_{\H}(y,k_n-1; \Pcal \setminus \{ (0,y),p_1\}) }\cdot k_n \cdot 2 \expH^{-1} \\
%&\leq 
%R_n e^{(1/2 -\alpha)R_n + \alpha y} \cdot \Prob{D_{\H}((0,y))=k_n-1)} \cdot k_n \cdot 2 \expH^{-1} \\
%&= R_n e^{(1/2 -\alpha)R_n + (\alpha-1/2)y} \cdot e^{y/2}\cdot \Prob{D_{\H}((0,y))=k_n-1)} \cdot k_n \cdot 2 \expH^{-1}. 
%\end{split}
%\end{equation*}
%But recall that $y< 2(1+\eps ) \ln k_n$. Since $\alpha >1/2$,
%\begin{equation*}
%\begin{split}
%&\Exp {\sum_{p_1 \in \Pcal \setminus \{(0,y) \}}  \ind{p_1\in \BallSym{(0,y)}} 
%D_{\H}(y,k_n-1; \Pcal \setminus \{ (0,y),p_1\}) }\cdot k_n \cdot 2 \expH^{-1}\\
%&\leq R_n e^{(1/2 -\alpha)R_n + (\alpha-1/2)2(1+\eps)\ln k_n} \cdot e^{y/2}\cdot \Prob{D_{\H}((0,y))=k_n-1)} \cdot k_n \cdot 2 \expH^{-1} \\
%&= R_n e^{(1/2 -\alpha)R_n} \cdot k_n^{(1+\eps)(2\alpha -1)} \cdot e^{y/2}\cdot \Prob{D_{\H}((0,y))=k_n-1)} \cdot k_n \cdot 2 \expH^{-1}.
%\end{split}
%\end{equation*}
%Substituting this into~\eqref{eq:expectation_total} yields for $n$ sufficiently large
%\begin{equation*}
%\begin{split} 
%&R_n {k_n \choose 2}^{-1} \cdot k_n \cdot 2 \expH^{-1} e^{(1/2 -\alpha)R_n} \cdot 
%k_n^{(1+\eps) (2\alpha-1)}\times \\
%& \hspace{2cm} 
%\int_{-I_{n}}^{I_n} \int_{ 2(1-\eps ) \ln k_n}^{2(1+\eps ) \ln k_n} \Prob{D_{\H}((0,y))=k_n-1)} e^{y/2 -\alpha y} dydx. 
%%&\leq   
%%5 R_n {k_n \choose 2}^{-1} \cdot k_n \cdot 2 \expH^{-1} e^{(1/2 -\alpha)R_n}
%%\int_{-I_{n}}^{I_n} \int_0^{R_n} \Prob{D_{\H}((0,y))=k_n-1)}  dy dx.
%\end{split}
%\end{equation*}
%Now 
%\begin{equation*}
%\begin{split}
%&\int_{-I_{n}}^{I_n} \int_{ 2(1-\eps ) \ln k_n}^{2(1+\eps ) \ln k_n} \Prob{D_{\H}((0,y))=k_n-1)} e^{y/2 -\alpha y} dydx =\\
%&= O(1)\cdot I_n \cdot  \int_{2(1-\eps)\ln k_n}^{2(1+\eps)\ln k_n} 
%e^{y/2} \cdot e^{-\lambda_y} 
%\frac{\lambda_y^{k_n-1}}{(k_n-1)!} e^{-\alpha y} dy dx \\ 
%&\stackrel{I_n = O(n)}{=}  O(1) \cdot n \cdot
%\int_{2(1-\eps)\ln k_n}^{2(1+\eps)\ln k_n} e^{y/2}\cdot e^{-\lambda_y} 
%\frac{\lambda_y^{k_n-1}}{(k_n-1)!} e^{-\alpha y}dy. 
%\end{split}
%\end{equation*}
%As above we perform a change of variable, setting $z = \lambda_y$ whereby 
%$dy=2 \lambda_y^{-1} dz$. 
%Therefore,  
%$$\int_{2(1-\eps)\ln k_n}^{2(1+\eps)\ln k_n} e^{y/2}\cdot e^{-\lambda_y} 
%\lambda_y^{k_n-1} e^{-\alpha y}dy= 
%\int_{k_n^{1-\eps}}^{k_n^{1+\eps}} e^z z^{k_n-1-2\alpha} dz \sim \Gamma(k_n -2\alpha).$$
%We thus conclude that 
%\begin{equation*}
%\begin{split}
%&\int_{-I_{n}}^{I_n} \int_{ 2(1-\eps ) \ln k_n}^{2(1+\eps ) \ln k_n} \Prob{D_{\H}((0,y))=k_n-1)} e^{y/2 -\alpha y} dydx = \\
%&=O(1) \cdot n \cdot \frac{\Gamma (k_n -2\alpha)}{\Gamma (k_n)} = O(1) \cdot n \cdot \frac{1}{k_n^{2\alpha}}.
%\end{split}
%\end{equation*}
%Now,~\eqref{eq:degree_expectation} yields 
%$\expH = O(1) \cdot n \cdot k_n^{-2\alpha -1}$. 
%Thus,
%\begin{equation*}
%\begin{split} 
%&I_3: =R_n {k_n \choose 2}^{-1} \cdot k_n \cdot 2 \expH^{-1} e^{(1/2 -\alpha)R_n} \cdot 
%k_n^{(1+\eps) (2\alpha-1)}\times \\
%& \hspace{2cm} 
%\int_{-I_n}^{I_n} \int_{ 2(1-\eps ) \ln k_n}^{2(1+\eps ) \ln k_n} \Prob{D_{\H}((0,y))=k_n-1)} e^{y/2 -\alpha y} dydx \\
%&= O(1) \cdot 
%R_n \cdot k_n^{-1} \cdot n^{-1} \cdot k_n^{2\alpha+1} \cdot 
%e^{(1/2-\alpha)R_n} \cdot k_n^{(1+\eps)(2\alpha -1)} \cdot n \cdot k_n^{-2\alpha}
% \\
%&=O(1) \cdot R_n \cdot e^{(1/2-\alpha)R_n} \cdot k_n^{(1+\eps)(2\alpha -1)}.
%\end{split}
%\end{equation*}
% 
%%Thereby, for $n$ sufficiently large
%%\begin{equation*}
%%\begin{split}
%%R_n {k_n \choose 2}^{-1} \cdot k_n \cdot 2 \expH^{-1} e^{(1/2 -\alpha)R_n} 
%%\int_{-I_{n}}^{I_n} \int_0^{R_n} e^{\alpha y -\alpha y} dydx \leq   6 R_n^2 e^{(1/2 -\alpha)R_n} k_n^{-1}. 
%%\end{split}
%%\end{equation*}
%For $1/2 < \alpha \leq 3/4$, we have 
%$$k_n^{4\alpha -2} I_3 =  O(1) \cdot 
%R_n \cdot  e^{(1/2 -\alpha)R_n}\cdot  k_n^{6\alpha-3 +\eps (2\alpha-1)} 
%=O(1)\cdot R_n \cdot n^{\frac{3(2\alpha-1)}{2\alpha+1} -(2\alpha -1) + \eps (2\alpha-1)}.$$
%But 
%$\frac{3}{2\alpha+1}-1 = \frac{3 -2\alpha -1}{2\alpha+1}$ 
% For $3/4 < \alpha \leq 3/2$, we have 
% $$k_n I_2 \leq  6 R_n^2 e^{(1/2 -\alpha)R_n} \to 0.$$
%%%%%%%%%%%%%%%%%%%%%%%%%% UP TO HERE %%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{The sum of~\eqref{eq:sum_6}}
Now, we will give an upper  bound on the term
\begin{equation*}
\Exp { \sum_{p_1,p_2 \in\Pcal \setminus \{(0,y)\}, \ y(p_1) < K}
\ind{p_1\in \BallSym{(0,y)}} \ind{p_2\in \BallHyp{(0,y)}\cap \BallPo{(0,y)}}}.
\end{equation*}
Using the Campbell-Mecke formula~\eqref{eq:Campbell-Mecke}, we write 
\begin{equation*}
\begin{split}
&\Exp { \sum_{p_1,p_2 \in\Pcal \setminus \{(0,y)\}, \ y(p_1) < K}
\ind{p_1\in \BallSym{(0,y)}} \ind{p_2\in \BallHyp{(0,y)}\cap \BallPo{(0,y)}}} =\\
&\leq \int_{-I_n}^{I_n} \int_0^K \int_{-I_n}^{I_n} \int_0^{R_n} 
\ind{(x_1,y_1) \in \BallSym{(0,y)}} 
 \ind{(x_2,y_2) \in \BallHyp{(0,y)}\cap \BallPo{(0,y)}} e^{-\alpha y_2} e^{-\alpha y_1} 
 dx_2 dy_2 dx_1 dy_1 \\
 &\leq  \mu_{\alpha, \nu} (\BallHyp{(0,y)})\cdot
 \int_{-I_n}^{I_n} \int_0^K \ind{(x_1,y_1) \in \BallSym{(0,y)}} 
e^{-\alpha y_1}  dx_1 dy_1.
\end{split}
\end{equation*}
By~\eqref{eq:HBall_measure},  the first factor is 
$$ \mu_{\alpha, \nu} (\BallHyp{(0,y)}) =O(1) \cdot e^{y/2}. $$ 
We bound the second factor using Lemma~\ref{lem:asymptotics_Omega_hyperbolic}. 
In particular,~\eqref{eq:asymp1} implies that 
if $(x_1,y_1) \in \BallSym{(0,y)}$, then 
$$ |x_1 - e^{(y+y_1)/2} |\leq e^{(y+y_1)/2} \cdot K e^{y+y_1- R_n} \stackrel{y_1<K}{=} O(1) 
e^{(y+y_1)/2} \cdot e^{y- R_n}.$$
Therefore, 
$$ \int_{-I_n}^{I_n} \int_0^K \ind{(x_1,y_1) \in \BallSym{(0,y)}} 
e^{-\alpha y_1}  dx_1 dy_1 = O(1) \cdot e^{y-R_n} 
\cdot \int_0^K e^{(y+y_1)/2} 
e^{-\alpha y_1}   dy_1 = O(1)\cdot e^{3y/2 - R_n}. 
$$
Therefore, 
\begin{equation*}
\begin{split}
\Exp { \sum_{p_1,p_2 \in\Pcal \setminus \{(0,y)\}, \ y(p_1) < K}
\ind{p_1\in \BallSym{(0,y)}} \ind{p_2\in \BallHyp{(0,y)}\cap \BallPo{(0,y)}}} =
O(1) \cdot e^{2y - R_n}.
\end{split}
\end{equation*}
Now, we integrate this over $y$: 
\begin{equation*}
\begin{split}
&e^{-R_n} \int_{-I_n}^{I_n} \int_{I_\eps (k_n)} e^{2y -\alpha y} dy dx = O(1) \cdot n \cdot 
e^{-R_n} \int_{I_\eps (k_n)} e^{2y -\alpha y} dy \\
&\stackrel{n =\nu e^{R_n/2}}{=} O(1) \cdot n^{-1} \int_{I_\eps (k_n)} e^{2y -\alpha y} dy \\
&= O(1) \cdot n^{-1} \cdot 
\begin{cases}
k_n^{2(2-\alpha) (1+\eps)}, & \mbox{if $\alpha < 2$} \\
\log k_n, & \mbox{if $\alpha =2$} \\
1, & \mbox{if $\alpha >2$}
\end{cases}.
\end{split}
\end{equation*}
We will multiply the above by $${k_n \choose 2}^{-1} (\expH^{-1} + \expP^{-1}) = 
O(1) \cdot n^{-1} \cdot k_n^{-2 +2\alpha +1} = O(1) \cdot n^{-1} \cdot k_n^{2\alpha -1}.$$

Assume first that $1/2 <\alpha \leq 3/4$. In this case, we will consider
$$ k_n^{4\alpha -2} \cdot n^{-2} \cdot k_n^{2\alpha -1 + 4 - 2\alpha} = 
n^{-2} \cdot k_n^{4\alpha +1}.$$
But, $\alpha \leq 3/4$, we have $4\alpha +1 \leq 4$ and $k_n = o(n^{1/2})$, whereby $k_n^{4\alpha +1} = o(n^{2})$. 

Now, suppose that $3/4 < \alpha < 2$. Here, we will consider 
$$ k_n \cdot n^{-2} \cdot k_n^{2\alpha -1 + 4 - 2\alpha} =\frac{k_n^2}{n^2} = o(1).$$ 
When $\alpha \geq 2$, we will bound $\log k_n$ and 1 by $k_n$ and we will consider 
$$ k_n \cdot n^{-2} \cdot k_n^{2\alpha -1 +1}= n^{-2} k_n^{2\alpha +1}.$$ 
But $k_n = O(1)\cdot n^{\frac{1}{2\alpha +1}}$, whereby $k_n^{2\alpha +1} =O(n)$ and 
the above term is therefore $o(1)$.

\end{proof}


%\subsubsection{old material}
%
%
%It suffices to prove the lemma for 
%$$\Exp{ | \Delta_{\H}(p) -  \Delta_{\Pcal} (p)| 
%\ind{k_n/2 \leq e^{y(p)/2} \leq 2k_n}\ind{D_\H (p) =k_n}}.$$
%Firstly, observe that the quantity inside the expectation 
%is invariant under the $x$-coordinate of $p$. 
%
%Therefore, using the Camplell-Mecke formula~\eqref{eq:Campbell-Mecke},
%we can write: 
%\begin{align*} %\label{eq:expectation-diff-1}
%&\Exp{\sum_{p\in \Pcal}|  \Delta_{\H}(p) -  \Delta_{\Pcal} (p)|\ind{k_n/2 \leq e^{y(p)/2} \leq 2k_n}\ind{D_{\H}((0,y))=k_n}} = \\
%& \frac{\nu \alpha}{\pi} 2I_n \int_{k_n/2}^{2k_n} \Exp{|\Delta_{\H}((0,y)) -  \Delta_{\Pcal} ((0,y))| \ind{D_{\H}((0,y))=k_n}; \Pcal \setminus \{(0,y)\}} e^{-\alpha y} dy.
%\end{align*}
%We will now bound from above the quantity:
%$$  \Exp{|\Delta_{\H}((0,y)) -  \Delta_{\Pcal} ((0,y))|\ind{D_{\H}((0,y))=k_n}; \Pcal \setminus \{(0,y)\}}.$$
%Let $\BallSym{p'} = \BallHyp{p'} \bigtriangleup \BallPo{p'}$ and let 
%$\hsym{p'}:= |\BallSym{p'}\cap \Pcal|.$
%The Camplell-Mecke formula again yields: 
%\begin{align} \label{eq:difference}
%& \Exp{|\Delta_{\H}((0,y)) -  \Delta_{\Pcal} ((0,y))|\ind{D_{\H}((0,y))=k_n}; \Pcal \setminus \{(0,y)\}}  \nonumber \\ 
%\leq &\Exp{\ind{D_{\H}((0,y))=k_n}\sum_{p'\in \Pcal \setminus \{(0,y)\}} \ind{p' \in \BallPo{(0,y)} \cup \BallHyp{(0,y)}} 
%\hsym{p'};\Pcal \setminus \{(0,y)\} } \nonumber \\ 
%\leq &\Exp{\ind{D_{\H}((0,y))=k_n}\sum_{p'\in \Pcal \setminus \{(0,y)\}} \ind{p' \in \BallPo{(0,y+\delta_K)}} 
%\hsym{p'};\Pcal \setminus \{(0,y)\} } \nonumber \\
% &\leq \int_{\Rcal_n} \int_{\Rcal_n} 
%|\Delta_{\H}((0,y), (x',y'), (x'',y'')) -  \Delta_{\Pcal} ((0,y), (x',y'),(x'',y''))| \times \nonumber  \\
%&\hspace{1.5cm}\Prob{D_{\H}((0,y)) =k_n-2;\Pcal \setminus \{(0,y),(x',y'),(x'',y'')\}}
%e^{-\alpha y' -\alpha y''} dx' dy' dx'' dy'' \nonumber \\
%&\leq \int_{\Rcal_n}\mu(B_{\H \bigtriangleup \Pcal} ((x',y'))) \cdot \Prob{D_{\H}((0,y)) =k_n-1;\Pcal \setminus \{(0,y),(x',y')\}} e^{-\alpha y'} dx' dy'.
%\end{align}
%
%We will now give an upper bound on $\mu (\BallSym{(x',y')})$. 
%Let $r' = R_n- y'$.
%Lemma~\ref{lem:asymptotics_Omega_hyperbolic} implies that for such a $p'$, if a point $p'' = (x'',y'')$ belongs to $\BallSym{p'} \cap \Rcal ([0,R-y'])$ then 
%\[
%	|x' - x''| = \Theta(1) \cdot e^{\frac{3}{2} (y' + y'') - R_n}.
%\]
%Now, if $y'' \in [r', r' + 2 \ln \frac{\pi}{2})]$ and also $p'' \in \BallHyp{p'} \bigtriangleup \BallPo{p'}$, then 
%\[
%	|x'-x''| =\frac{\pi}{2} e^{R_n/2} - e^{\frac{1}{2} (y' + y'')}.
%\]
%Finally,~\eqref{eq:symm_diff_upper_P} implies tha no point in $\Rcal ([r'+2\ln \frac{\pi}{2},R_n])$ belongs to $\BallHyp{p'} \bigtriangleup \BallPo{p'}$, as this region is in fact part of $\BallHyp{p'} \cap \BallPo{p'}$. We first compute the expected number of points $p'' \in \BallHyp{p'} \bigtriangleup \BallPo{p'}$ that have $R_n - y'' \le r'$. The result depends on the value of $\alpha$, yielding the following three cases
%\begin{align*}
%	\mu_{\alpha,\nu} (\BallSym{p'} \cap \Rcal ([0,r'])) 
%	&= \Theta(1) \cdot e^{3y'/2 - R_n}\int_0^{r'} e^{(3/2-\alpha) y''} \, dy'' \\
%	&= \Theta(1)\cdot \begin{cases} e^{(1/2-\alpha)R_n + \alpha y'}, & \mbox{if } \alpha < 3/2 \\
%		r' e^{3y'/2 - R_n}, & \mbox{if }\alpha = 3/2\\
%		e^{3y'/2 - R_n}, &  \mbox{if } \alpha > 3/2
%	\end{cases}.
%\end{align*}
%Also, 
%\begin{align*}
%\mu_{\alpha,\nu}& (\BallSym{p'} \cap \Rcal ([r',r' +2\ln (\pi/2)])) \leq
%\pi e^{R_n/2} \int_{r'}^{r' + 2 \ln (\pi/2)} e^{-\alpha y'} dy'\\
%& = O(1)\cdot 
%e^{R_n/2 - \alpha (R_n-y')} = O(1) \cdot e^{(1/2 -\alpha) R_n + \alpha y'}. 
%\end{align*}
%Let us consider first the case where $1/2 < \alpha \leq 3/4$. 
%The above estimates imply that in this case 
%\begin{equation} \label{eq:symm_diff-measure} 
%\mu_{\alpha,\nu} ( \BallSym{p'} ) = O(1) \cdot e^{(1/2 -\alpha) R_n + \alpha y'}.
%\end{equation}
%Note that we require that $p'=(x',y') \in \BallPo{(0,y)}$.  Lemma~\ref{lem:asymptotics_Omega_hyperbolic} implies that 
%if $y' > R-y + 2 \ln (\pi/2 )$, then $p' \in \BallPo{(0,y)}$; otherwise, 
%$p' \in \BallPo{(0,y)}$ if and only if $|x| < e^{y/2 + y'/2}$. 
%We can now substitute the estimate on $\mu_{\alpha, \nu} (\BallSym{p'})$ we obtained above into~\eqref{eq:difference}. This gives
%\begin{align*} 
%&\Exp{|\Delta_{\H}((0,y)) -  \Delta_{\Pcal} ((0,y))|\ind{D_{\H}((0,y))=k_n}; \Pcal \setminus \{(0,y)\} } = O(1) \times \\
%&\Prob{D_{\H}((0,y)) =k_n-1;\Pcal \setminus \{(0,y),(x',y')\}} \times \\
%&\left( e^{(1/2-\alpha)R_n + y/2}
%\int_0^{R_n-y+2 \ln (\pi/2)} e^{y'/2 + \alpha y'} e^{-\alpha y'} dy' 
%+ e^{(1/2 -\alpha)R_n} \int_{R_n-y+ 2 \ln (\pi/2)}^{R_n} e^{\alpha y' - \alpha y'}dy'\right).
%\end{align*}
%
%
%We first give an expression for $\Prob{D_{\H}((0,y)) =k_n-2;\Pcal \setminus \{(0,y),(x',y'),(x'',y'')\}}$. Recall that we have assumed that $y$ is such that 
%$k_n/2 < e^{y/2} < 2k_n$. This implies that 
%$$\Exp{|\BallHyp{(0,y)} \cap \Pcal \setminus \{(0,y),(x',y'),(x'',y'')\}|} = \Theta (k_n). 
%$$ 
%Thereby, we conclude that uniformly over all choices of $(x',y')$ and $(x'',y'')$ we 
%have 
%\begin{equation} \label{eq:deg_p_minus} 
%\Prob{D_{\H}((0,y)) =k_n-2;\Pcal \setminus \{(0,y),(x',y'),(x'',y'')\}} = \Theta \left(k_n^{-1/2}\right). 
%\end{equation}
%
%Now, the difference 
%$|\Delta_{\H}((0,y), (x',y'), (x'',y'')) -  \Delta_{\Pcal} ((0,y), (x',y'),(x'',y''))|$ 
%can only take the values 0 or 1. In particular, if 
%it is equal to 1, then there are three possibilities 
%\begin{enumerate}
%\item either $(x',y') \in \BallHyp{(0,y)}\bigtriangleup \BallPo{(0,y)}$,
%\item or $(x'',y'') \in \BallHyp{(0,y)}\bigtriangleup \BallPo{(0,y)}$,
%\item or $(x'',y'') \in \BallHyp{(x',y')} \bigtriangleup \BallPo{(x',y')}$ for $(x',y') \in 
%\BallHyp{(0,y)} \cup \BallPo{(0,y)}$.
%\end{enumerate}
%By Lemma~\ref...  
%if $y' \geq R - y$, then $(x',y') \in \BallHyp{(0,y)} \cup \BallPo{(0,y)}$ for any 
%$x' \in [-I_n,I_n]$. 
%Also, for $y'< R-y$, we have that if $(x',y') \in \BallHyp{(0,y)} \cup \BallPo{(0,y)}$, 
%then $|x'| < K e^{y/2 + y'/2}$. 
%
%Therefore, the integral in~\eqref{eq:difference} can be bounded as follows: 
%\begin{align*}
% &\Exp{|\Delta_{\H}((0,y)) -  \Delta_{\Pcal} ((0,y))|; \Pcal \setminus \{(0,y)\}}  
%\leq 2 k_n^{-1/2} \mu_{\alpha,\nu} (\BallHyp{(0,y)} \bigtriangleup \BallPo{(0,y)}) \\
%& \hspace{1cm} + k_n^{-1/2} e^{y/2}
%\int_{0}^{R_n-y} e^{y'/2} \mu_{\alpha,\nu} (\BallHyp{(x',y')} \bigtriangleup \BallPo{(x',y')}) e^{-\alpha y'} dy' dx' \\
%& \hspace{1cm} + k_n^{-1/2} I_n
%\int_{R_n-y}^{R_n}  \mu_{\alpha,\nu} (\BallHyp{(x',y')} \bigtriangleup \BallPo{(x',y')}) e^{-\alpha y'} dy' dx
%\end{align*}
%But $\mu_{\alpha,\nu} (\BallHyp{(x',y')} \bigtriangleup \BallPo{(x',y')})$ does not depend on $x'$, whereby we eventually obtain:
%\begin{align*}
% &\Exp{|\Delta_{\H}((0,y)) -  \Delta_{\Pcal} ((0,y))|; \Pcal \setminus \{(0,y)\}}  
%\leq 2 k_n^{-1/2} \mu_{\alpha,\nu} (\BallHyp{(0,y)} \bigtriangleup \BallPo{(0,y)}) \\
%& \hspace{1cm} + k_n^{-1/2} e^{y/2}
%\int_{0}^{R_n-y} e^{y'/2} \mu_{\alpha,\nu} (\BallHyp{(0,y')} \bigtriangleup \BallPo{(0,y')}) e^{-\alpha y'} dy' dx' \\
%& \hspace{1cm} + k_n^{-1/2} I_n
%\int_{R_n-y}^{R_n}  \mu_{\alpha,\nu} (\BallHyp{(0,y')} \bigtriangleup \BallPo{(0,y')}) e^{-\alpha y'} dy' dx
%\end{align*}
%To bound these quantities, we need an estimate on $\mu_{\alpha,\nu} (\BallHyp{(0,y)} \bigtriangleup \BallPo{(0,y)})$. 
%This is given in the following claim. 
%\begin{lemma} 
%For any $y' \in (0,R_n - y)$ we have
%\[
%	\mu_{\alpha,\nu} (\BallHyp{(0,y')} \bigtriangleup \BallPo{(0,y')} ) 
%	= \Theta (1) \cdot \begin{cases} 
%		e^{(1/2-\alpha)R_n + \alpha y'}, & \mbox{if } \alpha < 3/2 \\
%		(R_n-y) e^{3y'/2 - R_n}, & \mbox{if }\alpha = 3/2\\
%		e^{3y'/2 - R_n}, &  \mbox{if } \alpha > 3/2 
%	\end{cases}.
%\]
%\end{lemma}
%
%
%
%
%Assume first that $\alpha \leq 3/2$. 
%Then
%\begin{align*} 
%&k_n^{-1/2} e^{y/2}\int_{0}^{R_n-y}e^{y'/2} \mu_{\alpha,\nu} (\BallHyp{(0,y')} \bigtriangleup \BallPo{(0,y')}) e^{-\alpha y'} dy' = O (R_n) k_n^{-1/2} e^{(1-\alpha)R_n}.
%\end{align*}
%Let us consider first the sub-case where $1/2 < \alpha <3/4$.
%By~\eqref{eq:exp_deg} we have that 
%$$\frac{1}{\Exp{N_\H(k_n)}} \frac{k_n^{4\alpha -2}}{{k_n \choose 2}} = \Theta (1) \cdot e^{-R_n/2} k_n^{4(\alpha - 1) + 2\alpha +1} = \Theta (1) \cdot 
%e^{-R_n/2} k_n^{6\alpha - 3} $$
%The product of the above two expressions yields: 
%\begin{align*} 
%&\frac{1}{\Exp{N_\H(k_n)}} \frac{k_n^{4\alpha -2}}{{k_n \choose 2}} 
%k_n^{-1/2} e^{y/2}\int_{0}^{R_n-y}e^{y'/2} \mu_{\alpha,\nu} (\BallHyp{(0,y')} \bigtriangleup \BallPo{(0,y')}) e^{-\alpha y'} dy' =\\
%& O(R_n) \cdot k_n^{6\alpha - 3.5} e^{(1/2 -\alpha )R_n}. 
%\end{align*}
%But recall that $k_n = O(n^{\frac{1}{2\alpha +1}})$ and moreover we can write 
%$e^{(1/2 -\alpha )R_n} = \Theta (1) \cdot n^{1 - 2\alpha}$. 
%Therefore, 
%$$  k_n^{6\alpha - 3.5} e^{(1/2 -\alpha )R_n} = O(1) \cdot 
%n^{\frac{6 \alpha -3.5}{2\alpha +1}+ 1  - 2\alpha}.
%$$ 
%But $$\frac{6 \alpha -3.5}{2\alpha +1}+ 1  - 2\alpha = \frac{6 \alpha -3.5 - 4\alpha^2 +1}{2\alpha+1}=\frac{-4\alpha^2 + 6\alpha -2.5}{2\alpha +1}.$$ 
%Elementary analysis shows that the polynomial $-4\alpha^2 + 6\alpha -2.5$ 
%has no real roots and since the leading coefficient is negative, it is negative for any $\alpha$.  
%Therefore, 
%\begin{align*} 
%&\frac{1}{\Exp{N_\H(k_n)}} \frac{k_n^{4\alpha -2}}{{k_n \choose 2}} 
%k_n^{-1/2} e^{y/2}\int_{0}^{R_n-y}e^{y'/2} \mu_{\alpha,\nu} (\BallHyp{(0,y')} \bigtriangleup \BallPo{(0,y')}) e^{-\alpha y'} dy' =o(1). 
%\end{align*}
%Assume now that $3/4 \leq \alpha \leq 3/2$. 
%By~\eqref{eq:exp_deg} we have that 
%$$\frac{1}{\Exp{N_\H(k_n)}} \frac{k_n}{{k_n \choose 2}} = \Theta (1) \cdot e^{-R_n/2} k_n^{-1+ 2\alpha +1} = \Theta (1) \cdot 
%e^{-R_n/2} k_n^{2\alpha}.$$
%Hence, 
%\begin{align*} 
%&\frac{1}{\Exp{N_\H(k_n)}} \frac{k_n}{{k_n \choose 2}} 
%k_n^{-1/2} e^{y/2}\int_{0}^{R_n-y}e^{y'/2} \mu_{\alpha,\nu} (\BallHyp{(0,y')} \bigtriangleup \BallPo{(0,y')}) e^{-\alpha y'} dy' =\\
%& O(R_n) \cdot k_n^{2\alpha - 1/2} e^{(1/2 -\alpha )R_n} = O(R_n) \cdot 
%k_n^{2\alpha -1/2} n^{1 - 2\alpha}.  
%\end{align*}
%As $k_n = O(n^{\frac{1}{2\alpha +1}})$, we have 
%$$ k_n^{2\alpha -1/2} n^{1 - 2\alpha} = O(1) \cdot n^{\frac{2\alpha - 1/2}{2\alpha +1} + 1 - 2\alpha}.$$ 
%But 
%$$ \frac{2\alpha - 1/2}{2\alpha +1} + 1 - 2\alpha =\frac{-4 \alpha^2 + 2\alpha +1/2}{2\alpha +1}.$$ 
%Elementary analysis shows that the largest root of $-4 \alpha^2 + 2\alpha +1/2$ 
%is smaller than $3/4$. As the leading coefficient is negative, it follows that this 
%quadratic polynomial is negative for $3/4 \leq \alpha  \leq 3/2$.  
%
%Suppose now that $\alpha > 3/2$. In this case, 
%\begin{align*} 
%&\frac{1}{\Exp{N_\H(k_n)}} \frac{k_n}{{k_n \choose 2}} 
%k_n^{-1/2} e^{y/2}\int_{0}^{R_n-y}e^{y'/2} \mu_{\alpha,\nu} (\BallHyp{(0,y')} \bigtriangleup \BallPo{(0,y')}) e^{-\alpha y'} dy' =\\
%& O(R_n) \cdot k_n^{2\alpha + 1/2} e^{y/2} e^{-3R_n /2} \stackrel{e^{y/2} \leq 2 k_n}{=} O(R_n) \cdot 
%k_n^{2\alpha +1/2 +1} n^{-3} = 
%O(R_n) n^{\frac{2\alpha +3/2}{2\alpha +1} - 3} = o(1).   
%\end{align*}
%

%\subsection{Missing number of neighbors}
%
%Lemma \ref{lem:asymptotics_Omega_hyperbolic} will allow us to find for some vertex $u \in G_{\H,n}$ its corresponding degree in $G_{\Pcal}$, i.e. $D_{\mathcal{P}}(\Psi(u))$. Note that this equals $N_{\Pcal}\left(\BallHyp{p}\right)$ and recall that this is distributed as $\Po({\mu_{\alpha, \nu} (\BallHyp{p})})$.
%
%\begin{lemma} \label{lem:edges_in_fat_hyp_ball}
%Let $k_n$ be an increasing sequence of integers such that $k_n = O(n^{\frac{1}{2\alpha +1}})$. 
%Let $p_n=(0,y_n) \in \Rcal_n$. 
%
%such that $y_n \to \infty$ and $R_n - y_n \to \infty$ 
%as $n\to \infty$. Then with probability $1 - o()$, 
%\[
%\mathbb{P} 
%\]
%\end{lemma}
%\begin{proof} 
%Lemma~\ref{lem:asymptotics_Omega_hyperbolic} implies that, for such $p_n$, if a point $p' = (x',y')$ belongs to 
%$\BallHyp{p} \cap \Rcal ([0,R_n-y])$ if and only if 
%\[
%	|x_n-x^\prime| \leq  e^{\frac{1}{2} (y_n + y')}.
%\]
%Now, since $R_n - y_n \to \infty$,
%\[
%	\mu_{\alpha,\nu} (\BallHyp{p} \cap \Rcal ([0,R_n - y_n])) 
%	= \Theta(1) \cdot e^{y_n/2}\int_0^{R_n - y_n} e^{(1/2-\alpha) y} dy = \Theta (e^{y_n/2}).
%\] 
%Similarly, we compute: 
%\begin{align*}
%	\mu_{\alpha, \nu} (\Rcal (R_n - y_n,R_n)) 
%	&= \pi e^{R_n/2}\frac{\nu \alpha}{\pi} \int_{R_n - y_n}^{R_n} e^{-\alpha y} dy \\
%	&= \nu e^{R_n/2} (e^{-\alpha(R_n- y_n)}- e^{-\alpha R_n}) = \Theta (e^{\frac{R_n(1-2\alpha)}{2} + \alpha y_n}).
%\end{align*}
%Recall that we have $\Rcal([R_n - y_n,R_n])\subset \BallHyp{p}$. Therefore, by adding the above two expressions we deduce that 
%\[
%	\mu_{\alpha,\nu} (\BallHyp{p} ) = \Theta (1) \cdot (e^{y_n/2} + e^{\frac{R(1-2\alpha)}{2} + \alpha y_n}).
%\]
%But 
%$\frac{y_n}{2} >  \frac{R_n(1-2\alpha)}{2} + \alpha y_n$ as $y_n < R_n$ and $\alpha > 1/2$. 
%Hence, 
%\[
%	\mu_{\alpha,\nu} (\BallHyp{p} ) = \Theta (1) \cdot e^{y_n/2}.
%\]
%The lemma now follows as the number of points inside $\BallHyp{p}$ is 
%Poisson-distributed with parameter equal to $ \mu_{\alpha,\nu} (\BallHyp{p} )$, 
%and $y_n \to \infty$. 
%\end{proof}
%
%One other useful estimate is about the number of points that fall inside $\BallHyp{p} \bigtriangleup \BallPo{p}$, where $A \bigtriangleup B$ denotes the symmetric difference between $A$ and $B$. This will give us an estimate on the error introduced by the coupling. 
%
%
%
%Let $N_{\H \bigtriangleup \Pcal} (p)$ be the number of points inside $\BallHyp{p} \bigtriangleup \BallPo{p}$. Then we have the following result.
%
%\begin{lemma}\label{lem:symm_diff_balls_H_P}
%Let $p_n = (x_n,y_n) \in \Rcal_n$ be such that $y_n \to \infty$ and $R_n - y_n \to \infty$ 
%as $n\to \infty$. Then 
%\[
%	N_{\mathbb{H} \bigtriangleup \Gamma} (p)= o_p (N_{\Pcal}(\BallHyp{p})).
%\]
%\end{lemma}
%
%\begin{proof}
%Let $r_n := R_n - y_n$. Lemma~\ref{lem:asymptotics_Omega_hyperbolic} implies that for such a $p_n$, if a point $p^\prime$ belongs to $\BallHyp{p_n} \bigtriangleup \BallPo{p_n} \cap \Rcal ([0,r_n])$ then 
%\[
%	|x_n - x^\prime| = \Theta(1) \cdot e^{\frac{3}{2} (y_n + y^\prime) - R_n}.
%\]
%Now, if $p^\prime \in [r_n, r_n + 2 \ln \frac{\pi}{2})]$ and also $p^\prime \in \BallHyp{p_n} \bigtriangleup \BallPo{p_n}$, then 
%\[
%	|x_n-x^\prime| =\frac{\pi}{2} e^{R_n/2} - e^{\frac{1}{2} (y_n + y^\prime)}.
%\]
%Finally,~\eqref{eq:symm_diff_upper_P} implies that no point in $\Rcal ([r_n+2\ln \frac{\pi}{2},R_n])$ belongs to $\BallHyp{p_n} \bigtriangleup \BallPo{p_n}$. We first compute the expected number of points in $p^\prime \in \BallHyp{p_n} \bigtriangleup \BallPo{p_n}$ that have $R_n - y^\prime \le r_n$. The result depends on the value of $\alpha$, yielding the following three cases
%\begin{align*}
%	\mu_{\alpha,\nu} (\BallHyp{p_n} \bigtriangleup \BallPo{p_n} \cap \Rcal ([0,r_n])) 
%	&= \Theta(1) \cdot e^{3y_n/2 - R_n}\int_0^{r_n} e^{(3/2-\alpha) y} \, dy \\
%	&= \Theta(1)\cdot \begin{cases} e^{(1/2-\alpha)R_n + \alpha y_n}, & \mbox{if } \alpha < 3/2 \\
%		r_n e^{3y_n/2 - R_n}, & \mbox{if }\alpha = 3/2\\
%		e^{3y_n/2 - R_n}, &  \mbox{if } \alpha > 3/2
%	\end{cases}.
%\end{align*}
%Next we compute the number of remaining points in $\BallHyp{p_n} \bigtriangleup \BallPo{p_n}$, 
%\begin{align*}
%	\mu_{\alpha,\nu} (\BallHyp{p_n} \bigtriangleup \BallPo{p_n} \cap \Rcal ([r_n, R_n])) 
%	&= \frac{\nu\alpha}{\pi} \int_{r_n}^{r_n + 2 \ln \frac{\pi}{2}} 
%		\left(\frac{\pi}{2} e^{R_n/2} - e^{\frac{1}{2} (y_n + y)}\right)e^{-\alpha y} \, dy \\
%	&= O(1) \cdot e^{R_n/2} \int_{r_n}^{r_n + 2 \ln \frac{\pi}{2}} e^{-\alpha y} \, dy 
%		= O(1) \cdot e^{R_n/2} e^{-\alpha r_n} \\
%	&=O(1) \cdot e^{(1/2 - \alpha) R_n + \alpha y_n}.
%\end{align*}
%Now note that for any $\alpha > 3/2$, we have 
%\[
%	\left( (1/2 - \alpha) R_n + \alpha y_n\right) - \left(3y_n/2 - R_n \right)\to -\infty,
%\]
%since
%\begin{align*}
%	\left( (1/2 - \alpha) R_n + \alpha y_n\right) - \left(3y_n/2 - R_n \right) 
%	&= (3/2-\alpha )R_n - (3/2 - \alpha) y_n 
%	&= (3/2 -\alpha) (R_n- y_n) \to -\infty.
%\end{align*}
%For $\alpha = 3/2$, these two quantities are equal. From these observations, we deduce that 
%\[
%	\mu_{\alpha,\nu} (\BallHyp{p_n} \bigtriangleup \BallPo{p_n} ) 
%	= \Theta (1) \cdot \begin{cases} 
%		e^{(1/2-\alpha)R_n + \alpha y_n}, & \mbox{if } \alpha < 3/2 \\
%		r_n e^{3y_n/2 - R_n}, & \mbox{if }\alpha = 3/2\\
%		e^{3y_n/2 - R_n}, &  \mbox{if } \alpha > 3/2 
%	\end{cases}.
%\]
%Recall that the number of points inside $\BallHyp{p_n}\bigtriangleup \BallPo{p_n}$ is 
%Poisson-distributed with parameter equal to 
%$ \mu_{\alpha,\nu} (\BallHyp{p_n} \bigtriangleup \BallPo{p_n})$. 
%Lemma \ref{lem:edges_in_fat_hyp_ball} implies that the number of points inside $\BallHyp{p_n}$ is Poisson-distributed with parameter which is $\Theta (1) \cdot e^{y_n/2}$. 
%But note that for any $\alpha > 1/2$ we have 
%\[
%	\frac{y_n}{2} - \left( (1/2-\alpha)R_n + \alpha y_n \right) = (1/2 - \alpha)( y_n - R_n) = (\alpha -1/2) (R_n- y_n) \to \infty,
%\]
%and also 
%\[
%	\frac{y_n}{2} - \left(\frac{3y_n}{2} - R_n\right) = R_n - y_n \to \infty.
%\] 
%Therefore, since $y_n\to \infty$, we have 
%\[
%	N_{\mathbb{H} \bigtriangleup \Gamma} (p)= o_p (N_{\Pcal} (\BallHyp{p})).
%\]
%\end{proof}

%\subsection{Missing edges between neighbors}
%
%Note that it follows from Lemma \ref{lem:symm_diff_balls_H_P} that
%\[
%	\frac{D_{\Pcal}(p)}{D_{\H}(p)} \plim 1,
%\]
%so that the number of neighbors of a point $p \in G_{\Pcal,n}(\alpha,\nu)$ closely approximates the number of neighbors of $\Psi^{-1}(p) \in G_{\H,n}(\alpha,\nu)$. The next step is to compute the difference between the number of edges between notes in $\BallPo{p}$ and $\BallHyp{p}$. For this let $E_{\H}(p)$ denote the number of edges in $\BallHyp{p}$ and $E_{\mathcal{P}}(p)$ the number of edges in $\BallPo{p}$. We decompose $E_{\H,n}(p)$ into the number of edges where both endpoints belong to $\BallHyp{p}\cap \BallPo{p}$ together with the number of edges where one endpoint belongs to $\BallHyp{p}\setminus \BallPo{p}$ and the other to $\BallHyp{p}$. Denoting these quantities by $E_{\H \cap \mathcal{P}}$ and $E_{\H \setminus \mathcal{P}}$ we have 
%\[
%	E_{\H} (p) = E_{\H\cap \mathcal{P}} + E_{\H \setminus \mathcal{P}}.
%\] 
%Following notation of analogous meaning we can also write: 
%\[
%	E_{\mathcal{P}} (p) = E_{\H \cap \mathcal{P}} + E_{\mathcal{P} \setminus \H}
%\]
%
%With these notation we have
%\begin{align*}
%	\left| \frac{E_{\H} (p)}{{N_{\H} (p) \choose 2}} -\frac{E_\Pcal (p)}{{N_\Pcal (p) \choose 2}} \right| &= \left| \frac{E_{\Pcal \cap \H}(p) + E_{\H\setminus \Pcal}(p)}{{N_{\H} (p) \choose 2}} 
%		- \frac{E_{\Pcal \cap \H} (p) + E_{\Pcal \setminus \H}}{{N_\Pcal (p) \choose 2}} \right| \\
%	&\leq \frac{E_{\H\setminus \Pcal}(p)}{{N_{\H} (p) \choose 2}} 
%		+ \frac{E_{\Pcal \setminus \H}(p)}{{N_{\Pcal} (p) \choose 2}} 
%		+ E_{\Pcal \cap \H}(p) \left| \frac{1}{{N_{\H} (p) \choose 2}} 
%		- \frac{1}{{\binom{N_{\Pcal}(p)}{2}}}\right| \\
%	 &= \frac{E_{\H\setminus \Pcal}(p)}{{N_{\H} (p) \choose 2}} 
%	 	+ \frac{E_{\Pcal \setminus \H}(p)}{{N_{\Pcal} (p) \choose 2}} 
%	 	+ \frac{E_{\Pcal \cap \H}(p)}{\binom{N_{\Pcal}(p)}{2}} 
%	 	\left| \frac{\binom{N_{\Pcal}(p)}{2}}{{N_{\H} (p) \choose 2}} - 1\right| 
%\end{align*}
%
%The next lemma shows that the the first two terms go to zero at appropriate rates.
%
%\PvdH{For the proof technique I had in mind we only need the asymptotic expression for $\Exp{E_{\H\setminus \mathcal{P}}(p_n)}$ in the case where $R_n - y_n \to \infty$. I have therefore extracted this result from the the lemma by Nikolaos.}
%
%\begin{lemma}\label{lem:missing_edges_ball_H_P}
%Let $p_n = (0, y_n)$ be a point in $\mathbb{R}_n$, such that $R_n - y_n \to \infty$ as $n \to \infty$. Then we have
%\[
%	\Exp{E_{\H\setminus \mathcal{P}}(p_n)} = \bigO{e^{(4\alpha - 2)y_n - R_n} + e^{y_n - \left(\alpha - \frac{1}{2}\right)R_n}
%    + e^{(2\alpha - 1)y_n - (2\alpha - 1)R_n}},
%\]
%and the same holds if we interchange $\H$ and $\mathcal{P}$.
%\end{lemma}
%
%\begin{proof}
%To prove this lemma we will use again Lemma~\ref{lem:asymptotics_Omega_hyperbolic} to approximate $\BallHyp{p}$. Note that if $y < y' - R$, then 
%\[
%	K e^{\frac{3}{2} (y+y') - R} < K e^{\frac{1}{2} (y+y')}.
%\]
%Using this fact, Lemma~\ref{lem:asymptotics_Omega_hyperbolic} motivates us to define a ball around $p$ that contains $\BallHyp{p}$.
%For $K > 0$, we define, for any point $p \in \R \times \R_+$,
%\begin{equation}\label{eq:def_fatball}
%	\FatBallHyp{p} : = \{ p^\prime : y^\prime < R_n - y, \ |x - x^\prime| < (1+K) e^{\frac{1}{2} (y + y^\prime)}  \}.
%\end{equation}
%%Note that $\Delta (r(p),r') = \frac12 e^{R/2} \theta_R (p)$.
%Observe that,
%\begin{equation*} %\label{eq:ball_inclusion} 
%\BallHyp{p} \cap \Rcal ([0,r(p))) \subseteq \FatBallHyp{p}
%\end{equation*}
%and 
%\begin{equation*} %\label{eq:ball_inclusion_lower}
%\BallHyp{p} \cap \Rcal ([r(p),R]) = \Rcal ([r(p),R]).
%\end{equation*}
%We thus conclude that 
%\begin{equation} \label{eq:hyperbolic_ball_inclusion}
%\BallHyp{p} \subseteq \FatBallHyp{p} \cup \Rcal([r(p),R]).
%\end{equation}
%
%We will consider the points of $\Pcal_{\alpha, \nu}$ that are contained in $\BallHyp{p_n}$ but not in $\BallPo{p_n}$ and in particular the number of ordered pairs of points $(p^\prime,p^{\prime \prime})$ with $p^\prime \in \BallHyp{p} \setminus \BallPo{p}$ and $p^{\prime \prime} \in \BallHyp{p^\prime}$. 
%
%To this end, we shall make use of the~\emph{Campbell-Mecke} formula:
%for a Poisson point process $\mathcal{P}$ on a measurable space $S$ with intensity $\rho$ and a measurable non-negative function $h: S^r \rightarrow \mathbb{R}$ we have
%\begin{equation} \label{eq:Campbell-Mecke}
%\begin{split}
%& \Exp{\sum_{x_1,\ldots, x_r \in \mathcal{P}}^{\neq} h (x_1, \ldots, x_r,
%\mathcal{P} \setminus \{x_1,\ldots, x_r\}) }\\
%& = \int_S \cdots \int_S
%\Exp{h(x_1,\ldots, x_r,\mathcal{P} \setminus \{x_1,\ldots, x_r\}) \rho (x_1) \cdots \rho (x_r) dx_1 \cdots dx_r},
%\end{split}
%\end{equation}
%where the sum ranges over all pairwise distinct $r$-tuples of points of $\mathcal{P}$.
%
%To obtain an upper bound on the expectation of $|\EdgeDiff|$ we shall make use of~\eqref{eq:hyperbolic_ball_inclusion}. More specifically, we will apply~\eqref{eq:Campbell-Mecke} to the Poisson point process $\Pcal_{\alpha,\nu}$ with 
%\[
%	h(p^\prime, \Pcal_{\alpha,\nu}) = \ind{p^\prime \in \Psi (\BallHyp{p_n})\setminus \BallPo{p_n}} \cdot
%    |\Pcal_{\alpha,\mu} \cap \left( \FatBallHyp{p^\prime} \cap \FatBallHyp{p_n} \right)|.
%\]
%
%To calculate the expectation of the above function we need to approximate the 
%intersection of the two balls $\FatBallHyp{p_n}$ and $\FatBallHyp{p^\prime}$, where $p^\prime= (x^\prime,y^\prime)$. 
%Let us suppose without loss of generality that $x^\prime > 0$. 
%The right boundary of $\FatBallHyp{p_n}$ is given by the equation 
%$x^\prime = (1+K)e^{\frac{1}{2} (y_n + y^\prime)}$ whereas the left boundary of $\FatBallHyp{p^\prime}$ is given by the curve having equation $x^\prime = x - (1+ K)e^{\frac{1}{2} (y_n + y^\prime)}.$ 
%
%The equation that determines the intersecting point of the two curves is
%\[
%	x - (1+K)e^{(y_n + \hat{y})/2}= (1+K) e^{(y_n + \hat{y})/2},
%\]
%where $\hat{y}$ is the $y$-coordinate of the intersecting point. 
%We can solve the above for $\hat{y}$  
%\begin{equation*} 
%\begin{split}
%x &=(1+K) e^{\hat{y}/2} \left( e^{y_n/2} + e^{y_n/2} \right).
%\end{split}
%\end{equation*}
%But since $(x,y) \not \in \BallPo{p_n}$, we also have $x > e^{\frac{y_n + y}{2}}$. Therefore, 
%\begin{equation*}
%\begin{split}
% e^{\hat{y}/2}& > \frac{1}{1+K}~\frac{e^{\frac{y_n + y}{2}}}{ e^{y/2}+ e^{y_n/2}} \geq 
%\frac{1}{2(1+K)}~\frac{e^{\frac{y_n + y}{2}}}{ e^{\max \{y, y_n\} /2}} 
%> \frac{1}{2(1 + K)} ~ e^{\min\{y, y_n\}/2}. 
% \end{split}
%\end{equation*}
%The above yields
%\begin{equation} \label{eq:to_use_I}
%\hat{y} > \min\{y, y_n\} - 2\log(2(1+K)) := c(y_n, y). 
%\end{equation}
%which yields the following 
%\begin{equation}\label{eq:intersex_approx}
%	p^\prime \in \FatBallHyp{p_n}\cap \FatBallHyp{(x,y)} \Rightarrow y^\prime \ge c(y_n,y).
%\end{equation}
%Therefore we conclude that 
%\[ 
%	\BallHyp{(x,y)} \cap \BallHyp{p_n} \subseteq \FatBallHyp{p_n} \cap \Rcal([c(y_n,y), R_n]) 
%	\bigcup \Rcal ([R_n - y_n,R_n]).
%\]
%This in turn implies that
%\[
%	|\Pcal_{\alpha,\nu} \cap \left( \FatBallHyp{p^\prime} \cap \BallHyp{p_n} \right)| \leq 
%	|\Pcal_{\alpha,\nu} \cap \left( \FatBallHyp{p_n} \cap  \Rcal([c(y_n,y), R_n]) \right)| + 
%	|\Pcal_{\alpha,\nu} \cap \Rcal ([R_n - y_n, R_n]) |
%\]
%and therefore
%\begin{align} 
%	h(p^\prime, \Pcal_{\alpha,\nu}) &\leq \ind{p^\prime \in \Psi (\BallHyp{p_n})\setminus \BallPo{p_n}} 
%    	\left|\Pcal_{\alpha,\nu} \cap \left( \FatBallHyp{p_n} \cap  \Rcal([c(y_n,y), R_n])\right) \right| 
%        \label{eq:h_upper_bound_1}\\
%	&\hspace{10pt}+ \ind{p^\prime \in \Psi (\BallHyp{p_n})\setminus \BallPo{p_n}}
%    	\left|\Pcal_{\alpha,\nu} \cap \Rcal ([R_n - y_n, R_n]) \right|.\label{eq:h_upper_bound_2}
%\end{align}
%
%Hence,~\eqref{eq:Campbell-Mecke} gives
%\begin{align*}
%	\Exp{ |\EdgeDiff|} &\leq \Exp{\left( \sum_{p^\prime \in \Pcal_{\alpha, \nu}} 
%		h(p^\prime, \Pcal_{\alpha, \nu}\setminus \{p^\prime\})\right)} \\
%	&=\frac{\nu \alpha}{\pi} \int_{\Rcal_n} \Exp{h((x,y), \Pcal_{\alpha, \nu} \setminus \{(x,y)\})}
%		e^{-\alpha y} \, dx \, dy.
%\end{align*}
%Recall that $(\BallHyp{p_n}\setminus \BallPo{p_n} )\cap \Rcal([R_n - y_n + 2 \log\left(\frac{\pi}{2}\right),R_n]) = \emptyset$. We will calculate the measure of each one of the two summands. The first one is:
%\begin{align*}
%	\mu_{\alpha,\nu}\left( \FatBallHyp{p_n} \cap  \Rcal([c(y_n,y), R_n])\right) 
%	&\leq (1+ K) \frac{\nu \alpha}{\pi} \cdot e^{y_n/2}  \int_{c(y_n,y)}^{R_n-y_n} e^{-(\alpha - \frac{1}{2}) y^\prime} \, dy^\prime \\
%	&=  \bigO{e^{\frac{y_n}{2} - (\alpha-\frac{1}{2}) \min \{y,y_n\}}}.
%\end{align*}
%
%The second summand is: 
%\begin{align*}
%	\mu_{\alpha,\nu} \left( \Rcal([R_n - y_n,R_n]) \right) 
%    &= \frac{\nu \alpha}{\pi} \int_{R_n - y_n}^{R_n} \pi e^{\frac{R_n}{2}} e^{-\alpha y^\prime} \, dy^\prime  
%    	= \bigO{e^{\frac{R_n}{2}} e^{-\alpha (R_n-y_n)}} = \bigO{e^{\alpha y_n - (\alpha - \frac{1}{2})R_n}}. 
%\end{align*}
%Thus, 
%\[ 
%	\Exp{ \left| (\Pcal_{\alpha, \nu} \setminus \{(x,y)\}) \cap \BallHyp{(x,y)} \cap \BallHyp{p_n} \right|}
%    = \bigO{e^{\frac{y_n}{2} -(\alpha - \frac{1}{2}) \min \{y,y_n\}} + e^{\alpha y_n - (\alpha - \frac{1}{2})R_n}}.
%\]
%This implies that:
%\begin{eqnarray} 
%	\lefteqn{\int_{\Rcal_n ([0, R_n - y_n + 2 \ln \frac{\pi}{2}])} \Exp{h((x,y), \Pcal_{\alpha, \nu } \setminus \{(x,y)\})} 
%    e^{-\alpha y} \, dx \, dy =}  \nonumber \\
%	& & O(1) \cdot \left(\int_{\Rcal_n ([0, R_n - y_n+ 2 \ln \frac{\pi}{2}])} \ind{(x,y) \in \Psi (\BallHyp{p})\setminus \BallPo{p}} 		e^{\frac{y_n}{2} - (\alpha - \frac{1}{2}) \min \{y,y_n\} - \alpha y} \, dx \, dy \right.  \nonumber \\ 
%	& & \hspace{1cm}+\left. \int_{\Rcal_n ([0, R_n - y_n + 2 \ln \frac{\pi}{2}])} 
%    	\ind{(x,y) \in \Psi (\BallHyp{p_n})\setminus \BallPo{p_n}} 
%    	e^{\alpha y_n - (\alpha - \frac{1}{2})R_n - \alpha y} \, dx \, dy\right). \nonumber \\
%	& &\label{eq:Mecke_sum}
%\end{eqnarray}
%Now, the definitions of $\BallHyp{(x,y)}$ and of $\BallPo{p}$ together with Lemma~\ref{lem:asymptotics_Omega_hyperbolic} imply that 
%for any $y \in [0, R_n - y_n + 2 \ln \frac{\pi}{2}]$, we have 
%\[ 
%	\int \ind{(x,y) \in \Psi (\BallHyp{p_n})\setminus \BallPo{p_n}} \, dx \leq K e^{\frac{3}{2} (y_n + y) - R_n}.
%\]
%Therefore, the first integral in~\eqref{eq:Mecke_sum} is 
%\begin{align*}
%	&\hspace{-20pt}\int_{\Rcal ([0, R_n - y_n + 2 \ln \frac{\pi}{2}])} \Exp{h((x,y), \Pcal_{\alpha, \nu } \setminus \{(x,y)\})} 
%    	e^{-\alpha y} \, dx \, dy \\
%	&= O(1) \cdot e^{2 y_n - R_n} \int_{0}^{R_n - y_n + 2 \ln \frac{\pi}{2}} 
%    	e^{\frac{3y}{2} - (\alpha - \frac{1}{2})\min\{y_n,y\} - \alpha y} \, dy \\
% 	&=  O(1) \cdot e^{2 y_n - R_n} \left( \int_{0}^{y_n} e^{\frac{3y}{2} - (2\alpha - \frac{1}{2})y} \, dy 
% 		+ e^{-(\alpha-\frac{1}{2}) y_n}\int_{y_n}^{R_n - y_n + 2 \ln \frac{\pi}{2}} e^{(\frac{3}{2} - \alpha) y} \, dy \right)\\
%  	&= O(1) \cdot \left(e^{(4-2\alpha) y_n - R_n} +e^{-(\alpha - \frac{1}{2})R_n +y_n} \right).
%\end{align*}
%
%Similarly, the second integral in~\eqref{eq:Mecke_sum} is
%\begin{align*}
%	&\hspace{-30pt}\int_{\Rcal ([0, r(p) + 2 \ln \frac{\pi}{2}])} \ind{(x,y) \in \Psi (\BallHyp{p_n})\setminus 
%    	\BallPo{p_n}} e^{\alpha y_n - (\alpha - \frac{1}{2})R_n - \alpha y} \, dx \, dy\\
%	&= e^{\frac{3y_n}{2} - R_n + \alpha y_n - (\alpha - \frac{1}{2})R_n} 
%    	\cdot \int_{0}^{R_n - y_n + 2 \ln \frac{\pi}{2}} e^{\frac{3y}{2}-\alpha y} \, dy\\
%	&= O(1)\cdot e^{\frac{3y_n}{2} - R_n + \alpha y_n - (\alpha - \frac{1}{2})R_n + (\frac{3}{2} - \alpha)(R_n-y_n)} \\
%	&= O(1) \cdot e^{-(2\alpha-1) R_n + 2 \alpha y_n}.
%\end{align*}
%
%Therefore, 
%\begin{equation} \label{eq:upper_bound_faulty_edges} 
%\Exp{ |\EdgeDiff|}  = O(1) \cdot 
%\left( e^{(4-2\alpha) y_n - R_n} +e^{-(\alpha - \frac{1}{2})R_n +y_n} + e^{-(2\alpha - 1) R_n + 2 \alpha y_n} \right).
%\end{equation}
%\end{proof}
%
%\PvdH{The following lemma would be nice and I think it follows from Lemma \ref{lem:missing_edges_ball_H_P}.}
%
%\begin{lemma}
%\[
%	\lim_{n \to \infty} \frac{\Exp{N_\Pcal(k_n)}}{\Exp{N_\H(k_n)}} = 1.
%\]
%\end{lemma}
%
%\subsection{The number of missing triangles}
%
%With the above results we are now reading to analyze the number of missing triangles due to the coupling and prove Proposition \ref{prop:couling_c_H_P}. 
%
%\begin{proof}[Proof of Proposition \ref{prop:couling_c_H_P}]
%\PvdH{Will add it next week.}
%
%\begin{align*}
%	\left|\tilde{c}_{\H}^\ast(k) - c_\Pcal^\ast(k)\right| 
%    &= \frac{1}{\binom{k}{2}}\left|\sum_{p \in \Pcal} 
%    	\frac{\ind{D_\H(p) = k}}{\Exp{N_\H(k)}} \sum_{p_1,p_2 \in \Pcal} \Delta_\H(p,p_1,p_2)
%        - \frac{\ind{D_\Pcal(p) = k}}{\Exp{N_\Pcal(k)}} \sum_{p_1,p_2 \in \Pcal} \Delta_\Pcal(p,p_1,p_2)\right|\\
%    &\le c_\Pcal^\ast(k) \left|\frac{\Exp{N_\Pcal(k)}}{\Exp{N_\H(k)}} - 1\right| 
%    	+ \frac{1}{\Exp{N_\H(k)}\binom{k}{2}} \left|\Delta_\H(k) - \Delta_\Pcal(k)\right|
%    %&\hspace{10pt}+ \frac{1}{\Exp{N_\H(k)}\binom{k}{2}} \left|\sum_{p \in \Pcal} \left(\ind{D_\H(p) = k}
%    %	- \ind{D_\Pcal(p) = k}\right) \sum_{p_1, p_2 \in \Pcal}\Delta_\Pcal(p,p_1,p_2)\right|\\
%\end{align*}
%
%\end{proof}

\subsection{Coupling $G_{\H,n}$ to $G_{\widetilde{\H},n}$}\label{ssec:coupling_H_HP}

The main result of this section is the following
\[
	\lim_{n \to \infty} s_\alpha(k_n)\Exp{\left|c_{\H,n}(k_n) - c_{\HP,n}^\ast(k_n)\right|} = 0.
\]

We start by proving Lemma~\ref{lem:clustering_ast_H}


\begin{proof}[Proof of Lemma~\ref{lem:clustering_ast_H}]
Note that by Proposition~\ref{prop:clustering_ast_H_Pois} and Proposition
Let $0 < \delta < 1$ and define the event
\begin{align*}
	A_n &= \left\{\left|N_{\H,n}(k_n) - \Exp{N_{\H,n}(k_n)}\right| \le \Exp{N_{\H,n}(k_n)}^{\frac{1 + \delta}{2}}\right\}.
	%B_n &= \left\{\left|N_{\H,n}(k_n) - \Exp{N_{\H,n}(k_n)}\right| \le n\right\},
\end{align*}

Since $N_{\H,n}(k_n) = \sum_{i = 1}^n \ind{D_\H(i) = k_n}$ it follows from Lemma~\ref{lem:general_concentration_sum_indicators}, with $c = \Exp{N_{\H,n}(k_n)}^{-\frac{1-\delta}{2}}$, that
\begin{equation}\label{eq:clustering_ast_H_prob_A}
	\Prob{A_n} \ge 1 - \bigO{e^{-\frac{\Exp{N_{\H,n}(k_n)}^\delta}{2}}} = 1 - \bigO{e^{-\frac{n^\delta k_n^{-\delta(2\alpha + 1)}}{2}}},
\end{equation}
where the last part is due to Lemma~\ref{lem:diff_Nk_hyperbolic_binomial_poisson}. 

On the event $A_n$
\[
	\left|\frac{\Exp{N_{\H,n}(k_n)}}{N_{\H,n}(k_n)} - 1\right| 
	\le \frac{\Exp{N_{\H,n}(k_n)}^{\frac{1 + \delta}{2}}}{\Exp{N_{\H,n}(k_n)}+\Exp{N_{\H,n}(k_n)}^{\frac{1 + \delta}{2}}}
	\le \Exp{N_{\H,n}(k_n)}^{-\frac{1 - \delta}{2}}.
\]
%while on the event $B_n$
%\[
%	\left|\frac{\Exp{N_{\H,n}(k_n)}}{N_{\H,n}(k_n)} - 1\right| 
%	\le \frac{n}{\Exp{N_{\H,n}(k_n)} + n} \le 1.
%\]

Therefore we have
\begin{align*}
	\Exp{\left|c_{\H, n}^\ast(k_n) - c_{\H, n}(k_n)\right|}
	&\le \Exp{\left|c_{\H, n}^\ast(k_n) - c_{\H, n}(k_n)\right|\ind{A_n}} + \bigO{1 - \Prob{A_n}}\\
	&= \Exp{c_{\H, n}^\ast(k_n)\left|\frac{\Exp{N_{\H,n}(k_n)}}{N_{\H,n}(k_n)} - 1\right|\ind{A_n}}
		+ \bigO{e^{-\frac{n^\delta k_n^{-\delta(2\alpha + 1)}}{2}}}\\
	&\le \Exp{c_{\H, n}^\ast(k_n)}\Exp{N_{\H,n}(k_n)}^{-\frac{1 - \delta}{2}} 
		+ \bigO{e^{-\frac{n^\delta k_n^{-\delta(2\alpha + 1)}}{2}}}.
\end{align*}
Since $\Exp{N_{\H,n}(k_n)} = \bigT{n k_n^{-(2\alpha + 1)}} \to \infty$, the first term is clearly $\smallO{\Exp{c_{\H, n}^\ast(k_n)}}$. This term is $\smallO{s_\alpha(k_n)}$ finishes the proof. 
\end{proof}

Next we shall prove Proposition~\ref{prop:couling_c_H_P}. First note that by combining Proposition~\ref{prop:convergence_average_clustering_P_n} and Proposition~\ref{prop:asymptotics_average_clustering_ast_P} we have that
\begin{equation}
	\Exp{c_{\Pcal,n}^\ast(k_n)} = \bigT{s_\alpha(k_n)}
\end{equation}

To achieve the results we consider the standard coupling between the binomial and Poisson process. That is, we take a sequence of i.i.d. random elements $z_1, z_2, \dots$ uniformly on the hyperbolic disk of radius $R_n$, i.e. according to the distribution \eqref{eq:def_hyperbolic_point_distribution}. Then the original hyperbolic random graph consists of the first $n$ points and the poissonized version of the first $N \stackrel{d}{=} Po(n)$ many points ($N$ is a Poisson random variable with mean $n$). Under this coupling $N_{\H,n}(k) = \sum_{j=1}^n \ind{D_\H(z_j)=k}$ denotes the number of degree $k$ vertices in the original Hyperbolic random graph model with $n$ vertices and $N_{\HP,n}(k)=\sum_{j=1}^{N} \ind{D_{\HP}(z_j)=k}$ denotes the number of degree $k$ vertices in the Poisson version of the Hyperbolic random graph.

\begin{lemma}\label{lem:diff_Nk_hyperbolic_binomial_poisson}
Let $\{k_n\}_{n \ge 1}$ be sequence of natural numbers with $0 \leq k_n \leq n-1$ and $k_n = o(n^{\frac{1}{2\alpha+1}})$. Then
\[
	\Exp{\left|N_{\H,n}(k_n)-N_{\HP,n}(k_n)\right|} = \smallO{\Exp{N_{\HP,n}(k_n)}} = \smallO{n k^{-(2\alpha+1)}}.
\]

\end{lemma}
\begin{proof}
We use the Chernoff concentration result for a Poisson random~\eqref{eq:def_chernoff_bound_poisson}, with probability $n^{-c^2/2}$ the Poisson random variable $N$ with expectation $n$ is contained in the interval $[n-c\sqrt{n\log n},n+c\sqrt{n \log n}]$. We proceed by bounding the effect on the number of degree $k_n$ vertices of adding or removing $c\sqrt{n\log n}$ many vertices to $G_{\H,n}(\alpha,\nu$ and from $G_{\HP,n}(\alpha,\nu)$, respectively.

Define the events
\begin{align*}
	A_n^{(1)} &:= \{N \in [n, n+c\sqrt{n \log n}]\}\\
	A_n^{(2)} &:= \{N \in [n-c\sqrt{n\log n}, n)\}\\
\end{align*}
and let $A_n = A_n^{(1)} \cup A_n^{(2)}$. Then,
\begin{align*}
\Exp{\left|N_{\H,n}(k_n)-N_{\HP,n}(k_n)\right|} 
&\le \CExp{\left|N_{\H,n}(k_n)-N_{\HP,n}(k_n)\right|}{A_n} + \bigO{n^{1 - c^2/2}}\\
&= \CExp{\left|N_{\H,n}(k_n)-N_{\HP,n}(k_n)\right|}{A_n} + \smallO{\Exp{N_{\HP,n}(k_n)}}
\end{align*}
by choosing $c$ large enough, e.g. $c > \sqrt{2}$. What is left to show is that for any $c > 0$
\[
	 \CExp{\left|N_{\H,n}(k_n)-N_{\HP,n}(k_n)\right|}{A_n} = \smallO{\Exp{N_{\HP,n}(k_n)}}.
\]


Let $V_{\H,n}(k_n)$ be the set of degree $k_n$ vertices in the binomial graph $G_{\H,n}$ and $V_{\HP,n}(k_n)$ be the set of degree $k_n$ vertices in the Poisson model $G_{\HP,n}$. Then 
\[
	|N_k^{(n)}-N_k^{(Po(n))}| = |V_{\H,n}(k_n) \Delta V_{\HP,n}(k_n)| = |V_{\H,n}(k_n) \backslash V_{\HP,n}(k_n) | + |V_{\HP,n}(k_n) \backslash V_{\H,n}(k_n) |,
\]
where $A \Delta B$ denotes the symmetric difference between two sets $A$ and $B$.

We first consider the case $N \in [n,n+c\sqrt{n\log n}]$, i.e. event $A_n^{(1)}$. For $z \in V_{\HP,n}(k_n) \backslash V_{\H,n}(k_n)$, $z$ has degree $k$ in the Poisson graph, but not in the binomial graph; so as $N \geq n$, either $z$ or one of its $k$ neighbors must have been removed during the transition from the Poisson graph to the binomial graph. On the event $A_n$, at most $c\sqrt{n\log n}$ many vertices are removed. The probability of hitting a degree $k$ vertex or one of its neighbors is $\leq \frac{k+1}{N} \leq \frac{k+1}{n}$. Therefore, by the union bound the probability that a particular degree $k$ vertex of the Poisson graph is removed is upper bounded by $c\sqrt{n\log n}\frac{k+1}{n}$. Hence, the expected number of degree $k_n$ vertices that disappear in the transition from the Poisson graph to the binomial graph is bounded by 
\[
	\CExp{|V_{\HP,n}(k_n) \backslash V_{\H,n}(k_n)}{A_n^{(1)}}\leq \E[N_{\HP,n}(k_n)] c\sqrt{n \log n}\frac{k_n+1}{n}  = o(\E[N_k^{(Po(n))}]),
\] 
where the last line follows since for $\alpha > 1/2$,
\[
	k_n \sqrt{\frac{\log(n)}{n}} = \smallO{n^{\frac{1}{2\alpha + 1}}\sqrt{\frac{\log(n)}{n}}} 
	= \smallO{n^{-\frac{2\alpha - 1}{4\alpha + 2}} \sqrt{\log(n)}} = \smallO{1}.
\]

For $z \in V_{\H,n}(k_n) \backslash V_{\HP,n}(k_n)$, $z$ is a degree $k_n$ vertex in the binomial graph, but 
must have degree $k_n+\ell$ in the Poisson graph (where $1 \leq \ell \leq c\sqrt{n\log n}$). By linearity of expectation the expected number of degree $k_n+\ell$ vertices of the Poisson graph which turn into degree $k_n$ vertices of the binomial graph is equal to the expected number of degree $k_n+\ell$ vertices in the Poisson graph times the probability that a degree $k_n+\ell$ vertex turns into a degree $k_n$ vertex in the transition back, from the Poisson graph to the binomial graph. The probability of choosing uniformly a set of $\ell$ neighbors of a degree $k_n+\ell$ vertex of the Poisson graph is given by $\frac{k_n+\ell}{N}\cdots \frac{k_+1}{N-\ell + 1}$. Now, using $k_n = \smallO{n^{\frac{1}{2\alpha+1}}} = \smallO{c\sqrt{n\log n}}$ for $\alpha > \frac{1}{2}$, $\ell \leq c\sqrt{n \log n}$ and $N-\ell + 1 \geq n$, this probability is bounded from above by $(c+1)^\ell (\frac{\sqrt{n \log n}}{n})^\ell =((c+1) \sqrt{\frac{\log n}{n}})^\ell$ which is upper bounded by $(\frac{1}{2})^\ell$ for $n$ large enough, i.e. $n \geq n_0$. Therefore, using the geometric series, we conclude
\begin{align*}
\CExp{|V_{\H,n}(k_n) \backslash V_{\HP,n}(k_n)|}{A_n^{(1)}} 
&\le  \sum_{\ell=1}^{\sqrt{n\log n}} \E[N_{k_n+\ell}^{(Po(n))}] ((c+1)\sqrt{\frac{\log n}{n}})^{\ell}\\
&\le \sum_{\ell=1}^{\sqrt{n\log n}} \bigT{n (k_n+\ell)^{-2\alpha-1}} ((c+1)\sqrt{\frac{\log n}{n}})^{\ell}\\
&= \bigO{\Exp{N_{\HP,n}(k_n)}} \sum_{\ell=1}^{\sqrt{n\log n}} ((c+1)\sqrt{\frac{\log n}{n}})^{\ell}
= \smallO{\Exp{N_{\HP,n}(k_n)}},
\end{align*}
and hence
\[
	\CExp{\left|N_{\H,n}(k_n)-N_{\HP,n}(k_n)\right|}{A_n^{(1)}} = \smallO{\Exp{N_{\HP,n}(k_n)}}.
\]

The case $N \in [n-c\sqrt{n\log n},n)$ (event $A_n^{(2)}$) works similarly.
As $N < n$, a vertex $z \in V_{\HP,n}(k_n) \backslash V_{\H,n}(k_n)$ with degree $k_n$ in the Poisson graph must have a strictly larger degree in the binomial graph, i.e. in the transition from the Poisson graph to the binomial graph, a vertex must have been dropped in the neighborhood of $z$. By the union bound, this can be upper bounded by the number of additional vertices (of the binomial graph) times the probability that a random point falls into the neighborhood of a degree $k$ vertex. We obtain
\begin{align*}
	\CExp{|V_{\HP,n}(k_n) \backslash V_{\H,n}(k_n)|}{A_n^{(2)}} = \bigO{ \sqrt{n\log n}\frac{k}{n} \Exp{N_{\HP,n}(k_n)} }
	= \smallO{\Exp{N_{\HP,n}(k_n)}}
\end{align*}

A vertex $z \in V_{\H,n}(k_n) \backslash V_{\HP,n}(k_n)$ could be one of the additional vertices in the binomial graph or it is a degree $k_n-\ell$ vertex of the Poisson graph which receives exactly $\ell$ new vertices in its neighborhood in the transition from the Poisson graph to the binomial graph. The probability that one of the additional vertices of the binomial graph (compared to the smaller Poisson graph) has degree $k_n$ has the asymptotic order $k_n^{-(2\alpha+1)}$ (as can be seen by considering the alternative coupling between the binomial and the Poisson process, where instead of taking $z_1, \dots, z_N$ for the Poisson process, we take the points $z_n, z_{n-1}, \dots, z_{n-N+1}$ (resp. points with index larger than $n$ after we hit $z_1$): for this graph, we have that the expected number of degree $k_n$ vertices is $\bigT{n k^{-2\alpha-1}}$, so the probability that a vertex chosen uniformly from the Poisson graph has degree $k$ is $\Theta(k^{-2\alpha-1})$). Therefore, the expected number of additional points with degree $k_n$ is $\bigO{\sqrt{n\log n} k_n^{-2\alpha-1}} = \smallO{n k_n^{-2\alpha-1}} = \smallO{\Exp{N_{\HP,n}(k_n)}}$. The expected number of degree $k_n-\ell$ vertices of the Poisson graph which receive exactly $\ell$ new vertices can be bounded in a sum resp. series similarly as done for $z \in V_{\H,n}(k_n) \backslash V_{\HP,n}(k_n)$ in the case $N \geq n$. We therefore conclude that
\[
	\CExp{\left|N_{\H,n}(k_n)-N_{\HP,n}(k_n)\right|}{A_n^{(2)}} = \smallO{\Exp{N_{\HP,n}(k_n)}},
\]
which finishes the proof.
\end{proof}

\begin{proposition}\label{prop:clustering_ast_H_Pois}
\[
	\lim_{n \to \infty} s_\alpha(k_n)\Exp{\left|c_{\H,n}^\ast(k_n) - c_{\HP,n}^\ast(k_n)\right|} = 0.
\]
\end{proposition}

\begin{proof}
\PvdH{Proof taken from Markus notes (slightly edited). Should probably be improved.}
We are looking at the modified clustering coefficient, where we divide by the expected number of degree $k_n$ vertices. As the expected numbers of degree $k_n$ vertices in the Poisson and binomial graph are asymptotically equivalent, it is therefore sufficient to consider the sum of the clustering coefficients of all vertices of degree $k$.
Given again the standard coupling between the binomial and Poisson process, we denote by $V_{\H,n}(k_n)$ the set of degree $k_n$ vertices in the binomial graph and by $V_{\HP,n}(k_n)$ the set of degree $k_n$ vertices in the Poisson graph. If a vertex is contained in both sets, it must have the same degree in both the Poisson and binomial graph, and given the nature of the coupling, the neighbourhoods are therefore the same and hence also their clustering coefficients agree.

The difference of the sum of the clustering coefficients therefore comes from all the clustering coefficients of the symmetric difference $V_{\H,n}(k_n) \Delta V_{\HP,n}(k_n)$. This symmetric difference is again a Poisson process, whose expected number of points is $\E | N_k^{(n)}-N_k^{(Po(n))}| = o(\E N_k^{(Po(n))})$. Using the Palm-Mecke formula, we have that
\begin{align*}
	\Exp{\left|c_{\H,n}^\ast(k_n) - c_{\HP,n}^\ast(k_n)\right|}
	&\le \frac{\Exp{\left|N_{\H,n}(k_n)-N_{\HP,n}(k_n)\right|}}{\expH} \Exp{c_{\H,n}^\ast(k_n)}
	= \smallO{1}\Exp{c_{\H,n}^\ast(k_n)},
\end{align*}
where the last line follows from Lemma~\ref{lem:diff_Nk_hyperbolic_binomial_poisson}. The result now follows by applying Proposition~\ref{prop:couling_c_H_P}, \ref{prop:convergence_average_clustering_P_n} and Proposition~\ref{prop:asymptotics_average_clustering_ast_P}.
\end{proof}


